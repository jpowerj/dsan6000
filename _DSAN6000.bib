@book{eagar_data_2021,
  title = {Data {{Engineering}} with {{AWS}}: {{Learn}} How to Design and Build Cloud-Based Data Transformation Pipelines Using {{AWS}}},
  shorttitle = {Data {{Engineering}} with {{AWS}}},
  author = {Eagar, Gareth},
  year = {2021},
  month = dec,
  publisher = {Packt Publishing Ltd},
  url = {https://www.dropbox.com/scl/fi/s3wdlko7bczjtcisskx35/Data-Engineering-with-AWS-_-learn-how-to-design-and-build-Gareth-Eagar.pdf?rlkey=qt8862j5729nqzkj2yuec0t6l&dl=1},
  abstract = {The missing expert-led manual for the AWS ecosystem --- go from foundations to building data engineering pipelines effortlessly Purchase of the print or Kindle book includes a free eBook in the PDF format.Key FeaturesLearn about common data architectures and modern approaches to generating value from big dataExplore AWS tools for ingesting, transforming, and consuming data, and for orchestrating pipelinesLearn how to architect and implement data lakes and data lakehouses for big data analytics from a data lakes expertBook DescriptionWritten by a Senior Data Architect with over twenty-five years of experience in the business, Data Engineering for AWS is a book whose sole aim is to make you proficient in using the AWS ecosystem. Using a thorough and hands-on approach to data, this book will give aspiring and new data engineers a solid theoretical and practical foundation to succeed with AWS. As you progress, you'll be taken through the services and the skills you need to architect and implement data pipelines on AWS. You'll begin by reviewing important data engineering concepts and some of the core AWS services that form a part of the data engineer's toolkit. You'll then architect a data pipeline, review raw data sources, transform the data, and learn how the transformed data is used by various data consumers. You'll also learn about populating data marts and data warehouses along with how a data lakehouse fits into the picture. Later, you'll be introduced to AWS tools for analyzing data, including those for ad-hoc SQL queries and creating visualizations. In the final chapters, you'll understand how the power of machine learning and artificial intelligence can be used to draw new insights from data. By the end of this AWS book, you'll be able to carry out data engineering tasks and implement a data pipeline on AWS independently.What you will learnUnderstand data engineering concepts and emerging technologiesIngest streaming data with Amazon Kinesis Data FirehoseOptimize, denormalize, and join datasets with AWS Glue StudioUse Amazon S3 events to trigger a Lambda process to transform a fileRun complex SQL queries on data lake data using Amazon AthenaLoad data into a Redshift data warehouse and run queriesCreate a visualization of your data using Amazon QuickSightExtract sentiment data from a dataset using Amazon ComprehendWho this book is forThis book is for data engineers, data analysts, and data architects who are new to AWS and looking to extend their skills to the AWS cloud. Anyone new to data engineering who wants to learn about the foundational concepts while gaining practical experience with common data engineering services on AWS will also find this book useful. A basic understanding of big data-related topics and Python coding will help you get the most out of this book but it's not a prerequisite. Familiarity with the AWS console and core services will also help you follow along.},
  googlebooks = {xrpREAAAQBAJ},
  isbn = {978-1-80056-904-1},
  langid = {english}
}

@book{gopalan_cloud_2022,
  title = {The {{Cloud Data Lake}}: {{A Guide}} to {{Building Robust Cloud Data Architecture}}},
  shorttitle = {The {{Cloud Data Lake}}},
  author = {Gopalan, Rukmani},
  year = {2022},
  month = dec,
  publisher = {O'Reilly Media, Inc.},
  url = {https://www.dropbox.com/scl/fi/7u469ccc8y169us5hydk0/The-cloud-data-lake-a-guide-to-building-robust-cloud-data-Rukmani-Gopalan.pdf?rlkey=ey06a6zt9g90d4zq8tfncndta&dl=1},
  abstract = {More organizations than ever understand the importance of data lake architectures for deriving value from their data. Building a robust, scalable, and performant data lake remains a complex proposition, however, with a buffet of tools and options that need to work together to provide a seamless end-to-end pipeline from data to insights.This book provides a concise yet comprehensive overview on the setup, management, and governance of a cloud data lake. Author Rukmani Gopalan, a product management leader and data enthusiast, guides data architects and engineers through the major aspects of working with a cloud data lake, from design considerations and best practices to data format optimizations, performance optimization, cost management, and governance.Learn the benefits of a cloud-based big data strategy for your organizationGet guidance and best practices for designing performant and scalable data lakesExamine architecture and design choices, and data governance principles and strategiesBuild a data strategy that scales as your organizational and business needs increaseImplement a scalable data lake in the cloudUse cloud-based advanced analytics to gain more value from your data},
  googlebooks = {jkuhEAAAQBAJ},
  isbn = {978-1-0981-1654-5},
  langid = {english}
}

@book{janssens_python_2025,
  title = {Python {{Polars}}: {{The Definitive Guide}}: {{Transforming}}, {{Analyzing}}, and {{Visualizing Data}} with a {{Fast}} and {{Expressive DataFrame API}}},
  shorttitle = {Python {{Polars}}},
  author = {Janssens, Jeroen and Nieuwdorp, Thijs},
  year = {2025},
  month = feb,
  publisher = {O'Reilly Media, Inc.},
  url = {https://www.dropbox.com/scl/fi/xbku8b76x1zd3s6wpjmud/Python-Polars-the-Definitive-Guide.pdf?rlkey=y132hd73541e2mzwb9pppbwhq&dl=1},
  abstract = {Unlock the power of Polars, a Python package for transforming, analyzing, and visualizing data. In this hands-on guide, Jeroen Janssens and Thijs Nieuwdorp walk you through every feature of Polars, showing you how to use it for real-world tasks like data wrangling, exploratory data analysis, building pipelines, and more.Whether you're a seasoned data professional or new to data science, you'll quickly master Polars' expressive API and its underlying concepts. You don't need to have experience with pandas, but if you do, this book will help you make a seamless transition. The many practical examples and real-world datasets are available on GitHub, so you can easily follow along.Process data from CSV, Parquet, spreadsheets, databases, and the cloudGet a solid understanding of Expressions, the building blocks of every queryHandle complex data types, including text, time, and nested structuresUse both eager and lazy APIs, and know when to use eachVisualize your data with Altair, hvPlot, plotnine, and Great TablesExtend Polars with your own Python functions and Rust pluginsLeverage GPU acceleration to boost performance even further},
  googlebooks = {LI1IEQAAQBAJ},
  isbn = {978-1-0981-5605-3},
  langid = {english}
}

@book{kleppmann_designing_2017,
  title = {Designing {{Data-Intensive Applications}}: {{The Big Ideas Behind Reliable}}, {{Scalable}}, and {{Maintainable Systems}}},
  shorttitle = {Designing {{Data-Intensive Applications}}},
  author = {Kleppmann, Martin},
  year = {2017},
  month = mar,
  publisher = {O'Reilly Media, Inc.},
  url = {https://www.dropbox.com/scl/fi/42hm32h6qz8mrj5bm5eet/DesigningData-IntensiveApplications_TheBigIdeasBehind-Kleppmann-Martin.pdf?rlkey=nournir9s213rsmegi3eo0zn3&dl=1},
  abstract = {Data is at the center of many challenges in system design today. Difficult issues need to be figured out, such as scalability, consistency, reliability, efficiency, and maintainability. In addition, we have an overwhelming variety of tools, including relational databases, NoSQL datastores, stream or batch processors, and message brokers. What are the right choices for your application? How do you make sense of all these buzzwords?In this practical and comprehensive guide, author Martin Kleppmann helps you navigate this diverse landscape by examining the pros and cons of various technologies for processing and storing data. Software keeps changing, but the fundamental principles remain the same. With this book, software engineers and architects will learn how to apply those ideas in practice, and how to make full use of data in modern applications.Peer under the hood of the systems you already use, and learn how to use and operate them more effectivelyMake informed decisions by identifying the strengths and weaknesses of different toolsNavigate the trade-offs around consistency, scalability, fault tolerance, and complexityUnderstand the distributed systems research upon which modern databases are builtPeek behind the scenes of major online services, and learn from their architectures},
  googlebooks = {p1heDgAAQBAJ},
  isbn = {978-1-4919-0310-0},
  langid = {english}
}

@book{leskovec_mining_2014,
  title = {Mining of {{Massive Datasets}}},
  author = {Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David},
  year = {2014},
  month = nov,
  publisher = {Cambridge University Press},
  abstract = {Written by leading authorities in database and Web technologies, this book is essential reading for students and practitioners alike. The popularity of the Web and Internet commerce provides many extremely large datasets from which information can be gleaned by data mining. This book focuses on practical algorithms that have been used to solve key problems in data mining and can be applied successfully to even the largest datasets. It begins with a discussion of the map-reduce framework, an important tool for parallelizing algorithms automatically. The authors explain the tricks of locality-sensitive hashing and stream processing algorithms for mining data that arrives too fast for exhaustive processing. Other chapters cover the PageRank idea and related tricks for organizing the Web, the problems of finding frequent itemsets and clustering. This second edition includes new and extended coverage on social networks, machine learning and dimensionality reduction.},
  googlebooks = {16YaBQAAQBAJ},
  isbn = {978-1-107-07723-2},
  langid = {english}
}

@misc{loukides_what_2010,
  title = {What Is Data Science?},
  author = {Loukides, Mike},
  year = {2010},
  month = jun,
  journal = {O'Reilly Media},
  url = {https://www.oreilly.com/radar/what-is-data-science/},
  urldate = {2025-09-02},
  abstract = {The future belongs to the companies and people that turn data into products.},
  langid = {american},
  file = {/Users/jpj/Zotero/storage/A9NMGBE5/what-is-data-science.html}
}

@article{mell_nist_2011,
  title = {The {{NIST Definition}} of {{Cloud Computing}}},
  author = {Mell, Peter and Grance, Timothy},
  year = {2011},
  journal = {National Institute of Standards and Technology, Special Publication},
  volume = {800},
  number = {2011},
  pages = {145},
  url = {https://nvlpubs.nist.gov/nistpubs/legacy/sp/nistspecialpublication800-145.pdf}
}

@book{needham_duckdb_2024,
  title = {{{DuckDB}} in {{Action}}},
  author = {Needham, Mark and Hunger, Michael and Simons, Michael},
  year = {2024},
  month = aug,
  publisher = {{Simon and Schuster}},
  url = {https://www.dropbox.com/scl/fi/l1lz7dqz1nhumbvmre408/PythonPolars_TheDefinitiveGuide_Transforming-JeroenJanssens-ThijsNieuwdorp-1-2025apr01-O-ReillyMedia-Incorporated-9781098156084-094d66f3df5acfda8eb1b5f6d2002878-Anna-sArchive.pdf?rlkey=6nbdad5khui8tc2qe1i8yhv4c&dl=1},
  abstract = {Dive into DuckDB and start processing gigabytes of data with ease---all with no data warehouse.DuckDB is a cutting-edge SQL database that makes it incredibly easy to analyze big data sets right from your laptop. In DuckDB in Action you'll learn everything you need to know to get the most out of this awesome tool, keep your data secure on prem, and save you hundreds on your cloud bill. From data ingestion to advanced data pipelines, you'll learn everything you need to get the most out of DuckDB---all through hands-on examples. Open up DuckDB in Action and learn how to:{$\bullet$} Read and process data from CSV, JSON and Parquet sources both locally and remote {$\bullet$} Write analytical SQL queries, including aggregations, common table expressions, window functions, special types of joins, and pivot tables {$\bullet$} Use DuckDB from Python, both with SQL and its "Relational"-API, interacting with databases but also data frames {$\bullet$} Prepare, ingest and query large datasets {$\bullet$} Build cloud data pipelines {$\bullet$} Extend DuckDB with custom functionality  Pragmatic and comprehensive, DuckDB in Action introduces the DuckDB database and shows you how to use it to solve common data workflow problems. You won't need to read through pages of documentation---you'll learn as you work. Get to grips with DuckDB's unique SQL dialect, learning to seamlessly load, prepare, and analyze data using SQL queries. Extend DuckDB with both Python and built-in tools such as MotherDuck, and gain practical insights into building robust and automated data pipelines. Purchase of the print book includes a free eBook in PDF and ePub formats from Manning Publications.  About the technology  DuckDB makes data analytics fast and fun! You don't need to set up a Spark or run a cloud data warehouse just to process a few hundred gigabytes of data. DuckDB is easily embeddable in any data analytics application, runs on a laptop, and processes data from almost any source, including JSON, CSV, Parquet, SQLite and Postgres.  About the book  DuckDB in Action guides you example-by-example from setup, through your first SQL query, to advanced topics like building data pipelines and embedding DuckDB as a local data store for a Streamlit web app. You'll explore DuckDB's handy SQL extensions, get to grips with aggregation, analysis, and data without persistence, and use Python to customize DuckDB. A hands-on project accompanies each new topic, so you can see DuckDB in action.  What's inside{$\bullet$} Prepare, ingest and query large datasets {$\bullet$} Build cloud data pipelines {$\bullet$} Extend DuckDB with custom functionality {$\bullet$} Fast-paced SQL recap: From simple queries to advanced analytics About the reader  For data pros comfortable with Python and CLI tools.  About the author  Mark Needham is a blogger and video creator at @?LearnDataWithMark. Michael Hunger leads product innovation for the Neo4j graph database. Michael Simons is a Java Champion, author, and Engineer at Neo4j.},
  googlebooks = {zfgWEQAAQBAJ},
  isbn = {978-1-63343-725-8},
  langid = {english}
}

@inproceedings{raasveldt_duckdb_2019,
  title = {{{DuckDB}}: An {{Embeddable Analytical Database}}},
  shorttitle = {{{DuckDB}}},
  booktitle = {Proceedings of the 2019 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Raasveldt, Mark and M{\"u}hleisen, Hannes},
  year = {2019},
  month = jun,
  series = {{{SIGMOD}} '19},
  pages = {1981--1984},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3299869.3320212},
  url = {https://doi.org/10.1145/3299869.3320212},
  urldate = {2025-09-13},
  abstract = {The immense popularity of SQLite shows that there is a need for unobtrusive in-process data management solutions. However, there is no such system yet geared towards analytical workloads. We demonstrate DuckDB, a novel data management system designed to execute analytical SQL queries while embedded in another process. In our demonstration, we pit DuckDB against other data management solutions to showcase its performance in the embedded analytics scenario. DuckDB is available as Open Source software under a permissive license.},
  isbn = {978-1-4503-5643-5},
  file = {/Users/jpj/Zotero/storage/P4EFUVN7/Raasveldt and Mühleisen - 2019 - DuckDB an Embeddable Analytical Database.pdf}
}

@book{reis_fundamentals_2022,
  title = {Fundamentals of {{Data Engineering}}: {{Plan}} and {{Build Robust Data Systems}}},
  shorttitle = {Fundamentals of {{Data Engineering}}},
  author = {Reis, Joe and Housley, Matt},
  year = {2022},
  month = jun,
  publisher = {O'Reilly Media, Inc.},
  abstract = {Data engineering has grown rapidly in the past decade, leaving many software engineers, data scientists, and analysts looking for a comprehensive view of this practice. With this practical book, you'll learn how to plan and build systems to serve the needs of your organization and customers by evaluating the best technologies available through the framework of the data engineering lifecycle. Authors Joe Reis and Matt Housley walk you through the data engineering lifecycle and show you how to stitch together a variety of cloud technologies to serve the needs of downstream data consumers. You'll understand how to apply the concepts of data generation, ingestion, orchestration, transformation, storage, and governance that are critical in any data environment regardless of the underlying technology. This book will help you: Get a concise overview of the entire data engineering landscapeAssess data engineering problems using an end-to-end framework of best practicesCut through marketing hype when choosing data technologies, architecture, and processesUse the data engineering lifecycle to design and build a robust architectureIncorporate data governance and security across the data engineering lifecycle},
  googlebooks = {26d2EAAAQBAJ},
  isbn = {978-1-0981-0825-0},
  langid = {english}
}

@book{topol_inmemory_2024,
  title = {In-{{Memory Analytics}} with {{Apache Arrow}}},
  shorttitle = {In-{{Memory Analytics}} with {{Apache Arrow}}},
  author = {Topol, Matthew and McKinney, Wes},
  year = {2024},
  month = sep,
  publisher = {Packt Publishing Ltd},
  abstract = {Harness the power of Apache Arrow to optimize tabular data processing and develop robust, high-performance data systems with its standardized, language-independent columnar memory formatKey FeaturesExplore Apache Arrow's data types and integration with pandas, Polars, and ParquetWork with Arrow libraries such as Flight SQL, Acero compute engine, and Dataset APIs for tabular dataEnhance and accelerate machine learning data pipelines using Apache Arrow and its subprojectsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionApache Arrow is an open source, columnar in-memory data format designed for efficient data processing and analytics. This book harnesses the author's 15 years of experience to show you a standardized way to work with tabular data across various programming languages and environments, enabling high-performance data processing and exchange. This updated second edition gives you an overview of the Arrow format, highlighting its versatility and benefits through real-world use cases. It guides you through enhancing data science workflows, optimizing performance with Apache Parquet and Spark, and ensuring seamless data translation. You'll explore data interchange and storage formats, and Arrow's relationships with Parquet, Protocol Buffers, FlatBuffers, JSON, and CSV. You'll also discover Apache Arrow subprojects, including Flight, SQL, Database Connectivity, and nanoarrow. You'll learn to streamline machine learning workflows, use Arrow Dataset APIs, and integrate with popular analytical data systems such as Snowflake, Dremio, and DuckDB. The latter chapters provide real-world examples and case studies of products powered by Apache Arrow, providing practical insights into its applications. By the end of this book, you'll have all the building blocks to create efficient and powerful analytical services and utilities with Apache Arrow.What you will learnUse Apache Arrow libraries to access data files, both locally and in the cloudUnderstand the zero-copy elements of the Apache Arrow formatImprove the read performance of data pipelines by memory-mapping Arrow filesProduce and consume Apache Arrow data efficiently by sharing memory with the C APILeverage the Arrow compute engine, Acero, to perform complex operationsCreate Arrow Flight servers and clients for transferring data quicklyBuild the Arrow libraries locally and contribute to the communityWho this book is forThis book is for developers, data engineers, and data scientists looking to explore the capabilities of Apache Arrow from the ground up. Whether you're building utilities for data analytics and query engines, or building full pipelines with tabular data, this book can help you out regardless of your preferred programming language. A basic understanding of data analysis concepts is needed, but not necessary. Code examples are provided using C++, Python, and Go throughout the book.},
  googlebooks = {G0EgEQAAQBAJ},
  isbn = {978-1-83546-968-2},
  langid = {english}
}

@book{virtuoso_serverless_2021,
  title = {Serverless {{Analytics}} with {{Amazon Athena}}: {{Query}} Structured, Unstructured, or Semi-Structured Data in Seconds without Setting up Any Infrastructure},
  shorttitle = {Serverless {{Analytics}} with {{Amazon Athena}}},
  author = {Virtuoso, Anthony and Hocanin, Mert Turkay and Wishnick, Aaron and Pathak, Rahul},
  year = {2021},
  month = nov,
  publisher = {Packt Publishing Ltd},
  abstract = {Get more from your data with Amazon Athena\&\#39;s ease-of-use, interactive performance, and pay-per-query pricingKey FeaturesExplore the promising capabilities of Amazon Athena and Athena\&\#39;s Query Federation SDKUse Athena to prepare data for common machine learning activitiesCover best practices for setting up connectivity between your application and Athena and security considerationsBook DescriptionAmazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using SQL, without needing to manage any infrastructure.This book begins with an overview of the serverless analytics experience offered by Athena and teaches you how to build and tune an S3 Data Lake using Athena, including how to structure your tables using open-source file formats like Parquet. You\&\#39;ll learn how to build, secure, and connect to a data lake with Athena and Lake Formation. Next, you\&\#39;ll cover key tasks such as ad hoc data analysis, working with ETL pipelines, monitoring and alerting KPI breaches using CloudWatch Metrics, running customizable connectors with AWS Lambda, and more. Moving on, you\&\#39;ll work through easy integrations, troubleshooting and tuning common Athena issues, and the most common reasons for query failure. You will also review tips to help diagnose and correct failing queries in your pursuit of operational excellence. Finally, you\&\#39;ll explore advanced concepts such as Athena Query Federation and Athena ML to generate powerful insights without needing to touch a single server.By the end of this book, you\&\#39;ll be able to build and use a data lake with Amazon Athena to add data-driven features to your app and perform the kind of ad hoc data analysis that often precedes many of today\&\#39;s ML modeling exercises.What you will learnSecure and manage the cost of querying your dataUse Athena ML and User Defined Functions (UDFs) to add advanced features to your reportsWrite your own Athena Connector to integrate with a custom data sourceDiscover your datasets on S3 using AWS Glue CrawlersIntegrate Amazon Athena into your applicationsSetup Identity and Access Management (IAM) policies to limit access to tables and databases in Glue Data CatalogAdd an Amazon SageMaker Notebook to your Athena queriesGet to grips with using Athena for ETL pipelinesWho this book is forBusiness intelligence (BI) analysts, application developers, and system administrators who are looking to generate insights from an ever-growing sea of data while controlling costs and limiting operational burden, will find this book helpful. Basic SQL knowledge is expected to make the most out of this book.},
  googlebooks = {RzpIEAAAQBAJ},
  isbn = {978-1-80056-786-3},
  langid = {english}
}

@book{white_hadoop_2015,
  title = {Hadoop: {{The Definitive Guide}}},
  shorttitle = {Hadoop},
  author = {White, Tom E.},
  year = {2015},
  publisher = {O'Reilly Media, Inc.},
  abstract = {Get ready to unlock the power of your data. With the fourth edition of this comprehensive guide, you'll learn how to build and maintain reliable, scalable, distributed systems with Apache Hadoop. This book is ideal for programmers looking to analyze datasets of any size, and for administrators who want to set up and run Hadoop clusters. Using Hadoop 2 exclusively, author Tom White presents new chapters on YARN and several Hadoop-related projects such as Parquet, Flume, Crunch, and Spark. You'll learn about recent changes to Hadoop, and explore new case studies on Hadoop's role in healthcare systems and genomics data processing. Learn fundamental components such as MapReduce, HDFS, and YARNExplore MapReduce in depth, including steps for developing applications with itSet up and maintain a Hadoop cluster running HDFS and MapReduce on YARNLearn two data formats: Avro for data serialization and Parquet for nested dataUse data ingestion tools such as Flume (for streaming data) and Sqoop (for bulk data transfer)Understand how high-level data processing tools like Pig, Hive, Crunch, and Spark work with HadoopLearn the HBase distributed database and the ZooKeeper distributed configuration service.},
  googlebooks = {MhqkBwAAQBAJ},
  isbn = {978-1-4919-0171-7},
  langid = {english}
}

@book{wolohan_mastering_2020,
  title = {Mastering {{Large Datasets}} with {{Python}}: {{Parallelize}} and {{Distribute Your Python Code}}},
  shorttitle = {Mastering {{Large Datasets}} with {{Python}}},
  author = {Wolohan, John},
  year = {2020},
  month = jan,
  publisher = {{Simon and Schuster}},
  url = {https://www.dropbox.com/scl/fi/ugkacs7krqdgo0c5xqgy0/John-T.-Wolohan-Mastering-Large-Datasets-with-Python.pdf?rlkey=397zleff6uuibvhfzfme18f41&dl=1},
  abstract = {Summary Modern data science solutions need to be clean, easy to read, and scalable. In Mastering Large Datasets with Python, author J.T. Wolohan teaches you how to take a small project and scale it up using a functionally influenced approach to Python coding. You'll explore methods and built-in Python tools that lend themselves to clarity and scalability, like the high-performing parallelism method, as well as distributed technologies that allow for high data throughput. The abundant hands-on exercises in this practical tutorial will lock in these essential skills for any large-scale data science project.   Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications. About the technology Programming techniques that work well on laptop-sized data can slow to a crawl---or fail altogether---when applied to massive files or distributed datasets. By mastering the powerful map and reduce paradigm, along with the Python-based tools that support it, you can write data-centric applications that scale efficiently without requiring codebase rewrites as your requirements change. About the book Mastering Large Datasets with Python teaches you to write code that can handle datasets of any size. You'll start with laptop-sized datasets that teach you to parallelize data analysis by breaking large tasks into smaller ones that can run simultaneously. You'll then scale those same programs to industrial-sized datasets on a cluster of cloud servers. With the map and reduce paradigm firmly in place, you'll explore tools like Hadoop and PySpark to efficiently process massive distributed datasets, speed up decision-making with machine learning, and simplify your data storage with AWS S3. What's inside   An introduction to the map and reduce paradigm Parallelization with the multiprocessing module and pathos framework Hadoop and Spark for distributed computing Running AWS jobs to process large datasets   About the reader For Python programmers who need to work faster with more data. About the author J. T. Wolohan is a lead data scientist at Booz Allen Hamilton, and a PhD researcher at Indiana University, Bloomington.  Table of Contents: PART 1 1 ¦ Introduction 2 ¦ Accelerating large dataset work: Map and parallel computing 3 ¦ Function pipelines for mapping complex transformations 4 ¦ Processing large datasets with lazy workflows 5 ¦ Accumulation operations with reduce 6 ¦ Speeding up map and reduce with advanced parallelization PART 2 7 ¦ Processing truly big datasets with Hadoop and Spark 8 ¦ Best practices for large data with Apache Streaming and mrjob 9 ¦ PageRank with map and reduce in PySpark 10 ¦ Faster decision-making with machine learning and PySpark PART 3 11 ¦ Large datasets in the cloud with Amazon Web Services and S3 12 ¦ MapReduce in the cloud with Amazon's Elastic MapReduce},
  googlebooks = {XDszEAAAQBAJ},
  isbn = {978-1-63835-036-1},
  langid = {english}
}
