@book{gopalan_cloud_2022,
  title = {The {{Cloud Data Lake}}: {{A Guide}} to {{Building Robust Cloud Data Architecture}}},
  shorttitle = {The {{Cloud Data Lake}}},
  author = {Gopalan, Rukmani},
  year = {2022},
  month = dec,
  publisher = {O'Reilly Media, Inc.},
  abstract = {More organizations than ever understand the importance of data lake architectures for deriving value from their data. Building a robust, scalable, and performant data lake remains a complex proposition, however, with a buffet of tools and options that need to work together to provide a seamless end-to-end pipeline from data to insights.This book provides a concise yet comprehensive overview on the setup, management, and governance of a cloud data lake. Author Rukmani Gopalan, a product management leader and data enthusiast, guides data architects and engineers through the major aspects of working with a cloud data lake, from design considerations and best practices to data format optimizations, performance optimization, cost management, and governance.Learn the benefits of a cloud-based big data strategy for your organizationGet guidance and best practices for designing performant and scalable data lakesExamine architecture and design choices, and data governance principles and strategiesBuild a data strategy that scales as your organizational and business needs increaseImplement a scalable data lake in the cloudUse cloud-based advanced analytics to gain more value from your data},
  googlebooks = {jkuhEAAAQBAJ},
  isbn = {978-1-0981-1654-5},
  langid = {english}
}

@book{kleppmann_designing_2017,
  title = {Designing {{Data-Intensive Applications}}: {{The Big Ideas Behind Reliable}}, {{Scalable}}, and {{Maintainable Systems}}},
  shorttitle = {Designing {{Data-Intensive Applications}}},
  author = {Kleppmann, Martin},
  year = {2017},
  month = mar,
  publisher = {"O'Reilly Media, Inc."},
  abstract = {Data is at the center of many challenges in system design today. Difficult issues need to be figured out, such as scalability, consistency, reliability, efficiency, and maintainability. In addition, we have an overwhelming variety of tools, including relational databases, NoSQL datastores, stream or batch processors, and message brokers. What are the right choices for your application? How do you make sense of all these buzzwords?In this practical and comprehensive guide, author Martin Kleppmann helps you navigate this diverse landscape by examining the pros and cons of various technologies for processing and storing data. Software keeps changing, but the fundamental principles remain the same. With this book, software engineers and architects will learn how to apply those ideas in practice, and how to make full use of data in modern applications.Peer under the hood of the systems you already use, and learn how to use and operate them more effectivelyMake informed decisions by identifying the strengths and weaknesses of different toolsNavigate the trade-offs around consistency, scalability, fault tolerance, and complexityUnderstand the distributed systems research upon which modern databases are builtPeek behind the scenes of major online services, and learn from their architectures},
  googlebooks = {p1heDgAAQBAJ},
  isbn = {978-1-4919-0310-0},
  langid = {english}
}

@book{leskovec_mining_2014,
  title = {Mining of {{Massive Datasets}}},
  author = {Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David},
  year = {2014},
  month = nov,
  publisher = {Cambridge University Press},
  abstract = {Written by leading authorities in database and Web technologies, this book is essential reading for students and practitioners alike. The popularity of the Web and Internet commerce provides many extremely large datasets from which information can be gleaned by data mining. This book focuses on practical algorithms that have been used to solve key problems in data mining and can be applied successfully to even the largest datasets. It begins with a discussion of the map-reduce framework, an important tool for parallelizing algorithms automatically. The authors explain the tricks of locality-sensitive hashing and stream processing algorithms for mining data that arrives too fast for exhaustive processing. Other chapters cover the PageRank idea and related tricks for organizing the Web, the problems of finding frequent itemsets and clustering. This second edition includes new and extended coverage on social networks, machine learning and dimensionality reduction.},
  googlebooks = {16YaBQAAQBAJ},
  isbn = {978-1-107-07723-2},
  langid = {english}
}

@misc{loukides_what_2010,
  title = {What Is Data Science?},
  author = {Loukides, Mike},
  year = {2010},
  month = jun,
  journal = {O'Reilly Media},
  url = {https://www.oreilly.com/radar/what-is-data-science/},
  urldate = {2025-09-02},
  abstract = {The future belongs to the companies and people that turn data into products.},
  langid = {american},
  file = {/Users/jpj/Zotero/storage/A9NMGBE5/what-is-data-science.html}
}

@article{mell_nist_2011,
  title = {The {{NIST Definition}} of {{Cloud Computing}}},
  author = {Mell, Peter and Grance, Timothy},
  year = {2011},
  journal = {National Institute of Standards and Technology, Special Publication},
  volume = {800},
  number = {2011},
  pages = {145},
  url = {https://nvlpubs.nist.gov/nistpubs/legacy/sp/nistspecialpublication800-145.pdf}
}

@inproceedings{raasveldt_duckdb_2019,
  title = {{{DuckDB}}: An {{Embeddable Analytical Database}}},
  shorttitle = {{{DuckDB}}},
  booktitle = {Proceedings of the 2019 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Raasveldt, Mark and M{\"u}hleisen, Hannes},
  year = {2019},
  month = jun,
  series = {{{SIGMOD}} '19},
  pages = {1981--1984},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3299869.3320212},
  url = {https://doi.org/10.1145/3299869.3320212},
  urldate = {2025-09-13},
  abstract = {The immense popularity of SQLite shows that there is a need for unobtrusive in-process data management solutions. However, there is no such system yet geared towards analytical workloads. We demonstrate DuckDB, a novel data management system designed to execute analytical SQL queries while embedded in another process. In our demonstration, we pit DuckDB against other data management solutions to showcase its performance in the embedded analytics scenario. DuckDB is available as Open Source software under a permissive license.},
  isbn = {978-1-4503-5643-5},
  file = {/Users/jpj/Zotero/storage/P4EFUVN7/Raasveldt and MÃ¼hleisen - 2019 - DuckDB an Embeddable Analytical Database.pdf}
}

@book{reis_fundamentals_2022,
  title = {Fundamentals of {{Data Engineering}}: {{Plan}} and {{Build Robust Data Systems}}},
  shorttitle = {Fundamentals of {{Data Engineering}}},
  author = {Reis, Joe and Housley, Matt},
  year = {2022},
  month = jun,
  publisher = {O'Reilly Media, Inc.},
  abstract = {Data engineering has grown rapidly in the past decade, leaving many software engineers, data scientists, and analysts looking for a comprehensive view of this practice. With this practical book, you'll learn how to plan and build systems to serve the needs of your organization and customers by evaluating the best technologies available through the framework of the data engineering lifecycle. Authors Joe Reis and Matt Housley walk you through the data engineering lifecycle and show you how to stitch together a variety of cloud technologies to serve the needs of downstream data consumers. You'll understand how to apply the concepts of data generation, ingestion, orchestration, transformation, storage, and governance that are critical in any data environment regardless of the underlying technology. This book will help you: Get a concise overview of the entire data engineering landscapeAssess data engineering problems using an end-to-end framework of best practicesCut through marketing hype when choosing data technologies, architecture, and processesUse the data engineering lifecycle to design and build a robust architectureIncorporate data governance and security across the data engineering lifecycle},
  googlebooks = {26d2EAAAQBAJ},
  isbn = {978-1-0981-0825-0},
  langid = {english}
}

@book{topol_inmemory_2024,
  title = {In-{{Memory Analytics}} with {{Apache Arrow}}},
  shorttitle = {In-{{Memory Analytics}} with {{Apache Arrow}}},
  author = {Topol, Matthew and McKinney, Wes},
  year = {2024},
  month = sep,
  publisher = {Packt Publishing Ltd},
  abstract = {Harness the power of Apache Arrow to optimize tabular data processing and develop robust, high-performance data systems with its standardized, language-independent columnar memory formatKey FeaturesExplore Apache Arrow's data types and integration with pandas, Polars, and ParquetWork with Arrow libraries such as Flight SQL, Acero compute engine, and Dataset APIs for tabular dataEnhance and accelerate machine learning data pipelines using Apache Arrow and its subprojectsPurchase of the print or Kindle book includes a free PDF eBookBook DescriptionApache Arrow is an open source, columnar in-memory data format designed for efficient data processing and analytics. This book harnesses the author's 15 years of experience to show you a standardized way to work with tabular data across various programming languages and environments, enabling high-performance data processing and exchange. This updated second edition gives you an overview of the Arrow format, highlighting its versatility and benefits through real-world use cases. It guides you through enhancing data science workflows, optimizing performance with Apache Parquet and Spark, and ensuring seamless data translation. You'll explore data interchange and storage formats, and Arrow's relationships with Parquet, Protocol Buffers, FlatBuffers, JSON, and CSV. You'll also discover Apache Arrow subprojects, including Flight, SQL, Database Connectivity, and nanoarrow. You'll learn to streamline machine learning workflows, use Arrow Dataset APIs, and integrate with popular analytical data systems such as Snowflake, Dremio, and DuckDB. The latter chapters provide real-world examples and case studies of products powered by Apache Arrow, providing practical insights into its applications. By the end of this book, you'll have all the building blocks to create efficient and powerful analytical services and utilities with Apache Arrow.What you will learnUse Apache Arrow libraries to access data files, both locally and in the cloudUnderstand the zero-copy elements of the Apache Arrow formatImprove the read performance of data pipelines by memory-mapping Arrow filesProduce and consume Apache Arrow data efficiently by sharing memory with the C APILeverage the Arrow compute engine, Acero, to perform complex operationsCreate Arrow Flight servers and clients for transferring data quicklyBuild the Arrow libraries locally and contribute to the communityWho this book is forThis book is for developers, data engineers, and data scientists looking to explore the capabilities of Apache Arrow from the ground up. Whether you're building utilities for data analytics and query engines, or building full pipelines with tabular data, this book can help you out regardless of your preferred programming language. A basic understanding of data analysis concepts is needed, but not necessary. Code examples are provided using C++, Python, and Go throughout the book.},
  googlebooks = {G0EgEQAAQBAJ},
  isbn = {978-1-83546-968-2},
  langid = {english}
}

@book{white_hadoop_2015,
  title = {Hadoop: {{The Definitive Guide}}},
  shorttitle = {Hadoop},
  author = {White, Tom E.},
  year = {2015},
  publisher = {O'Reilly Media, Inc.},
  abstract = {Get ready to unlock the power of your data. With the fourth edition of this comprehensive guide, you'll learn how to build and maintain reliable, scalable, distributed systems with Apache Hadoop. This book is ideal for programmers looking to analyze datasets of any size, and for administrators who want to set up and run Hadoop clusters. Using Hadoop 2 exclusively, author Tom White presents new chapters on YARN and several Hadoop-related projects such as Parquet, Flume, Crunch, and Spark. You'll learn about recent changes to Hadoop, and explore new case studies on Hadoop's role in healthcare systems and genomics data processing. Learn fundamental components such as MapReduce, HDFS, and YARNExplore MapReduce in depth, including steps for developing applications with itSet up and maintain a Hadoop cluster running HDFS and MapReduce on YARNLearn two data formats: Avro for data serialization and Parquet for nested dataUse data ingestion tools such as Flume (for streaming data) and Sqoop (for bulk data transfer)Understand how high-level data processing tools like Pig, Hive, Crunch, and Spark work with HadoopLearn the HBase distributed database and the ZooKeeper distributed configuration service.},
  googlebooks = {MhqkBwAAQBAJ},
  isbn = {978-1-4919-0171-7},
  langid = {english}
}
