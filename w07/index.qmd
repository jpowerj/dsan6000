---
title: "Week 7: Spark DataFrames and Spark SQL"
subtitle: "{{< var course.slides-subtitle >}}"
author: "{{< var course.author >}}"
institute: "{{< var course.institute >}}"
date: 2025-10-06
date-format: full
lecnum: 7
categories:
  - "Class Sessions"
crossref:
  fig-title: Fig
cache: true
format:
  revealjs:
    output-file: "slides.html"
    html-math-method: mathjax
    slide-number: true
    scrollable: true
    link-external-icon: true
    link-external-newwindow: true
    footer: "{{< var weeks.7.footer >}}"
    include-in-header:
      text: "<link rel='stylesheet' href='https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css'><link rel='stylesheet' type='text/css' href='https://cdn.jsdelivr.net/gh/dreampulse/computer-modern-web-font@master/fonts.css'><link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css' rel='stylesheet' integrity='sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH' crossorigin='anonymous'><script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js' integrity='sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz' crossorigin='anonymous'></script>"
    theme: [default, "../dsan-globals/jjquarto.scss"]
    revealjs-plugins:
      - simplemenu
    simplemenu:
      flat: true
      barhtml:
        header: "<div class='menubar'><span style='position: absolute; left: 8; padding-left: 8px;'><a href='./index.html'>&larr; Return to Notes</a></span><ul class='menu'></ul></div>"
      scale: 0.5
  html:
    output-file: "index.html"
    html-math-method: mathjax
    df-print: kable
---

# Spark: a Unified Engine {data-stack-name="Spark Connectors"}

## Connected and extensible

![](images/spark-connectors.png){fig-align="center"}

<!-- 
## Caching and Persistence

By default, RDDs are recomputed every time you run an action on them. This can be expensive (in time) if you need to use a dataset more than once.

**Spark allows you to control what is cached in memory.**

To tell spark to cache an object in memory, use `persist()` or `cache()`:

* `cache():` is a shortcut for using default storage level, which is memory only
* `persist():` can be customized to other ways to persist data (including both memory and/or disk)

```{{python}}
# caches error RDD in memory, but only after an action is run
errors = logs.filter(lambda x: "error" in x and "2019-12" in x).cache()
``` -->

## Review of PySparkSQL Cheatsheet

[https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf)


## `collect` **CAUTION**

::: {.imgcenter}
<img src="images/collect-warning.png">
:::

<!-- ## Review of htop

::: {.imgcenter}
<img src="spark_udfs/img/htop.jpg">
::: -->

<!-- ## htop top section explanation

::: {.imgcenter}
<img src="spark_udfs/img/htop-top.jpg">
::: -->

<!-- ## htop bottom section explanation

::: {.imgcenter}
<img src="spark_udfs/img/htop-bottom.jpg">
:::

[https://codeahoy.com/2017/01/20/hhtop-explained-visually/](https://codeahoy.com/2017/01/20/hhtop-explained-visually/) -->

## Spark Diagnostic UI: Understanding how the cluster is running your job

Spark Application UI shows important facts about you Spark job:

* Event timeline for each stage of your work
* Directed acyclical graph (DAG) of your job
* Spark job history
* Status of Spark executors
* Physical / logical plans for any SQL queries

#### Tool to confirm you are getting the horizontal scaling that you need!

Adapted from [AWS Glue Spark UI docs](https://docs.aws.amazon.com/glue/latest/dg/monitor-spark-ui.html)
and [Spark UI docs](https://spark.apache.org/docs/latest/web-ui.html)

## Spark UI - Event timeline

![](images/spark-ui-timeline.png){fig-align="center"}

## Spark UI - DAG

![](images/spark-ui-dag.png){fig-align="center"}

## Spark UI - Job History

![](images/spark-ui-jobhistory.png){fig-align="center"}

## Spark UI - Executors

![](images/spark-ui-executors.png){fig-align="center"}

## Spark UI - SQL

![](images/spark-ui-sql.png){fig-align="center"}

# PySpark User Defined Functions

## UDF Workflow

![](images/spark-udf.png){fig-align="center"}

## UDF Code Structure

### Clear input - a single row of data with one or more columns used

### Function - some work written in python that process the input using python syntax. No PySpark needed!

### Clear output - output with a scoped data type 

## UDF Example

Problem: make a new column with ages for adults-only

```
+-------+--------------+
|room_id|   guests_ages|
+-------+--------------+
|      1|  [18, 19, 17]|
|      2|   [25, 27, 5]|
|      3|[34, 38, 8, 7]|
+-------+--------------+
```

Adapted from [UDFs in Spark](https://blog.damavis.com/en/avoiding-udfs-in-apache-spark/)

## UDF Code Solution

```{{python}}
from pyspark.sql.functions import udf, col

@udf("array<integer>")
   def filter_adults(elements):
   return list(filter(lambda x: x >= 18, elements))

# alternatively
from pyspark.sql.types IntegerType, ArrayType
@udf(returnType=ArrayType(IntegerType()))
def filter_adults(elements):
   return list(filter(lambda x: x >= 18, elements))
```

```
+-------+----------------+------------+
|room_id| guests_ages    | adults_ages|
+-------+----------------+------------+
| 1     | [18, 19, 17]   |    [18, 19]|
| 2     | [25, 27, 5]    |    [25, 27]|
| 3     | [34, 38, 8, 7] |    [34, 38]|
| 4     |[56, 49, 18, 17]|[56, 49, 18]|
+-------+----------------+------------+
```

## Alternative to Spark UDF

```{{python}}
# Spark 3.1
from pyspark.sql.functions import col, filter, lit

df.withColumn('adults_ages',
              filter(col('guests_ages'), lambda x: x >= lit(18))).show()
```

## Another UDF Example

* Separate function definition form

```{{python}}
from pyspark.sql.functions import udf
from pyspark.sql.types import LongType

# define the function that can be tested locally
def squared(s):
  return s * s

# wrap the function in udf for spark and define the output type
squared_udf = udf(squared, LongType())

# execute the udf
df = spark.table("test")
display(df.select("id", squared_udf("id").alias("id_squared")))
```

* Single function definition form

```{{python}}
from pyspark.sql.functions import udf
@udf("long")
def squared_udf(s):
  return s * s
df = spark.table("test")
display(df.select("id", squared_udf("id").alias("id_squared")))
```

## Can also refer to a UDF in SQL

```{{sql}}
spark.udf.register("squaredWithPython", squared)
select id, squaredWithPython(id) as id_squared from test
```

* Consider all the corner cases
* Where could the data be null or an unexpected value
* Leverage python control structure to handle corner cases

[source](https://docs.databricks.com/en/udf/python.html)

## UDF Speed Comparison

:::: {.columns}
::: {.column width="53%"}

![](images/spark-udf-speed.png){fig-align="center"}

:::
::: {.column width="45%"}
Costs:

* Serialization/deserialization (think pickle files)
* Data movement between JVM and Python
* Less Spark optimization possible

Other ways to make your Spark jobs faster [source](https://sparkbyexamples.com/spark/spark-performance-tuning/):

* Cache/persist your data into memory
* Using Spark DataFrames over Spark RDDs
* Using Spark SQL functions before jumping into UDFs
* Save to serialized data formats like Parquet

:::
::::

## Pandas UDF

From PySpark docs - Pandas UDFs are user defined functions that are executed by Spark using Arrow to transfer data and Pandas to work with the data, which allows vectorized operations. A Pandas UDF is defined using the pandas_udf as a decorator or to wrap the function, and no additional configuration is required. A Pandas UDF behaves as a regular PySpark function API in general.

```{{python}}
@pandas_udf("string")
def to_upper(s: pd.Series) -> pd.Series:
    return s.str.upper()

df = spark.createDataFrame([("John Doe",)], ("name",))
df.select(to_upper("name")).show()
+--------------+
|to_upper(name)|
+--------------+
|      JOHN DOE|
+--------------+
```

## Another example

```{{python}}
@pandas_udf("first string, last string")
def split_expand(s: pd.Series) -> pd.DataFrame:
    return s.str.split(expand=True)


df = spark.createDataFrame([("John Doe",)], ("name",))
df.select(split_expand("name")).show()
+------------------+
|split_expand(name)|
+------------------+
|       [John, Doe]|
+------------------+
```

[https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.pandas_udf.html](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.pandas_udf.html)

## Scalar Pandas UDFs

* Vectorizing scalar operations - one plus one
* Pandas UDF needs to have same size input and output series

#### UDF Form

```{{python}}
from pyspark.sql.functions import udf

# Use udf to define a row-at-a-time udf
@udf('double')
# Input/output are both a single double value
def plus_one(v):
      return v + 1

df.withColumn('v2', plus_one(df.v))
```

#### Pandas UDF Form - **faster vectorized form**

```{{python}}
from pyspark.sql.functions import pandas_udf, PandasUDFType

# Use pandas_udf to define a Pandas UDF
@pandas_udf('double', PandasUDFType.SCALAR)
# Input/output are both a pandas.Series of doubles

def pandas_plus_one(v):
    return v + 1

df.withColumn('v2', pandas_plus_one(df.v))
```

[source](https://www.databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html)

## Grouped Map Pandas UDFs

* Split, apply, combine using Pandas syntax

```{{python}}
@pandas_udf(df.schema, PandasUDFType.GROUPED_MAP)
# Input/output are both a pandas.DataFrame
def subtract_mean(pdf):
    return pdf.assign(v=pdf.v - pdf.v.mean())

df.groupby('id').apply(subtract_mean)
```

## Comparison of Scalar and Grouped Map Pandas UDFs

:::: {.columns}
::: {.column width="53%"}
* Input of the user-defined function:

   * Scalar: pandas.Series
   * Grouped map: pandas.DataFrame

* Output of the user-defined function:

   * Scalar: pandas.Series
   * Grouped map: pandas.DataFrame

* Grouping semantics:

   * Scalar: no grouping semantics
   * Grouped map: defined by "groupby" clause

* Output size:

   * Scalar: same as input size
   * Grouped map: any size


:::
::: {.column width="45%"}

![](images/pandas-udf-comparison.png){fig-align="center"}

:::
::::


# Lab Time! {data-stack-name="Lab"}

## References

::: {#refs}
:::
