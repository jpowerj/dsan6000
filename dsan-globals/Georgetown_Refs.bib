@article{bayes_essay_1763,
  title = {An Essay towards Solving a Problem in the Doctrine of Chances. {{By}} the Late {{Rev}}. {{Mr}}. {{Bayes}}, Communicated by {{Mr}}. {{Price}}, in a Letter to {{John Canton}}},
  author = {Bayes, Thomas},
  year = {1763},
  journal = {Philosophical transactions of the Royal Society of London},
  number = {53},
  pages = {370--418},
  publisher = {{The Royal Society London}}
}

@article{chernozhukov_double_2018,
  title = {Double/Debiased Machine Learning for Treatment and Structural Parameters},
  author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
  year = {2018},
  month = feb,
  journal = {The Econometrics Journal},
  volume = {21},
  number = {1},
  pages = {C1-C68},
  issn = {1368-4221},
  doi = {10.1111/ectj.12097},
  urldate = {2023-04-10},
  abstract = {We revisit the classic semi-parametric problem of inference on a low-dimensional parameter \texttheta 0 in the presence of high-dimensional nuisance parameters {$\eta$}0. We depart from the classical setting by allowing for {$\eta$}0 to be so high-dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate {$\eta$}0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high-dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating {$\eta$}0 cause a heavy bias in estimators of \texttheta 0 that are obtained by naively plugging ML estimators of {$\eta$}0 into estimating equations for \texttheta 0. This bias results in the naive estimator failing to be N-1/2 consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest \texttheta 0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman-orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate \texttheta 0; (2) making use of cross-fitting, which provides an efficient form of data-splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an N-1/2-neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.},
  file = {C\:\\Users\\jpjac\\Zotero\\storage\\D96NFBCV\\Chernozhukov et al. - 2018 - Doubledebiased machine learning for treatment and.pdf}
}

@book{degroot_probability_2013,
  title = {Probability and {{Statistics}}},
  author = {DeGroot, Morris H. and Schervish, Mark J.},
  year = {2013},
  publisher = {{Pearson Education}},
  abstract = {The revision of this well-respected text presents a balanced approach of the classical and Bayesian methods and now includes a chapter on simulation (including Markov chain Monte Carlo and the Bootstrap), coverage of residual analysis in linear models, and many examples using real data. Probability \& Statistics, Fourth Edition, was written for a one- or two-semester probability and statistics course. This course is offered primarily at four-year institutions and taken mostly by sophomore and junior level students majoring in mathematics or statistics. Calculus is a prerequisite, and a familiarity with the concepts and elementary properties of vectors and matrices is a plus.},
  googlebooks = {hIPkngEACAAJ},
  isbn = {978-1-292-02504-9},
  langid = {english}
}

@book{gelman_data_2007,
  title = {Data {{Analysis Using Regression}} and {{Multilevel}}/{{Hierarchical Models}}},
  author = {Gelman, Andrew and Hill, Jennifer},
  year = {2007},
  publisher = {{Cambridge University Press}},
  abstract = {Data Analysis Using Regression and Multilevel/Hierarchical Models is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors' own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout. Author resource page: http://www.stat.columbia.edu/\textasciitilde gelman/arm/},
  googlebooks = {lV3DIdV0F9AC},
  isbn = {978-0-521-68689-1},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General,Political Science / General,{Psychology / Assessment, Testing \& Measurement},Social Science / Research}
}

@article{godambe_foundations_1970,
  title = {Foundations of {{Survey-Sampling}}},
  author = {Godambe, V. P.},
  year = {1970},
  month = feb,
  journal = {The American Statistician},
  volume = {24},
  number = {1},
  pages = {33--38},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.1970.10477175},
  urldate = {2023-05-16},
  abstract = {Dedicated to the late Professor J. B. S. Haldane who brought to my attention the following very significant story from the ancient Indian epic Mahabharat (Nala\textemdash Damayanti Akhy[abar]n): The king lost his way in a jungle and was required to spend the night in a tree. The next day he told some fellow traveller that the total number of leaves on the tree were ``so many''. On being challenged as to whether he counted all the leaves he replied; ``No, but I counted leaves on a few branches of the tree and I know the science of die throwing''. (I can vouch for accurateness of the reproduction only in the essential respects.)}
}

@book{hacking_emergence_1975,
  title = {The {{Emergence}} of {{Probability}}: {{A Philosophical Study}} of {{Early Ideas}} about {{Probability}}, {{Induction}} and {{Statistical Inference}}},
  shorttitle = {The {{Emergence}} of {{Probability}}},
  author = {Hacking, Ian},
  year = {1975},
  publisher = {{Cambridge University Press}},
  abstract = {Historical records show that there was no real concept of probability in Europe before the mid-seventeenth century, although the use of dice and other randomizing objects was commonplace. Ian Hacking presents a philosophical critique of early ideas about probability, induction, and statistical inference and the growth of this new family of ideas in the fifteenth, sixteenth, and seventeenth centuries. Hacking invokes a wide intellectual framework involving the growth of science, economics, and the theology of the period. He argues that the transformations that made it possible for probability concepts to emerge have constrained all subsequent development of probability theory and determine the space within which philosophical debate on the subject is still conducted. First published in 1975, this edition includes an introduction that contextualizes his book in light of developing philosophical trends. Ian Hacking is the winner of the Holberg International Memorial Prize 2009.},
  googlebooks = {ja00AAAAQBAJ},
  isbn = {978-1-107-26885-2},
  langid = {english},
  keywords = {History / Social History,Mathematics / History \& Philosophy,Science / Philosophy \& Social Aspects}
}

@article{hansen_large_1982,
  title = {Large Sample Properties of Generalized Method of Moments Estimators},
  author = {Hansen, Lars Peter},
  year = {1982},
  journal = {Econometrica: Journal of the econometric society},
  pages = {1029--1054},
  publisher = {{JSTOR}}
}

@book{hastie_elements_2013,
  title = {The {{Elements}} of {{Statistical Learning}}: {{Data Mining}}, {{Inference}}, and {{Prediction}}},
  shorttitle = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2013},
  month = nov,
  publisher = {{Springer Science \& Business Media}},
  abstract = {During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression \& path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.},
  googlebooks = {yPfZBwAAQBAJ},
  isbn = {978-0-387-21606-5},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / General,Computers / Database Administration \& Management,Computers / Mathematical \& Statistical Software,Mathematics / Discrete Mathematics,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes,Science / Life Sciences / Biology,Science / Life Sciences / General}
}

@book{immerwahr_how_2019,
  title = {How to {{Hide}} an {{Empire}}: {{A History}} of the {{Greater United States}}},
  shorttitle = {How to {{Hide}} an {{Empire}}},
  author = {Immerwahr, Daniel},
  year = {2019},
  publisher = {{Farrar, Straus and Giroux}},
  abstract = {Named one of the ten best books of the year by the Chicago TribuneA Publishers Weekly best book of 2019 | A 2019 NPR Staff PickA pathbreaking history of the United States' overseas possessions and the true meaning of its empireWe are familiar with maps that outline all fifty states. And we are also familiar with the idea that the United States is an ``empire,'' exercising power around the world. But what about the actual territories\textemdash the islands, atolls, and archipelagos\textemdash this country has governed and inhabited?In How to Hide an Empire, Daniel Immerwahr tells the fascinating story of the United States outside the United States. In crackling, fast-paced prose, he reveals forgotten episodes that cast American history in a new light. We travel to the Guano Islands, where prospectors collected one of the nineteenth century's most valuable commodities, and the Philippines, site of the most destructive event on U.S. soil. In Puerto Rico, Immerwahr shows how U.S. doctors conducted grisly experiments they would never have conducted on the mainland and charts the emergence of independence fighters who would shoot up the U.S. Congress.In the years after World War II, Immerwahr notes, the United States moved away from colonialism. Instead, it put innovations in electronics, transportation, and culture to use, devising a new sort of influence that did not require the control of colonies. Rich with absorbing vignettes, full of surprises, and driven by an original conception of what empire and globalization mean today, How to Hide an Empire is a major and compulsively readable work of history.},
  googlebooks = {GYtbDwAAQBAJ},
  isbn = {978-0-374-71512-0},
  langid = {english},
  keywords = {History / Military / United States,History / United States / General,Political Science / Colonialism \& Post-Colonialism}
}

@misc{kingma_adam_2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  number = {arXiv:1412.6980},
  eprint = {1412.6980},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1412.6980},
  urldate = {2023-04-10},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\jpjac\\Zotero\\storage\\UP3UQYS7\\Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;C\:\\Users\\jpjac\\Zotero\\storage\\TLHMS66C\\1412.html}
}

@book{knight_risk_1921,
  title = {Risk, {{Uncertainty}} and {{Profit}}},
  author = {Knight, Frank H.},
  year = {1921},
  publisher = {{Houghton}},
  abstract = {Risk, Uncertainty and Profit by Frank Hyneman Knight, first published in 1921, is a rare manuscript, the original residing in one of the great libraries of the world. This book is a reproduction of that original, which has been scanned and cleaned by state-of-the-art publishing tools for better readability and enhanced appreciation. Restoration Editors' mission is to bring long out of print manuscripts back to life. Some smudges, annotations or unclear text may still exist, due to permanent damage to the original work. We believe the literary significance of the text justifies offering this reproduction, allowing a new generation to appreciate it.},
  googlebooks = {jKoJAAAAIAAJ},
  isbn = {978-1-5485-6350-9},
  langid = {english}
}

@book{mcelreath_statistical_2020,
  title = {Statistical {{Rethinking}}: {{A Bayesian Course}} with {{Examples}} in {{R}} and {{STAN}}},
  shorttitle = {Statistical {{Rethinking}}},
  author = {McElreath, Richard},
  year = {2020},
  month = mar,
  publisher = {{CRC Press}},
  abstract = {Statistical Rethinking: A Bayesian Course with Examples in R and Stan builds your knowledge of and confidence in making inferences from data. Reflecting the need for scripting in today's model-based statistics, the book pushes you to perform step-by-step calculations that are usually automated. This unique computational approach ensures that you understand enough of the details to make reasonable choices and interpretations in your own modeling work.  The text presents causal inference and generalized linear multilevel models from a simple Bayesian perspective that builds on information theory and maximum entropy. The core material ranges from the basics of regression to advanced multilevel models. It also presents measurement error, missing data, and Gaussian process models for spatial and phylogenetic confounding.  The second edition emphasizes the directed acyclic graph (DAG) approach to causal inference, integrating DAGs into many examples. The new edition also contains new material on the design of prior distributions, splines, ordered categorical predictors, social relations models, cross-validation, importance sampling, instrumental variables, and Hamiltonian Monte Carlo. It ends with an entirely new chapter that goes beyond generalized linear modeling, showing how domain-specific scientific models can be built into statistical analyses.  Features   Integrates working code into the main text   Illustrates concepts through worked data analysis examples   Emphasizes understanding assumptions and how assumptions are reflected in code   Offers more detailed explanations of the mathematics in optional sections   Presents examples of using the dagitty R package to analyze causal graphs   Provides the rethinking R package on the author's website and on GitHub},
  googlebooks = {FuLWDwAAQBAJ},
  isbn = {978-0-429-64231-9},
  langid = {english},
  keywords = {Mathematics / General,Mathematics / Probability \& Statistics / General,Science / Life Sciences / Biological Diversity}
}

@book{oneil_weapons_2016,
  title = {Weapons of {{Math Destruction}}: {{How Big Data Increases Inequality}} and {{Threatens Democracy}}},
  shorttitle = {Weapons of {{Math Destruction}}},
  author = {O'Neil, Cathy},
  year = {2016},
  publisher = {{Crown}},
  abstract = {Longlisted for the National Book Award | New York Times Bestseller  A former Wall Street quant sounds an alarm on the mathematical models that pervade modern life and threaten to rip apart our social fabric.  We live in the age of the algorithm. Increasingly, the decisions that affect our lives--where we go to school, whether we get a car loan, how much we pay for health insurance--are being made not by humans, but by mathematical models. In theory, this should lead to greater fairness: Everyone is judged according to the same rules, and bias is eliminated.  But as Cathy O'Neil reveals in this urgent and necessary book, the opposite is true. The models being used today are opaque, unregulated, and uncontestable, even when they're wrong. Most troubling, they reinforce discrimination: If a poor student can't get a loan because a lending model deems him too risky (by virtue of his zip code), he's then cut off from the kind of education that could pull him out of poverty, and a vicious spiral ensues. Models are propping up the lucky and punishing the downtrodden, creating a "toxic cocktail for democracy." Welcome to the dark side of Big Data.  Tracing the arc of a person's life, O'Neil exposes the black box models that shape our future, both as individuals and as a society. These "weapons of math destruction" score teachers and students, sort r\'esum\'es, grant (or deny) loans, evaluate workers, target voters, set parole, and monitor our health.  O'Neil calls on modelers to take more responsibility for their algorithms and on policy makers to regulate their use. But in the end, it's up to us to become more savvy about the models that govern our lives. This important book empowers us to ask the tough questions, uncover the truth, and demand change.},
  isbn = {978-0-553-41881-1},
  langid = {english},
  keywords = {BUSINESS \& ECONOMICS / Statistics,SOCIAL SCIENCE / Privacy \& Surveillance}
}

@book{schelling_micromotives_1978,
  title = {Micromotives and {{Macrobehavior}}},
  author = {Schelling, Thomas C.},
  year = {1978},
  publisher = {{Norton}},
  abstract = {Micromotives and Macrobehavior deals with all involve systems of behavior where a person reacting, responding, and adapting to his surroundings fails to perceive, or doesn't care, how his actions combine with the actions of others to produce unanticipated results.},
  googlebooks = {ouJzQgAACAAJ},
  isbn = {978-0-393-05701-0},
  langid = {english}
}

@article{schmidinger_exploring_2019,
  title = {Exploring {{Neural Turing Machines}}},
  shorttitle = {Niklas {{Schmidinger}}},
  author = {Schmidinger, Niklas},
  year = {2019},
  month = dec,
  journal = {Johannes Kepler University Blog},
  urldate = {2023-04-10},
  abstract = {End-to-end differentiable memory through attention mechanisms.},
  file = {C\:\\Users\\jpjac\\Zotero\\storage\\WKDFP75M\\2019-12-25-neural-turing-machines.html}
}

@book{silver_signal_2012,
  title = {The {{Signal}} and the {{Noise}}: {{Why So Many Predictions Fail--but Some Don}}'t},
  shorttitle = {The {{Signal}} and the {{Noise}}},
  author = {Silver, Nate},
  year = {2012},
  publisher = {{Penguin}},
  abstract = {UPDATED FOR 2020 WITH A NEW PREFACE BY NATE SILVER"One of the more momentous books of the decade." \textemdash The New York Times Book ReviewNate Silver built an innovative system for predicting baseball performance, predicted the 2008 election within a hair's breadth, and became a national sensation as a blogger\textemdash all by the time he was thirty. He solidified his standing as the nation's foremost political forecaster with his near perfect prediction of the 2012 election.~Silver is the founder and editor in chief~of the website FiveThirtyEight.~ ~Drawing on his own groundbreaking work, Silver examines the world of prediction, investigating how we can distinguish a true signal from a universe of noisy data. Most predictions fail, often at great cost to society, because most of us have a poor understanding of probability and uncertainty. Both experts and laypeople mistake more confident predictions for more accurate ones. But overconfidence is often the reason for failure. If our appreciation of uncertainty improves, our predictions can get better too. This is the ``prediction paradox'': The more humility we have about our ability to make predictions, the more successful we can be in planning for the future.In keeping with his own aim to seek truth from data, Silver visits the most successful forecasters in a range of areas, from hurricanes to baseball to global pandemics, from the poker table to the stock market, from Capitol Hill to the NBA. He explains and evaluates how these forecasters think and what bonds they share. What lies behind their success? Are they good\textemdash or just lucky? What patterns have they unraveled? And are their forecasts really right? He explores unanticipated commonalities and exposes unexpected juxtapositions. And sometimes, it is not so much how good a prediction is in an absolute sense that matters but how good it is relative to the competition. In other cases, prediction is still a very rudimentary\textemdash and dangerous\textemdash science.Silver observes that the most accurate forecasters tend to have a superior command of probability, and they tend to be both humble and hardworking. They distinguish the predictable from the unpredictable, and they notice a thousand little details that lead them closer to the truth. Because of their appreciation of probability, they can distinguish the signal from the noise.With everything from the health of the global economy to our ability to fight terrorism dependent on the quality of our predictions, Nate Silver's insights are an essential read.},
  googlebooks = {ekWLDQAAQBAJ},
  isbn = {978-0-14-312508-2},
  langid = {english},
  keywords = {Business \& Economics / Forecasting,Mathematics / Probability \& Statistics / General,Political Science / Political Process / Campaigns \& Elections}
}

@article{tibshirani_regression_1996,
  title = {Regression {{Shrinkage}} and {{Selection}} via the {{Lasso}}},
  author = {Tibshirani, Robert},
  year = {1996},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {58},
  number = {1},
  eprint = {2346178},
  eprinttype = {jstor},
  pages = {267--288},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  urldate = {2023-04-10},
  abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.}
}

@book{tukey_exploratory_1977,
  title = {Exploratory {{Data Analysis}}},
  author = {Tukey, John W.},
  year = {1977},
  publisher = {{Addison-Wesley Publishing Company}},
  address = {{New York}},
  abstract = {Scratching down numbers (stem-and-leaf); Schematic summaries (pictures and numbers); Easy re-expression; Effective comparison (including well-chosen expresion); Plots of relationship; Straightening out plots (using three points); Smoothing sequences; Optional sections for chapter 7; Parallel and wandering schematic plots; Delineations of batches of points; Using two-way analyses; Making two-way analyses; Advances fits; Three-way fits; Looking in two or more ways at batches of points; Counted fractions; Better smoothing; Counts in bin after bin; Product-ratio plots; Shapes of distribution; Mathematical distributions; Postscript.},
  googlebooks = {UT9dAAAAIAAJ},
  isbn = {978-0-201-07616-5},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General}
}
