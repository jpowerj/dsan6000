---
title: "Week 1: Course Overview"
subtitle: "{{< var course.slides-subtitle >}}"
author: "{{< var course.author >}}"
institute: "{{< var course.institute >}}"
date: 2025-08-28
date-format: full
lecnum: 1
categories:
  - "Class Sessions"
crossref:
  fig-title: Fig
# bibliography: "../_PPOL6805.bib"
cache: true
format:
  revealjs:
    output-file: "slides.html"
    html-math-method: mathjax
    slide-number: true
    scrollable: true
    link-external-icon: true
    link-external-newwindow: true
    footer: "{{< var weeks.1.footer >}}"
    include-in-header:
      text: "<link rel='stylesheet' href='https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css'><link rel='stylesheet' type='text/css' href='https://cdn.jsdelivr.net/gh/dreampulse/computer-modern-web-font@master/fonts.css'><link rel='preconnect' href='https://fonts.gstatic.com' crossorigin><link href='https://fonts.googleapis.com/css2?family=Honk&display=swap' rel='stylesheet'>"
    theme: [default, "../dsan-globals/jjquarto.scss"]
    revealjs-plugins:
      - simplemenu
    simplemenu:
      flat: true
      barhtml:
        header: "<div class='menubar'><span style='position: absolute; left: 8; padding-left: 8px;'><a href='./index.html'>&larr; Return to Notes</a></span><ul class='menu'></ul></div>"
      scale: 0.5
  html:
    output-file: "index.html"
    html-math-method: mathjax
    df-print: kable
---

::: {.content-visible unless-format="revealjs"}

<center class='mb-3'>
<a class="h2" href="./slides.html" target="_blank">Open slides in new tab &rarr;</a>
</center>

:::


## Agenda for today's session

1. Course and syllabus overview

1. Big Data Concepts
    1. Definition
    1. Challenges
    1. Approaches

1. Data Engineering

1. Introduction to `bash`
    1. Lab: Linux command line

# Course Overview {data-stack-name="Course Overview"}

## Bookmark these links! {.smaller}

* Course website: [{{< var course.url >}}]({{< var course.url >}})
* GitHub Organization for your deliverables: [{{< var course.github_org >}}]({{< var course.github_org >}})
* GitHub Classroom: [{{< var course.github_classroom >}}]({{< var course.github_classroom >}})
* Slack Workspace: **DSAN6000 Fall 2025** - [{{< var course.slack >}}]({{< var course.slack >}})
  * Join link: [{{< var course.slack_join >}}]({{< var course.slack_join >}})
* Instructors email: [{{< var course.email >}}](mailto::{{< var course.email >}})
* {{< var university.lms >}}: [{{< var course.lms_course_url >}}]({{< var course.lms_course_url >}})


::: {.callout-tip appearance="simple" icon=true }
These are also pinned on the Slack `main` channel
:::

## Instructional Team - Professors

* Amit Arora, [`aa1603@georgetown.edu`](mailto:aa1603@georgetown.edu)
* Jeff Jacobs, [`jj1088@georgetown.edu`](mailto:jj1088@georgetown.edu)

## Amit Arora, [`aa1603@georgetown.edu`](mailto:aa1603@georgetown.edu) {.smaller .title-12}

:::: {.columns} 
::: {.column width="75%"}

* Principal Solutions Architect - AI/ML at AWS
* Adjunct Professor at Georgetown University
* Multiple patents in telecommunications and applications of ML in telecommunications

Fun Facts

:::

::: {.column width="25%"}
![](./img/amit.jpeg 'Amit Arora')
:::
::::

* I am a self-published author https://blueberriesinmysalad.com/
* My book "Blueberries in my salad: my forever journey towards fitness & strength" is written as code in R and Markdown
* I love to read books about health and human performance, productivity, philosophy and <i>Mathematics for ML</i>. My reading list is [online](https://aarora79.github.io/my-reading-list/)!

## Jeff Jacobs, [`jj1088@georgetown.edu`](mailto:jj1088@georgetown.edu) {.smaller .title-12 .crunch-ul .crunch-img .crunch-title}

:::: {.columns} 
::: {.column width="75%"}

* Full-time Professor at Georgetown (DSAN and Public Policy)
* Background in Computational Social Science (Comp Sci MS &rarr; Political Economy PhD &rarr; Labor Econ Postdoc)

Fun Facts

* Used Apache Airflow daily for PhD projects! [(Example)](https://ieeexplore.ieee.org/document/9346539)

:::

::: {.column width="25%"}
![](./img/jeff.jpg 'Jeff Jacobs')
:::
::::

* Server admin for lab server &rarr; lab AWS account at Columbia (2015-2023) &rarr; new DSAN server (!) (2025-)
* Passion project 1: [Code for Palestine (2015-2022)](https://www.codedotx.org/code-for-palestine) &rarr; [YouthCode-Gaza (2023)](https://www.codedotx.org/youthcode-gaza) &rarr; [Ukraine Ministry of Digital Transformation (2024)](https://jjacobs.me/data-ethics-cdto/#/title-slide)
* Passion projects 2+3 [ðŸ¤“]: [Sample-based music production](https://open.spotify.com/track/43HqrCcK5FCkZG3mAQFmch?si=c6afba6dff394fe6), [web app frameworks](https://dbtskills.io/)
* Sleep disorder means lots of [reading](https://app.thestorygraph.com/profile/jpowerj) -- mainly history! -- at night
* Also teaching *PPOL6805 / DSAN 6750: GIS for Spatial Data Science* this semester

## Instructional Team - Teaching Assistants {.smaller .title-12}

* Binhui Chen, [`bc928@georgetown.edu`](mailto:bc928@georgetown.edu)
* Pranav Sudhir Patil, [`pp755@georgetown.edu`](mailto:pp755@georgetown.edu)
* Ofure Udabor, [`au195@georgetown.edu`](mailto:au195@georgetown.edu)
* Yifei Wu, [`yw924@georgetown.edu`](mailto:yw924@georgetown.edu)
* Naomi Yamaguchi, [`ny159@georgetown.edu`](mailto:ny159@georgetown.edu)
* Leqi Ying, [`ly290@georgetown.edu`](mailto:ly290@georgetown.edu)
* Xinyue (Monica) Zhang, [`xz646@georgetown.edu`](mailto:xz646@georgetown.edu)

## Binhui Chen, [`bc928@georgetown.edu`](mailto:bc928@georgetown.edu) {.smaller .title-12}

*(Lead TA for the course!)*

<center>

![](img/binhui.jpg)

</center>

## Pranav Sudhir Patil, [`pp755@georgetown.edu`](mailto:pp755@georgetown.edu) {.smaller .title-11}

<center>

![](img/pranav.jpg)

</center>

## Ofure Udabor, [`au195@georgetown.edu`](mailto:au195@georgetown.edu) {.smaller .title-12}

<center>

![](img/ofure2.jpg){width="35%"}

</center>

## Yifei Wu, [`yw924@georgetown.edu`](mailto:yw924@georgetown.edu) {.smaller .title-12}

<center>

![](img/yifei.jpg)

</center>

## Naomi Yamaguchi, [`ny159@georgetown.edu`](mailto:ny159@georgetown.edu) {.smaller .title-11}

<center>

![](img/naomi.jpg)

</center>

## Leqi Ying, [`ly290@georgetown.edu`](mailto:ly290@georgetown.edu) {.smaller}

<center>

![](img/leqi.jpg)

</center>

## Xinyue (Monica) Zhang, [`xz646@georgetown.edu`](mailto:xz646@georgetown.edu) {.smaller .title-10}

<center>

![](img/xinyue.jpg)

</center>

## Course Description {.smaller}

* Data is everywhere! Many times, it's just too big to work with traditional tools. This is a hands-on, practical workshop style course about using cloud computing resources to do analysis and manipulation of datasets that are too large to fit on a single machine and/or analyzed with traditional tools. The course will focus on Spark, MapReduce, the Hadoop Ecosystem and other tools.

* You will understand how to acquire and/or ingest the data, and then massage, clean, transform, analyze, and model it within the context of big data analytics. You will be able to think more programmatically and logically about your big data needs, tools and issues.

Always refer to the [syllabus](/syllabus.html) and [calendar](/schedule.html) in the [course website]({{< var course.url >}}) for class policies.

## Learning Objectives {.smaller}

-   Setup, operate and manage big data tools and cloud infrastructure, including Spark, DuckDB, Polars, Athena, Snowflake, and orchestration tools like Airflow on Amazon Web Services
-   Use ancillary tools that support big data processing, including git and the Linux command line
-   Execute a big data analytics exercise from start to finish: ingest, wrangle, clean, analyze, store, and present
-   Develop strategies to break down large problems and datasets into manageable pieces
-   Identify broad spectrum resources and documentation to remain current with big data tools and developments
-   Communicate and interpret the big data analytics results through written and verbal methods

## Evaluation
- Group project : 40%
- Assignments : 30%
- Lab completions : 20%
- Quizzes : 10%

## Course Materials
* Slides/labs/assignment on Website/GitHub
* Quizzes and readings in Canvas

## Communication

- **Slack is the primary form of communication** 
- [Instructional team email:](mailto:{{< var course.email >}})  `{{< var course.email >}}` 


## Slack rules {.smaller}

- Post any question/comment about the course, assignments or any technical issue.
- DMs are to be used sparingly
- You may not DM multiple people in the instructional team at the same time for the same issue
-  Keep an eye on the questions posted in Slack. Use the search function. It's very possible that we have already answered a questions
- You may DM us back only if we DM you first on a given issue
- Lab/assignment/project questions will only be answered up to 6 hours before something is due (i.e. 6pm on Mondays)

## Midterm Project (NEW!) {.crunch-title .crunch-ul}

* **Individual assignment** (not team-based)
* **Timing**: Around Week 5-6
* **Weight**: Equivalent to 2 homework assignments
* **Format**: 
  * We provide the dataset and problem statement
  * You apply big data tools and techniques learned in class
  * End-to-end data pipeline implementation
* **Details**: TBD (will be announced in Week 4)

## Final Project {.smaller}

* Groups of 3-4 students
* Use an archive of Reddit data, augmented with external data
* Exploratory analysis
* NLP
* Machine Learning
* Writeup
  * Data sourcing and ingesting
  * Exploratory analysis
  * Modeling
  * Challenges and Learnings
  * Conclusions 
  * Future work
  

# BIG DATA {data-stack-name="Big Data"}

**Where does it come from?**

**How is it being created?**

## In one minute of time (2018)

<center>
<img src="img/data-never-sleeps-2018.png" width="50%">
</center>

## In one minute of time (2019)

<center>
<img src="img/data-never-sleeps-2019.jpg" width="50%">
</center>

## In one minute of time (2020)

<center>
<img src="img/data-never-sleeps-2020.jpg" width="50%">
</center>

## In one minute of time (2021)

<center>
<img src="img/data-never-sleeps-2021.png" width="50%">
</center>

## In one minute of time (2025) {.crunch-title .text-80 .crunch-p .crunch-ul}

**Every 60 seconds in 2025:**

* ChatGPT serves **millions** of requests (exact numbers proprietary)
* **500 hours** of video uploaded to YouTube
* **1.04 million** Slack messages sent
* **362,000 hours** watched on Netflix
* **5.9-11.4 million** Google searches
* **$443,000** spent on Amazon
* AI-generated images created at **massive scale** (metrics not publicly available)
* **347,200** posts on X (formerly Twitter)
* **231-250 million** emails sent

## _A lot_ of it is hapenning online. {.crunch-title .crunch-ul .text-80}

:::: {.columns}
::: {.column width="50%"}

**We can record every:**

* click
* ad impression
* billing event
* video interaction
* server request
* transaction
* network message
* fault
* ...

:::
::: {.column width="50%"}

<img src="img/iceberg.jpg">

:::
::::

## It can also be user-generated content: {.crunch-title .crunch-ul .text-90}

:::: {layout="[1,1]" layout-valign="center"}
::: {#user-generated-text}

* Instagram posts & Reels
* X (Twitter) posts & Threads
* TikTok videos
* YouTube Shorts
* Reddit discussions
* Discord conversations
* AI-generated content (text, images, code)
* ...

:::
::: {#user-generated-img}

<img src="img/people.jpg">

:::
::::

## But health and scientific computing create a lot too!

<center>
<img src="img/scientific.png">
</center>

## There's lots of **graph** data too {.crunch-title .text-90}

:::: {layout="[1,1]" layout-valign="center"}
::: {#graph-text}

Many interesting datasets have a graph structure:

* Social networks
* Google's knowledge graph
* Telecom networks
* Computer networks
* Road networks
* Collaboration/relationships

Some of these are **HUGE**

:::
::: {#graph-img}

<img src="img/big-graph.jpg">

:::
::::

## Apache (web server) log files
<img src="img/apache-server-log.jpg" width=600>

## System log files

<img src="img/syslog-file.jpg" width=600>

## Internet of Things (IoT) in 2025 {.crunch-title .crunch-p .crunch-ul .crunch-img}

**75 billion connected devices generating data:**

:::: {layout="[50,50]" layout-valign="top"}
::: {#iot-text}

* Smart home devices (Alexa, Google Home, Apple HomePod)
* Wearables (Apple Watch, Fitbit, Oura rings)
* Connected vehicles & self-driving cars
* Industrial IoT sensors

:::
::: {#iot-img}

* Smart city infrastructure
* Medical devices & remote patient monitoring

<img src="img/sensors-everywhere.jpg" width=600>

:::
::::

## Smartphone Location Data

<img src="img/smartphone.jpg">

## Where else?

* The Internet

* Transactions


* Databases


* Excel

* PDF Files


* Anything digital (music, movies, apps)


* Some old floppy disk lying around the house


## Typical Real-World Scenarios in 2025 {.text-90}

**Scenario 1: Traditional Big Data**

You have a laptop with 16GB of RAM and a 256GB SSD. You are given a 1TB dataset in text files. **What do you do?**

**Scenario 2: AI/ML Pipeline**

Your company wants to build a RAG system using 10TB of internal documents. You need sub-second query response times. **How do you architect this?**

**Scenario 3: Real-Time Analytics**

You need to process 1 million events/second from IoT devices and provide real-time dashboards with <1s latency. **What's your stack?** 

# What is Big Data?

## Let's discuss!

![Exponential data growth](img/data-exp-growth-statista.png)

## Big Data Definitions

**Wikipedia**

_"A collection of datasets so large and complex that it becomes difficult to process using traditional tools and applications. Big Data technologies describe a new generation of technologies and architectures designed to economically extract value from very large volumes of a wide variety of data, by enabling high-velocity capture, discover and/or analysis"_


**O'Reilly**

_"Big data is when **the size of the data itself** becomes **part of the problem**"_


## Frameworks for Thinking About Big Data {.crunch-title .title-08}

**IBM (The 3 V's)**

* **Volume** (Gigabytes &rarr; Exabytes &rarr; Zettabytes)
* **Velocity** (Batch &rarr; Streaming &rarr; Real-time AI inference)
* **Variety** (Structured, Unstructured, Embeddings)

**Additional V's for 2025**

:::: {.columns}
::: {.column width="35%"}

* Variability
* Veracity
* Visualization

:::
::: {.column width="65%"}

* Value
* **Vectors** (embeddings for AI/ML)
* **Versatility** (multi-modal data)

:::
::::

## Data "Size"

$$
\text{``Size''} = f(\text{Processing Ability}, \text{Storage Space})
$$

* Can you analyze/process your data on a single machine?
* Can you store (or is it stored) on a single machine?
* Can you serve it fast enough for real-time AI applications?

If any of of the answers is _**no**_ then you have a big-ish data problem!


# Big Data in the Age of Generative AI

## The New Data Landscape (2025) {.crunch-ul}

**Training Foundation Models**

* GPT-4: Trained on about 13 trillion tokens
* Llama 3: 15 trillion tokens 
* Google Gemini: Multi-modal training (text, images, video)
* Each iteration requires **petabytes** of curated data

**Data Requirements Have Exploded**

* 2020: BERT trained on 3.3 billion words
* 2023: GPT-4 trained on ~13 trillion tokens
* 2024: Llama 3 trained on 15+ trillion tokens

## Big Data Infrastructure: Data Lakes, Warehouses {.title-09 .text-80}

**Traditional Use Cases:**

* Business intelligence
* Analytics & reporting
* Historical data storage

**Modern AI Use Cases:**

* Training data repositories
* Vector embeddings storage
* RAG (Retrieval-Augmented Generation) context
* Fine-tuning datasets
* Evaluation & benchmark data

## RAG and Context Engineering: The New Data Pipeline {.text-80 .crunch-title .crunch-p}

```{dot}
//| fig-height: 1
digraph G {
  rankdir="LR";
  raw[label="Raw Data"];
  lake[label="Data Lake"];
  proc[label="Processing"];
  vec[label="Vector DB"];
  context[label="LLM Context"];
  raw -> lake -> proc -> vec -> context;
}
```

**Key Components:**

* **Data Lakes** (S3, Azure Data Lake): Store massive unstructured data
* **Data Warehouses** (Snowflake, BigQuery): Structured data for context
* **Vector Databases** (Pinecone, Weaviate, Qdrant): Semantic search
* **Embedding Models**: Convert data to vectors
* **Orchestration** (Airflow, Prefect): Manage the pipeline


## MCP Servers & Agentic AI {.smaller .crunch-title .crunch-ul .crunch-p}

**Model Context Protocol (MCP)**

* Open protocol for connecting AI assistants to data sources
* Standardized way to expose tools and data to LLMs
* Enables "agentic" behavior - AI that can act autonomously

**MCP in Production**

```{dot}
//| fig-height: 1
digraph G {
  rankdir="LR";
  ware[label="Data Warehouse"];
  mcp[label="MCP Server"];
  agent[label="AI Agent"];
  action[label="Action"];
  ware -> mcp -> agent -> action;
}
```

**Examples:**

* AI agents querying Snowflake for real-time analytics
* Autonomous systems updating data lakes based on predictions
* Multi-agent systems coordinating through shared data contexts


## Data Quality for AI {.text-85 .crunch-p .crunch-ul}

*(Why Data Quality Matters More Than Ever)*

**Garbage In, Garbage Out - Amplified:**

* Bad training data â†’ Biased models
* Incorrect RAG data â†’ Hallucinations
* Poor data governance â†’ Compliance issues

**Data Quality Challenges in 2025**

* **Scale**: Validating trillions of tokens
* **Diversity**: Multi-modal, multi-lingual data
* **Velocity**: Real-time data for online learning
* **Veracity**: Detecting AI-generated synthetic data

## Real-World Big Data / AI Examples {.crunch-title .smaller .title-11 .crunch-ul .crunch-p}

**Netflix**

* **Data Scale**: 260+ million subscribers generating 100+ billion events/day
* **AI Use**: Personalization, content recommendations, thumbnail generation
* **Stack**: S3 â†’ Spark â†’ Iceberg â†’ ML models â†’ Real-time serving

**Uber**

* **Data Scale**: 35+ million trips per day, petabytes of location data
* **AI Use**: ETA prediction, surge pricing, driver-rider matching
* **Stack**: Kafka â†’ Spark Streaming â†’ Feature Store â†’ ML Platform

**OpenAI**

* **Data Scale**: Trillions of tokens for training, millions of queries/day
* **AI Use**: GPT models, DALL-E, embeddings
* **Stack**: Distributed training &rarr; Vector DBs &rarr; Inference clusters


## Emerging Trends (2025-2027) {.smaller .crunch-ul .crunch-p}

**Unified Platforms:**

* Data lakes becoming "AI lakes"
* Integrated vector + relational databases
* One-stop shops for data + AI (Databricks, Snowflake Cortex)

**Edge Computing + AI:**

* Processing at the data source
* Federated learning across devices
* 5G enabling real-time edge AI

**Synthetic Data:**

* AI generating training data for AI
* Privacy-preserving synthetic datasets

## Relative Data Sizes {.text-90 .crunch-title}

&nbsp;<br>

```{=html}
<style>
.honk-honk {
  font-size: 200% !important;
  /* font-family: "Bungee Spice", sans-serif; */
  font-family: "Honk", sans-serif;
  /* font-optical-sizing: auto; */
  font-weight: 400;
  font-style: normal;
}
</style>
```

```{=html}
<table>
<thead>
</thead>
<tbody>
<tr>
  <td align="center" rowspan="2" width="15%" style="vertical-align: middle; border-bottom: 0px;">Can be <b>processed</b> on single machine?</td>
  <td align="center" width="5%" style="vertical-align: middle; border-right: 2px solid #909090; border-bottom: 0px solid #909090;"><i>No</i></td>
  <td align="center" width="40%" style="vertical-align: middle; border-right: 2px solid #909090; border-top: 2px solid #909090; background-color: #FF991C90;"><span data-qmd="**Medium**<br>(*Parallel Processing*)"></span></td>
  <td align="center" width="40%" style="background-color: #FF991C90; vertical-align: middle; border-top: 2px solid #909090; border-right: 2px solid #909090;"><span data-qmd="[Big!]{.honk-honk}<br>Parallel + Distributed Processing"></span></td>
</tr>
<tr>
  <td align="center" style="vertical-align: middle; border-right: 2px solid #909090; border-bottom: 0px solid #909090;"><i>Yes</i></td>
  <td align="center" style="vertical-align: middle; border-right: 2px solid #909090;"><span data-qmd="Small<br>(*Your Laptop*)"></span></td>
  <td align="center" style="vertical-align: middle; border-right: 2px solid #909090; background-color: #FF991C90;"><span data-qmd="**Medium**<br>(*Data Streaming*)"></span></td>
</tr>
<tr>
  <td style="border-bottom: 0px;"></td>
  <td style="border-bottom: 0px solid #909090;"></td>
  <td align="center" style="vertical-align: middle; border-bottom: 0px solid #909090;"><i>Yes</i></td>
  <td align="center" style="vertical-align: middle; border-bottom: 0px solid #909090;"><i>No</i></td>
</tr>
<tr>
  <td></td>
  <td></td>
  <td align="center" colspan="2">Can be <b>stored</b> on single machine?</td>
</tr>
</tbody>
</table>
```


## What You'll Learn in This Course {.crunch-title .smaller}

*Modern Big Data Stack (2025)*

:::: {layout="[1,1]"}
::: {#bd-stack-left}

**Query Engines:**

* DuckDB - In-process analytical database
* Polars - Lightning-fast DataFrame library  
* Spark - Distributed processing at scale

**Data Warehouses & Lakes:**

* Snowflake - Cloud-native data warehouse
* Athena - Serverless SQL on S3
* Iceberg - Open table format

:::
::: {#bd-stack-right}

**AI/ML Integration:**

* Vector databases for embeddings
* RAG implementation patterns
* Streaming with Spark Structured Streaming

**Orchestration:**

* Airflow for pipeline management
* Serverless with AWS Lambda

:::
::::

## Data Types

* Structured
* Unstructured
* Natural language
* Machine-generated
* Graph-based
* Audio, video, and images
* Streaming


## Big Data vs. Small Data I {.smaller .crunch-title .text-60}

|  | Small Data is usually... | On the other hand, Big Data... |
|---|---|---|
| **Goals** | gathered for a specific goal | may have a goal in mind when it's first started, but things can evolve or take unexpected directions |
| **Location** | in one place, and often in a single computer file | can be in multiple files in multiple servers on computers in different geographic locations |
| **Structure/Contents** | highly structured like an Excel spreadsheet, and it's got rows and columns of data | can be unstructured, it can have many formats in files involved across disciplines, and may link to other resources |
| **Preparation** | prepared by the end user for their own purposes | is often prepared by one group of people, analyzed by a second group of people, and then used by a third group of people, and they may have different purposes, and they may have different disciplines |

: {tbl-colwidths="[20, 40, 40]"}

## Big Data vs. Small Data II {.smaller .crunch-title .text-60}

|  | Small Data is usually... | On the other hand, Big Data... |
|---|---|---|
| **Longevity** | kept for a specific amount of time after the project is over because there's a clear ending point. In the academic world it's maybe five or seven years and then you can throw it away | contains data that must be stored in perpetuity. Many big data projects extend into the past and future |
| **Measurements** | measured with a single protocol using set units and it's usually done at the same time | is collected and measured using many sources, protocols, units, etc |
| **Reproducibility** | be reproduced in their entirety if something goes wrong in the process | replication is seldom feasible |
| **Stakes** | if things go wrong the costs are limited, it's not an enormous problem | can have high costs of failure in terms of money, time and labor |
| **Access** | identified by a location specified in a row/column | unless it is exceptionally well designed, the organization can be inscrutable |
| **Analysis** | analyzed together, all at once | is ordinarily analyzed in incremental steps |

: {tbl-colwidths="[20, 40, 40]"}

## Challenges of Working with Large Datasets {.text-85 .title-09}

| The V | The Challenge |
|---|---|
| **Volume** | data scale |
| **Value** | data usefulness in decision making |
| **Velocity** | data processing: batch or stream |
| **Viscosity** | data complexity |
| **Variability** | data flow inconsistency |
| **Volatility** | data durability |
| **Viability** | data activeness |
| **Validity** | data properly understandable |
| **Variety** | data heterogeneity |


## Thinking About Big Data Workflows {.smaller .crunch-title .title-12}

[William Cohen](http://www.cs.cmu.edu/~wcohen/) (Director, Research Engineering, Google):

* Working with big data is _not_ about...
  * Code optimization
  * Learning the details of today's hardware/software (they are evolving...)
* Working with big data _is_ about **understanding**:
  * The cost of what you want to do
  * What the tools that are available offer
  * How much can be accomplished with linear or nearly-linear operations 
  * How to organize your computations so that they effectively use whateverâ€™s fast
  * How to test/debug/verify with large data
* Recall that traditional tools like `R` and `Python` are **single threaded** (by default)

## Tools at-a-glance

:::: {.columns}
::: {.column width="50%" .r-fit-text}
### Languages, libraries, and projects
- [Python](https://www.python.org/)
    - [`pandas`](https://pandas.pydata.org/)
    - [`polars`](https://www.pola.rs/)
    - [`PySpark`](https://spark.apache.org/docs/latest/api/python/index.html)
    - [`duckdb`](https://duckdb.org/docs/api/python/overview.html)
    - [`dask`](https://www.dask.org/)
    - [`ray`](https://www.ray.io/)
- [Apache Arrow](https://arrow.apache.org/)
- [Apache Spark](https://spark.apache.org/)
- [SQL](https://en.wikipedia.org/wiki/SQL) 
- [Apache Hadoop](https://hadoop.apache.org/) (briefly)

:::

::: {.column width="50%" .r-fit-text}

### Cloud Services 

- Amazon Web Services (AWS)
  - [AWS Sagemaker](https://aws.amazon.com/pm/sagemaker/)
  - [Amazon S3](https://aws.amazon.com/s3/)
- Azure
  - [Azure Blob](https://azure.microsoft.com/en-us/products/storage/blobs/)
  - [Azure Machine Learning](https://azure.microsoft.com/en-us/products/machine-learning/)

Other:

- [AWS Elastic MapReduce (EMR)](https://aws.amazon.com/emr/)
  
:::
:::: 

## Additional links of interest

- Matt Turck's Machine Learning, Artificial Intelligence & Data Landscape (MAD)
    - [Article](https://mattturck.com/mad2023/)
    - [Interactive Landscape](https://mad.firstmarkcap.com/)

- [Is there life after Hadoop?](https://www.cio.com/article/188940/is-there-life-after-hadoop-the-answer-is-a-resounding-yes.html)

- [10 Best Big Data Tools for 2023](https://jelvix.com/blog/top-5-big-data-frameworks) 


# Data Engineering {data-stack-name="Data Engineering"}

## Data Scientist vs. Data Engineer {.smaller}

In this course, you'll **augment** your data scientist skills with data engineering skills!

<center>

<img src="img/bdi-core-competencies-deng-and-dsci.png" width="60%">

</center>

## Data Engineer Responsibilities

<center>

<img src="img/datacamp-3.png" width=600>

</center>


## Data Engineering: Levels 2 and 3

<center>

<img src="img/rogati.png" width=600>

</center>

# Modern Data Architecture

## Architecture

![](img/01-data-platform-anatomy.png){fig-align="center"}

## Storage

![](img/02-data-platform-storage.png){fig-align="center"}

## Source control

![](img/03-data-platform-source-control.png){fig-align="center"}

## Orchestration

![](img/04-data-platform-orchestration.png){fig-align="center"}

## Processing

![](img/05-data-platform-processing.png){fig-align="center"}

## Analytics

![](img/06-data-platform-analytics.png){fig-align="center"}

## Machine Learning

![](img/07-data-platform-ml.png){fig-align="center"}

## Governance

![](img/08-data-platform-governance.png){fig-align="center"}

# Time for Lab! Linux Command Line {data-stack-name="Linux Lab"}

## The Terminal {.crunch-title .text-90}

:::: {layout="[1,1]" layout-valign="center"}
::: {#terminal-text}

- Terminal access was **THE ONLY** way to do programming
- No GUIs! No Spyder, Jupyter, RStudio, etc.
- Coding is still more powerful than graphical interfaces for complex jobs
- Coding makes work repeatable

:::
::: {#terminal-img}

![](img/terminal_old_computer.jpg)

:::
::::

## BASH {.crunch-title}

:::: {layout="[60,40]" layout-valign="center"}
::: {#bash-text}

* Created in 1989 by Brian Fox: "**B**ourne-**A**gain **Sh**ell"
* Brian Fox also built the first online interactive banking software
* BASH is a command processor
* Connection between you and the machine language and hardware

:::
::: {#bash-img}

![](img/bash.png)

:::
::::

## The Prompt

``` {.bash code-line-numbers="false"}
username@hostname:current_directory $
```

What do we learn from the prompt?

- Who you are - **`username`**
- The machine where your code is running - **`hostname`**
- The directory where your code is running - **`current_directory`**
- The shell type - **`$`** - this symbol means BA$H

## Syntax {.smaller .crunch-title .crunch-p .crunch-li-7}

``` {.bash code-line-numbers="false"}
COMMAND -F --FLAG
```

* `COMMAND` is the program, everything after that = arguments
* `F` is a single letter flag, `FLAG` is a single word or words connected by dashes. A space breaks things into a new argument.
* Sometimes argument has single letter and long form versions (e.g. `F` and `FLAG`)

```{.bash code-line-numbers="false"}
COMMAND -F --FILE file1
```

* Here we pass a text argument `"file1"` as the **value** for the `FILE` flag
* **`-h`** flag is usually to get help. You can also run the **`man`** command and pass the name of the program as the argument to get the help page.

Let's try basic commands:

- **`date`** to get the current date
- **`whoami`** to get your user name
- **`echo "Hello World"`** to print to the console

## Examining Files {.crunch-title .text-85 .crunch-ul .crunch-li-8}

* Find out your **P**resent **W**orking **D**irectory **`pwd`**
* Examine the contents of files and folders using **`ls`**
* Make new files from scratch using **`touch`**
* **Glob**: "Mini-language" for selecting files with wildcards
  - **`\*`** for wild card any number of characters
  - **`\?`** for wild card for a single character
  - **`[]`** for one of many character options
  - **`!`** for exclusion
  - **`[:alpha:]`**, **`[:alnum:]`**, **`[:digit:]`**, **`[:lower:]`**, **`[:upper:]`**

[Reference material: Shell Lesson 1,2,4,5](https://linuxjourney.com/lesson/the-shell)

## Navigating Directories {.crunch-title .text-80 .crunch-ul .crunch-p .crunch-li-8}

* Knowing **where your terminal is executing code** ensures you are working with the right inputs/outputs.
* Use `pwd` to determine the Present Working Directory.
* Change to a folder called "git-repo" with `cd git-repo`.
- **`.`** refers to the current directory, such as **`./git-repo`**
- **`..`** can be used to move up one level (**`cd ..`**), and can be combined to move up multiple levels (**`cd ../../my_folder`**)
- **`/`** is the **root** of the filesystem: contains core folders (system, users)
- **`~`** is the **home directory**. Move to folders referenced relative to this path by including it at the start of your path, for example **`~/projects`**.
* To **visualize the structure** of your working directory, use **`tree`**

[Reference link](https://www.freecodecamp.org/news/linux-command-line-bash-tutorial/)

## Interacting with Files {.crunch-title .crunch-ul .text-85 .crunch-p .crunch-li-8}

Now that we know how to navigate through **directories**, we need commands for interacting with **files**...

- **`mv`** to move files from one location to another
  * Can use glob here - `?`, `*`, `[]`, ...
- **`cp`** to copy files instead of moving
  + Can use glob here - `?`, `*`, `[]`, ...
- **`mkdir`** to make a directory
- **`rm`** to remove files
- **`rmdir`** to remove directories
- **`rm -rf`** to blast everything! WARNING!!! DO NOT USE UNLESS YOU KNOW WHAT YOU ARE DOING

## Using BASH for Data Exploration {.crunch-title .crunch-ul .text-90 .crunch-li-8}

- **`head FILENAME`** / **`tail FILENAME`** - glimpsing the first / last few rows of data
- **`more FILENAME`** / **`less FILENAME`** - viewing the data with basic up / (up & down) controls
- **`cat FILENAME`** - print entire file contents into terminal
- **`vim FILENAME`** - open (or edit!) the file in vim editor
- **`grep FILENAME`** - search for lines within a file that match a regex expression
- **`wc FILENAME`** - count the number of lines (**`-l`** flag) or number of words (**`-w`** flag)

[Reference material: Text Lesson 8,9,15,16](https://linuxjourney.com/lesson/stdout-standard-out-redirect)

## Pipes and Arrows

* **`|`** sends the stdout to another command (is the most powerful symbol in BASH!)
* **`>`**  sends stdout to a file and overwrites anything that was there before
* **`>>`** appends the stdout to the end of a file (or starts a new file from scratch if one does not exist yet)
* **`<`** sends stdin into the command on the left

[Reference material: Text Lesson 1,2,3,4,5](https://linuxjourney.com/lesson/stdout-standard-out-redirect)

## Alias and User Files {.crunch-title .text-85}

* `/.bashrc` is where your shell settings are located
* How many processes? **`whoami | xargs ps -u | wc -l`**
* Hard to remember full command! Let's make an **alias**
* General syntax:

  ``` {.bash code-line-numbers="false"}
  alias alias_name="command_to_run"
  ```

* For our case:

  ``` {.bash code-line-numbers="false"}
  alias nproc="whoami | xargs ps -u | wc -l"
  ```

* Now we need to put this alias into the .bashrc

  ``` {.bash code-line-numbers="false"}
  alias nproc="whoami | xargs ps -u | wc -l" >> ~/.bashrc
  ```

* Your commands get saved in **`~/.bash_history`**

## Process Management {.crunch-title .text-90 .crunch-ul .crunch-p .crunch-li-8}

* Use **`ps`** to see your running processes
* Use **`top`** or even better **`htop`** to see all running processes
* Install **htop** via **`sudo yum install htop -y`**
* To kill a broken process: first find the **process ID (PID)**
* Then use **`kill [PID NUM]`** to "ask" the process to terminate. If things get really bad: **`kill -9 [PID NUM]`**
* To kill a command in the terminal window it is running in, try using **Ctrl + C** or **Ctrl + /**
* Run **`cat`** on its own to let it stay open. Now open a new terminal to examine the processes and find the cat process.

[Reference material: Text Lesson 1,2,3,7,9,10](https://linuxjourney.com/lesson/monitor-processes-ps-command)

## Try playing a Linux game!

[Bash crawl](https://gitlab.com/slackermedia/bashcrawl) is a game to help you practice your navigation and file access skills. Click on the **binder** link in this repo to launch a jupyter lab session and explore!
