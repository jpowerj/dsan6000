[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resource Hub",
    "section": "",
    "text": "Week 4 Polars Demo\nWeek 4 DuckDB Demo",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#in-class-demos",
    "href": "resources.html#in-class-demos",
    "title": "Resource Hub",
    "section": "",
    "text": "Week 4 Polars Demo\nWeek 4 DuckDB Demo",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#lab-.zips",
    "href": "resources.html#lab-.zips",
    "title": "Resource Hub",
    "section": "Lab .zips",
    "text": "Lab .zips\nAccess the ZIP files here",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#pdfs-of-books",
    "href": "resources.html#pdfs-of-books",
    "title": "Resource Hub",
    "section": "PDFs of Books",
    "text": "PDFs of Books\nThis page is auto-generated from all of the references across the slides for each week: click on the name of a reference to download the ebook version, if available!\n\n\nGopalan, Rukmani. 2022. The Cloud Data Lake: A Guide to Building Robust Cloud Data Architecture. O‚ÄôReilly Media, Inc.\n\n\nJanssens, Jeroen, and Thijs Nieuwdorp. 2025. Python Polars: The Definitive Guide: Transforming, Analyzing, and Visualizing Data with a Fast and Expressive DataFrame API. O‚ÄôReilly Media, Inc.\n\n\nKleppmann, Martin. 2017. Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems. O‚ÄôReilly Media, Inc.\n\n\nLeskovec, Jure, Anand Rajaraman, and Jeffrey David Ullman. 2014. Mining of Massive Datasets. Cambridge University Press.\n\n\nLoukides, Mike. 2010. ‚ÄúWhat Is Data Science?‚Äù O‚ÄôReilly Media.\n\n\nMell, Peter, and Timothy Grance. 2011. ‚ÄúThe NIST Definition of Cloud Computing.‚Äù National Institute of Standards and Technology, Special Publication 800 (2011): 145.\n\n\nNeedham, Mark, Michael Hunger, and Michael Simons. 2024. DuckDB in Action. Simon and Schuster.\n\n\nRaasveldt, Mark, and Hannes M√ºhleisen. 2019. ‚ÄúDuckDB: An Embeddable Analytical Database.‚Äù In Proceedings of the 2019 International Conference on Management of Data, 1981‚Äì84. SIGMOD ‚Äô19. New York, NY, USA: Association for Computing Machinery.\n\n\nReis, Joe, and Matt Housley. 2022. Fundamentals of Data Engineering: Plan and Build Robust Data Systems. O‚ÄôReilly Media, Inc.\n\n\nTopol, Matthew, and Wes McKinney. 2024. In-Memory Analytics with Apache Arrow. Packt Publishing Ltd.\n\n\nWhite, Tom E. 2015. Hadoop: The Definitive Guide. O‚ÄôReilly Media, Inc.\n\n\nWolohan, John. 2020. Mastering Large Datasets with Python: Parallelize and Distribute Your Python Code. Simon and Schuster.",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "w01/index.html",
    "href": "w01/index.html",
    "title": "Week 1: Course Overview",
    "section": "",
    "text": "Open slides in new tab ‚Üí",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#agenda-for-todays-session",
    "href": "w01/index.html#agenda-for-todays-session",
    "title": "Week 1: Course Overview",
    "section": "Agenda for today‚Äôs session",
    "text": "Agenda for today‚Äôs session\n\nCourse and syllabus overview\nBig Data Concepts\n\nDefinition\nChallenges\nApproaches\n\nData Engineering\nIntroduction to bash\n\nLab: Linux command line",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#bookmark-these-links",
    "href": "w01/index.html#bookmark-these-links",
    "title": "Week 1: Course Overview",
    "section": "Bookmark these links!",
    "text": "Bookmark these links!\n\nCourse website: https://gu-dsan.github.io/6000-fall-2025/\nGitHub Organization for your deliverables: https://github.com/gu-dsan/\nGitHub Classroom: https://classroom.github.com/classrooms/34950344-georgetown-university-dsan6000-big-data-and-cloud-computing\nSlack Workspace: DSAN6000 Fall 2025 - https://dsan6000fall2025.slack.com\n\nJoin link: https://join.slack.com/t/dsan6000fall2025/shared_invite/zt-3b22qhque-GagQykwYYNiEzli9UXJn4w\n\nInstructors email: dsan-Fall-2025@georgetown.edu\nCanvas: https://georgetown.instructure.com/courses/TBA-2025\n\n\n\n\n\n\n\nThese are also pinned on the Slack main channel",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#instructional-team---professors",
    "href": "w01/index.html#instructional-team---professors",
    "title": "Week 1: Course Overview",
    "section": "Instructional Team - Professors",
    "text": "Instructional Team - Professors\n\nAmit Arora, aa1603@georgetown.edu\nJeff Jacobs, jj1088@georgetown.edu",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#amit-arora-aa1603georgetown.edu",
    "href": "w01/index.html#amit-arora-aa1603georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Amit Arora, aa1603@georgetown.edu",
    "text": "Amit Arora, aa1603@georgetown.edu\n\n\n\nPrincipal Solutions Architect - AI/ML at AWS\nAdjunct Professor at Georgetown University\nMultiple patents in telecommunications and applications of ML in telecommunications\n\nFun Facts\n\n\n\n\n\nI am a self-published author https://blueberriesinmysalad.com/\nMy book ‚ÄúBlueberries in my salad: my forever journey towards fitness & strength‚Äù is written as code in R and Markdown\nI love to read books about health and human performance, productivity, philosophy and Mathematics for ML. My reading list is online!",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#jeff-jacobs-jj1088georgetown.edu",
    "href": "w01/index.html#jeff-jacobs-jj1088georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Jeff Jacobs, jj1088@georgetown.edu",
    "text": "Jeff Jacobs, jj1088@georgetown.edu\n\n\n\nFull-time Professor at Georgetown (DSAN and Public Policy)\nBackground in Computational Social Science (Comp Sci MS ‚Üí Political Economy PhD ‚Üí Labor Econ Postdoc)\n\nFun Facts\n\nUsed Apache Airflow daily for PhD projects! (Example)\n\n\n\n\n\n\nServer admin for lab server ‚Üí lab AWS account at Columbia (2015-2023) ‚Üí new DSAN server (!) (2025-)\nPassion project 1: Code for Palestine (2015-2022) ‚Üí YouthCode-Gaza (2023) ‚Üí Ukraine Ministry of Digital Transformation (2024)\nPassion projects 2+3 [ü§ì]: Sample-based music production, web app frameworks\nSleep disorder means lots of reading ‚Äì mainly history! ‚Äì at night\nAlso teaching PPOL6805 / DSAN 6750: GIS for Spatial Data Science this semester",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#instructional-team---teaching-assistants",
    "href": "w01/index.html#instructional-team---teaching-assistants",
    "title": "Week 1: Course Overview",
    "section": "Instructional Team - Teaching Assistants",
    "text": "Instructional Team - Teaching Assistants\n\nBinhui Chen, bc928@georgetown.edu\nPranav Sudhir Patil, pp755@georgetown.edu\nOfure Udabor, au195@georgetown.edu\nYifei Wu, yw924@georgetown.edu\nNaomi Yamaguchi, ny159@georgetown.edu\nLeqi Ying, ly290@georgetown.edu\nXinyue (Monica) Zhang, xz646@georgetown.edu",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#binhui-chen-bc928georgetown.edu",
    "href": "w01/index.html#binhui-chen-bc928georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Binhui Chen, bc928@georgetown.edu",
    "text": "Binhui Chen, bc928@georgetown.edu\n(Lead TA for the course!)",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#pranav-sudhir-patil-pp755georgetown.edu",
    "href": "w01/index.html#pranav-sudhir-patil-pp755georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Pranav Sudhir Patil, pp755@georgetown.edu",
    "text": "Pranav Sudhir Patil, pp755@georgetown.edu",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#ofure-udabor-au195georgetown.edu",
    "href": "w01/index.html#ofure-udabor-au195georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Ofure Udabor, au195@georgetown.edu",
    "text": "Ofure Udabor, au195@georgetown.edu",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#yifei-wu-yw924georgetown.edu",
    "href": "w01/index.html#yifei-wu-yw924georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Yifei Wu, yw924@georgetown.edu",
    "text": "Yifei Wu, yw924@georgetown.edu",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#naomi-yamaguchi-ny159georgetown.edu",
    "href": "w01/index.html#naomi-yamaguchi-ny159georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Naomi Yamaguchi, ny159@georgetown.edu",
    "text": "Naomi Yamaguchi, ny159@georgetown.edu",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#leqi-ying-ly290georgetown.edu",
    "href": "w01/index.html#leqi-ying-ly290georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Leqi Ying, ly290@georgetown.edu",
    "text": "Leqi Ying, ly290@georgetown.edu",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#xinyue-monica-zhang-xz646georgetown.edu",
    "href": "w01/index.html#xinyue-monica-zhang-xz646georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Xinyue (Monica) Zhang, xz646@georgetown.edu",
    "text": "Xinyue (Monica) Zhang, xz646@georgetown.edu",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#course-description",
    "href": "w01/index.html#course-description",
    "title": "Week 1: Course Overview",
    "section": "Course Description",
    "text": "Course Description\n\nData is everywhere! Many times, it‚Äôs just too big to work with traditional tools. This is a hands-on, practical workshop style course about using cloud computing resources to do analysis and manipulation of datasets that are too large to fit on a single machine and/or analyzed with traditional tools. The course will focus on Spark, MapReduce, the Hadoop Ecosystem and other tools.\nYou will understand how to acquire and/or ingest the data, and then massage, clean, transform, analyze, and model it within the context of big data analytics. You will be able to think more programmatically and logically about your big data needs, tools and issues.\n\nAlways refer to the syllabus and calendar in the course website for class policies.",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#learning-objectives",
    "href": "w01/index.html#learning-objectives",
    "title": "Week 1: Course Overview",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nSetup, operate and manage big data tools and cloud infrastructure, including Spark, DuckDB, Polars, Athena, Snowflake, and orchestration tools like Airflow on Amazon Web Services\nUse ancillary tools that support big data processing, including git and the Linux command line\nExecute a big data analytics exercise from start to finish: ingest, wrangle, clean, analyze, store, and present\nDevelop strategies to break down large problems and datasets into manageable pieces\nIdentify broad spectrum resources and documentation to remain current with big data tools and developments\nCommunicate and interpret the big data analytics results through written and verbal methods",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#evaluation",
    "href": "w01/index.html#evaluation",
    "title": "Week 1: Course Overview",
    "section": "Evaluation",
    "text": "Evaluation\n\nGroup project : 40%\nAssignments : 30%\nLab completions : 20%\nQuizzes : 10%",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#course-materials",
    "href": "w01/index.html#course-materials",
    "title": "Week 1: Course Overview",
    "section": "Course Materials",
    "text": "Course Materials\n\nSlides/labs/assignment on Website/GitHub\nQuizzes and readings in Canvas",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#communication",
    "href": "w01/index.html#communication",
    "title": "Week 1: Course Overview",
    "section": "Communication",
    "text": "Communication\n\nSlack is the primary form of communication\nInstructional team email: dsan-Fall-2025@georgetown.edu",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#slack-rules",
    "href": "w01/index.html#slack-rules",
    "title": "Week 1: Course Overview",
    "section": "Slack rules",
    "text": "Slack rules\n\nPost any question/comment about the course, assignments or any technical issue.\nDMs are to be used sparingly\nYou may not DM multiple people in the instructional team at the same time for the same issue\nKeep an eye on the questions posted in Slack. Use the search function. It‚Äôs very possible that we have already answered a questions\nYou may DM us back only if we DM you first on a given issue\nLab/assignment/project questions will only be answered up to 6 hours before something is due (i.e.¬†6pm on Mondays)",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#midterm-project-new",
    "href": "w01/index.html#midterm-project-new",
    "title": "Week 1: Course Overview",
    "section": "Midterm Project (NEW!)",
    "text": "Midterm Project (NEW!)\n\nIndividual assignment (not team-based)\nTiming: Around Week 5-6\nWeight: Equivalent to 2 homework assignments\nFormat:\n\nWe provide the dataset and problem statement\nYou apply big data tools and techniques learned in class\nEnd-to-end data pipeline implementation\n\nDetails: TBD (will be announced in Week 4)",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#final-project",
    "href": "w01/index.html#final-project",
    "title": "Week 1: Course Overview",
    "section": "Final Project",
    "text": "Final Project\n\nGroups of 3-4 students\nUse an archive of Reddit data, augmented with external data\nExploratory analysis\nNLP\nMachine Learning\nWriteup\n\nData sourcing and ingesting\nExploratory analysis\nModeling\nChallenges and Learnings\nConclusions\nFuture work",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#in-one-minute-of-time-2018",
    "href": "w01/index.html#in-one-minute-of-time-2018",
    "title": "Week 1: Course Overview",
    "section": "In one minute of time (2018)",
    "text": "In one minute of time (2018)",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#in-one-minute-of-time-2019",
    "href": "w01/index.html#in-one-minute-of-time-2019",
    "title": "Week 1: Course Overview",
    "section": "In one minute of time (2019)",
    "text": "In one minute of time (2019)",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#in-one-minute-of-time-2020",
    "href": "w01/index.html#in-one-minute-of-time-2020",
    "title": "Week 1: Course Overview",
    "section": "In one minute of time (2020)",
    "text": "In one minute of time (2020)",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#in-one-minute-of-time-2021",
    "href": "w01/index.html#in-one-minute-of-time-2021",
    "title": "Week 1: Course Overview",
    "section": "In one minute of time (2021)",
    "text": "In one minute of time (2021)",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#in-one-minute-of-time-2025",
    "href": "w01/index.html#in-one-minute-of-time-2025",
    "title": "Week 1: Course Overview",
    "section": "In one minute of time (2025)",
    "text": "In one minute of time (2025)\nEvery 60 seconds in 2025:\n\nChatGPT serves millions of requests (exact numbers proprietary)\n500 hours of video uploaded to YouTube\n1.04 million Slack messages sent\n362,000 hours watched on Netflix\n5.9-11.4 million Google searches\n$443,000 spent on Amazon\nAI-generated images created at massive scale (metrics not publicly available)\n347,200 posts on X (formerly Twitter)\n231-250 million emails sent",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#a-lot-of-it-is-hapenning-online.",
    "href": "w01/index.html#a-lot-of-it-is-hapenning-online.",
    "title": "Week 1: Course Overview",
    "section": "A lot of it is hapenning online.",
    "text": "A lot of it is hapenning online.\n\n\nWe can record every:\n\nclick\nad impression\nbilling event\nvideo interaction\nserver request\ntransaction\nnetwork message\nfault\n‚Ä¶",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#it-can-also-be-user-generated-content",
    "href": "w01/index.html#it-can-also-be-user-generated-content",
    "title": "Week 1: Course Overview",
    "section": "It can also be user-generated content:",
    "text": "It can also be user-generated content:\n\n\n\n\nInstagram posts & Reels\nX (Twitter) posts & Threads\nTikTok videos\nYouTube Shorts\nReddit discussions\nDiscord conversations\nAI-generated content (text, images, code)\n‚Ä¶",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#but-health-and-scientific-computing-create-a-lot-too",
    "href": "w01/index.html#but-health-and-scientific-computing-create-a-lot-too",
    "title": "Week 1: Course Overview",
    "section": "But health and scientific computing create a lot too!",
    "text": "But health and scientific computing create a lot too!",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#theres-lots-of-graph-data-too",
    "href": "w01/index.html#theres-lots-of-graph-data-too",
    "title": "Week 1: Course Overview",
    "section": "There‚Äôs lots of graph data too",
    "text": "There‚Äôs lots of graph data too\n\n\n\nMany interesting datasets have a graph structure:\n\nSocial networks\nGoogle‚Äôs knowledge graph\nTelecom networks\nComputer networks\nRoad networks\nCollaboration/relationships\n\nSome of these are HUGE",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#apache-web-server-log-files",
    "href": "w01/index.html#apache-web-server-log-files",
    "title": "Week 1: Course Overview",
    "section": "Apache (web server) log files",
    "text": "Apache (web server) log files",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#system-log-files",
    "href": "w01/index.html#system-log-files",
    "title": "Week 1: Course Overview",
    "section": "System log files",
    "text": "System log files",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#internet-of-things-iot-in-2025",
    "href": "w01/index.html#internet-of-things-iot-in-2025",
    "title": "Week 1: Course Overview",
    "section": "Internet of Things (IoT) in 2025",
    "text": "Internet of Things (IoT) in 2025\n75 billion connected devices generating data:\n\n\n\n\nSmart home devices (Alexa, Google Home, Apple HomePod)\nWearables (Apple Watch, Fitbit, Oura rings)\nConnected vehicles & self-driving cars\nIndustrial IoT sensors\n\n\n\n\nSmart city infrastructure\nMedical devices & remote patient monitoring",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#smartphone-location-data",
    "href": "w01/index.html#smartphone-location-data",
    "title": "Week 1: Course Overview",
    "section": "Smartphone Location Data",
    "text": "Smartphone Location Data",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#where-else",
    "href": "w01/index.html#where-else",
    "title": "Week 1: Course Overview",
    "section": "Where else?",
    "text": "Where else?\n\nThe Internet\nTransactions\nDatabases\nExcel\nPDF Files\nAnything digital (music, movies, apps)\nSome old floppy disk lying around the house",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#typical-real-world-scenarios-in-2025",
    "href": "w01/index.html#typical-real-world-scenarios-in-2025",
    "title": "Week 1: Course Overview",
    "section": "Typical Real-World Scenarios in 2025",
    "text": "Typical Real-World Scenarios in 2025\nScenario 1: Traditional Big Data\nYou have a laptop with 16GB of RAM and a 256GB SSD. You are given a 1TB dataset in text files. What do you do?\nScenario 2: AI/ML Pipeline\nYour company wants to build a RAG system using 10TB of internal documents. You need sub-second query response times. How do you architect this?\nScenario 3: Real-Time Analytics\nYou need to process 1 million events/second from IoT devices and provide real-time dashboards with &lt;1s latency. What‚Äôs your stack?",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#lets-discuss",
    "href": "w01/index.html#lets-discuss",
    "title": "Week 1: Course Overview",
    "section": "Let‚Äôs discuss!",
    "text": "Let‚Äôs discuss!\n\n\n\nExponential data growth",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#big-data-definitions",
    "href": "w01/index.html#big-data-definitions",
    "title": "Week 1: Course Overview",
    "section": "Big Data Definitions",
    "text": "Big Data Definitions\nWikipedia\n‚ÄúA collection of datasets so large and complex that it becomes difficult to process using traditional tools and applications. Big Data technologies describe a new generation of technologies and architectures designed to economically extract value from very large volumes of a wide variety of data, by enabling high-velocity capture, discover and/or analysis‚Äù\nO‚ÄôReilly\n‚ÄúBig data is when the size of the data itself becomes part of the problem‚Äù",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#frameworks-for-thinking-about-big-data",
    "href": "w01/index.html#frameworks-for-thinking-about-big-data",
    "title": "Week 1: Course Overview",
    "section": "Frameworks for Thinking About Big Data",
    "text": "Frameworks for Thinking About Big Data\nIBM (The 3 V‚Äôs)\n\nVolume (Gigabytes ‚Üí Exabytes ‚Üí Zettabytes)\nVelocity (Batch ‚Üí Streaming ‚Üí Real-time AI inference)\nVariety (Structured, Unstructured, Embeddings)\n\nAdditional V‚Äôs for 2025\n\n\n\nVariability\nVeracity\nVisualization\n\n\n\nValue\nVectors (embeddings for AI/ML)\nVersatility (multi-modal data)",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#data-size",
    "href": "w01/index.html#data-size",
    "title": "Week 1: Course Overview",
    "section": "Data ‚ÄúSize‚Äù",
    "text": "Data ‚ÄúSize‚Äù\n\\[\n\\text{``Size''} = f(\\text{Processing Ability}, \\text{Storage Space})\n\\]\n\nCan you analyze/process your data on a single machine?\nCan you store (or is it stored) on a single machine?\nCan you serve it fast enough for real-time AI applications?\n\nIf any of of the answers is no then you have a big-ish data problem!",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#the-new-data-landscape-2025",
    "href": "w01/index.html#the-new-data-landscape-2025",
    "title": "Week 1: Course Overview",
    "section": "The New Data Landscape (2025)",
    "text": "The New Data Landscape (2025)\nTraining Foundation Models\n\nGPT-4: Trained on about 13 trillion tokens\nLlama 3: 15 trillion tokens\nGoogle Gemini: Multi-modal training (text, images, video)\nEach iteration requires petabytes of curated data\n\nData Requirements Have Exploded\n\n2020: BERT trained on 3.3 billion words\n2023: GPT-4 trained on ~13 trillion tokens\n2024: Llama 3 trained on 15+ trillion tokens",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#big-data-infrastructure-data-lakes-warehouses",
    "href": "w01/index.html#big-data-infrastructure-data-lakes-warehouses",
    "title": "Week 1: Course Overview",
    "section": "Big Data Infrastructure: Data Lakes, Warehouses",
    "text": "Big Data Infrastructure: Data Lakes, Warehouses\nTraditional Use Cases:\n\nBusiness intelligence\nAnalytics & reporting\nHistorical data storage\n\nModern AI Use Cases:\n\nTraining data repositories\nVector embeddings storage\nRAG (Retrieval-Augmented Generation) context\nFine-tuning datasets\nEvaluation & benchmark data",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#rag-and-context-engineering-the-new-data-pipeline",
    "href": "w01/index.html#rag-and-context-engineering-the-new-data-pipeline",
    "title": "Week 1: Course Overview",
    "section": "RAG and Context Engineering: The New Data Pipeline",
    "text": "RAG and Context Engineering: The New Data Pipeline\n\n\n\n\n\n\n\nG\n\n\n\nraw\n\nRaw Data\n\n\n\nlake\n\nData Lake\n\n\n\nraw-&gt;lake\n\n\n\n\n\nproc\n\nProcessing\n\n\n\nlake-&gt;proc\n\n\n\n\n\nvec\n\nVector DB\n\n\n\nproc-&gt;vec\n\n\n\n\n\ncontext\n\nLLM Context\n\n\n\nvec-&gt;context\n\n\n\n\n\n\n\n\n\n\nKey Components:\n\nData Lakes (S3, Azure Data Lake): Store massive unstructured data\nData Warehouses (Snowflake, BigQuery): Structured data for context\nVector Databases (Pinecone, Weaviate, Qdrant): Semantic search\nEmbedding Models: Convert data to vectors\nOrchestration (Airflow, Prefect): Manage the pipeline",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#mcp-servers-agentic-ai",
    "href": "w01/index.html#mcp-servers-agentic-ai",
    "title": "Week 1: Course Overview",
    "section": "MCP Servers & Agentic AI",
    "text": "MCP Servers & Agentic AI\nModel Context Protocol (MCP)\n\nOpen protocol for connecting AI assistants to data sources\nStandardized way to expose tools and data to LLMs\nEnables ‚Äúagentic‚Äù behavior - AI that can act autonomously\n\nMCP in Production\n\n\n\n\n\n\n\nG\n\n\n\nware\n\nData Warehouse\n\n\n\nmcp\n\nMCP Server\n\n\n\nware-&gt;mcp\n\n\n\n\n\nagent\n\nAI Agent\n\n\n\nmcp-&gt;agent\n\n\n\n\n\naction\n\nAction\n\n\n\nagent-&gt;action\n\n\n\n\n\n\n\n\n\n\nExamples:\n\nAI agents querying Snowflake for real-time analytics\nAutonomous systems updating data lakes based on predictions\nMulti-agent systems coordinating through shared data contexts",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#data-quality-for-ai",
    "href": "w01/index.html#data-quality-for-ai",
    "title": "Week 1: Course Overview",
    "section": "Data Quality for AI",
    "text": "Data Quality for AI\n(Why Data Quality Matters More Than Ever)\nGarbage In, Garbage Out - Amplified:\n\nBad training data ‚Üí Biased models\nIncorrect RAG data ‚Üí Hallucinations\nPoor data governance ‚Üí Compliance issues\n\nData Quality Challenges in 2025\n\nScale: Validating trillions of tokens\nDiversity: Multi-modal, multi-lingual data\nVelocity: Real-time data for online learning\nVeracity: Detecting AI-generated synthetic data",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#real-world-big-data-ai-examples",
    "href": "w01/index.html#real-world-big-data-ai-examples",
    "title": "Week 1: Course Overview",
    "section": "Real-World Big Data / AI Examples",
    "text": "Real-World Big Data / AI Examples\nNetflix\n\nData Scale: 260+ million subscribers generating 100+ billion events/day\nAI Use: Personalization, content recommendations, thumbnail generation\nStack: S3 ‚Üí Spark ‚Üí Iceberg ‚Üí ML models ‚Üí Real-time serving\n\nUber\n\nData Scale: 35+ million trips per day, petabytes of location data\nAI Use: ETA prediction, surge pricing, driver-rider matching\nStack: Kafka ‚Üí Spark Streaming ‚Üí Feature Store ‚Üí ML Platform\n\nOpenAI\n\nData Scale: Trillions of tokens for training, millions of queries/day\nAI Use: GPT models, DALL-E, embeddings\nStack: Distributed training ‚Üí Vector DBs ‚Üí Inference clusters",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#emerging-trends-2025-2027",
    "href": "w01/index.html#emerging-trends-2025-2027",
    "title": "Week 1: Course Overview",
    "section": "Emerging Trends (2025-2027)",
    "text": "Emerging Trends (2025-2027)\nUnified Platforms:\n\nData lakes becoming ‚ÄúAI lakes‚Äù\nIntegrated vector + relational databases\nOne-stop shops for data + AI (Databricks, Snowflake Cortex)\n\nEdge Computing + AI:\n\nProcessing at the data source\nFederated learning across devices\n5G enabling real-time edge AI\n\nSynthetic Data:\n\nAI generating training data for AI\nPrivacy-preserving synthetic datasets",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#relative-data-sizes",
    "href": "w01/index.html#relative-data-sizes",
    "title": "Week 1: Course Overview",
    "section": "Relative Data Sizes",
    "text": "Relative Data Sizes\n¬†\n\n\n\n\nCan be processed on single machine?\nNo\nMedium(Parallel Processing)\nBig!Parallel + Distributed Processing\n\n\nYes\nSmall(Your Laptop)\nMedium(Data Streaming)\n\n\n\n\nYes\nNo\n\n\n\n\nCan be stored on single machine?",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#what-youll-learn-in-this-course",
    "href": "w01/index.html#what-youll-learn-in-this-course",
    "title": "Week 1: Course Overview",
    "section": "What You‚Äôll Learn in This Course",
    "text": "What You‚Äôll Learn in This Course\nModern Big Data Stack (2025)\n\n\n\nQuery Engines:\n\nDuckDB - In-process analytical database\nPolars - Lightning-fast DataFrame library\n\nSpark - Distributed processing at scale\n\nData Warehouses & Lakes:\n\nSnowflake - Cloud-native data warehouse\nAthena - Serverless SQL on S3\nIceberg - Open table format\n\n\n\nAI/ML Integration:\n\nVector databases for embeddings\nRAG implementation patterns\nStreaming with Spark Structured Streaming\n\nOrchestration:\n\nAirflow for pipeline management\nServerless with AWS Lambda",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#data-types",
    "href": "w01/index.html#data-types",
    "title": "Week 1: Course Overview",
    "section": "Data Types",
    "text": "Data Types\n\nStructured\nUnstructured\nNatural language\nMachine-generated\nGraph-based\nAudio, video, and images\nStreaming",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#big-data-vs.-small-data-i",
    "href": "w01/index.html#big-data-vs.-small-data-i",
    "title": "Week 1: Course Overview",
    "section": "Big Data vs.¬†Small Data I",
    "text": "Big Data vs.¬†Small Data I\n\n\n\n\n\n\n\n\n\nSmall Data is usually‚Ä¶\nOn the other hand, Big Data‚Ä¶\n\n\n\n\nGoals\ngathered for a specific goal\nmay have a goal in mind when it‚Äôs first started, but things can evolve or take unexpected directions\n\n\nLocation\nin one place, and often in a single computer file\ncan be in multiple files in multiple servers on computers in different geographic locations\n\n\nStructure/Contents\nhighly structured like an Excel spreadsheet, and it‚Äôs got rows and columns of data\ncan be unstructured, it can have many formats in files involved across disciplines, and may link to other resources\n\n\nPreparation\nprepared by the end user for their own purposes\nis often prepared by one group of people, analyzed by a second group of people, and then used by a third group of people, and they may have different purposes, and they may have different disciplines",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#big-data-vs.-small-data-ii",
    "href": "w01/index.html#big-data-vs.-small-data-ii",
    "title": "Week 1: Course Overview",
    "section": "Big Data vs.¬†Small Data II",
    "text": "Big Data vs.¬†Small Data II\n\n\n\n\n\n\n\n\n\nSmall Data is usually‚Ä¶\nOn the other hand, Big Data‚Ä¶\n\n\n\n\nLongevity\nkept for a specific amount of time after the project is over because there‚Äôs a clear ending point. In the academic world it‚Äôs maybe five or seven years and then you can throw it away\ncontains data that must be stored in perpetuity. Many big data projects extend into the past and future\n\n\nMeasurements\nmeasured with a single protocol using set units and it‚Äôs usually done at the same time\nis collected and measured using many sources, protocols, units, etc\n\n\nReproducibility\nbe reproduced in their entirety if something goes wrong in the process\nreplication is seldom feasible\n\n\nStakes\nif things go wrong the costs are limited, it‚Äôs not an enormous problem\ncan have high costs of failure in terms of money, time and labor\n\n\nAccess\nidentified by a location specified in a row/column\nunless it is exceptionally well designed, the organization can be inscrutable\n\n\nAnalysis\nanalyzed together, all at once\nis ordinarily analyzed in incremental steps",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#challenges-of-working-with-large-datasets",
    "href": "w01/index.html#challenges-of-working-with-large-datasets",
    "title": "Week 1: Course Overview",
    "section": "Challenges of Working with Large Datasets",
    "text": "Challenges of Working with Large Datasets\n\n\n\nThe V\nThe Challenge\n\n\n\n\nVolume\ndata scale\n\n\nValue\ndata usefulness in decision making\n\n\nVelocity\ndata processing: batch or stream\n\n\nViscosity\ndata complexity\n\n\nVariability\ndata flow inconsistency\n\n\nVolatility\ndata durability\n\n\nViability\ndata activeness\n\n\nValidity\ndata properly understandable\n\n\nVariety\ndata heterogeneity",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#thinking-about-big-data-workflows",
    "href": "w01/index.html#thinking-about-big-data-workflows",
    "title": "Week 1: Course Overview",
    "section": "Thinking About Big Data Workflows",
    "text": "Thinking About Big Data Workflows\nWilliam Cohen (Director, Research Engineering, Google):\n\nWorking with big data is not about‚Ä¶\n\nCode optimization\nLearning the details of today‚Äôs hardware/software (they are evolving‚Ä¶)\n\nWorking with big data is about understanding:\n\nThe cost of what you want to do\nWhat the tools that are available offer\nHow much can be accomplished with linear or nearly-linear operations\nHow to organize your computations so that they effectively use whatever‚Äôs fast\nHow to test/debug/verify with large data\n\nRecall that traditional tools like R and Python are single threaded (by default)",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#tools-at-a-glance",
    "href": "w01/index.html#tools-at-a-glance",
    "title": "Week 1: Course Overview",
    "section": "Tools at-a-glance",
    "text": "Tools at-a-glance\n\n\n\nLanguages, libraries, and projects\n\nPython\n\npandas\npolars\nPySpark\nduckdb\ndask\nray\n\nApache Arrow\nApache Spark\nSQL\nApache Hadoop (briefly)\n\n\n\n\nCloud Services\n\nAmazon Web Services (AWS)\n\nAWS Sagemaker\nAmazon S3\n\nAzure\n\nAzure Blob\nAzure Machine Learning\n\n\nOther:\n\nAWS Elastic MapReduce (EMR)",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#additional-links-of-interest",
    "href": "w01/index.html#additional-links-of-interest",
    "title": "Week 1: Course Overview",
    "section": "Additional links of interest",
    "text": "Additional links of interest\n\nMatt Turck‚Äôs Machine Learning, Artificial Intelligence & Data Landscape (MAD)\n\nArticle\nInteractive Landscape\n\nIs there life after Hadoop?\n10 Best Big Data Tools for 2023",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#data-scientist-vs.-data-engineer",
    "href": "w01/index.html#data-scientist-vs.-data-engineer",
    "title": "Week 1: Course Overview",
    "section": "Data Scientist vs.¬†Data Engineer",
    "text": "Data Scientist vs.¬†Data Engineer\nIn this course, you‚Äôll augment your data scientist skills with data engineering skills!",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#data-engineer-responsibilities",
    "href": "w01/index.html#data-engineer-responsibilities",
    "title": "Week 1: Course Overview",
    "section": "Data Engineer Responsibilities",
    "text": "Data Engineer Responsibilities",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#data-engineering-levels-2-and-3",
    "href": "w01/index.html#data-engineering-levels-2-and-3",
    "title": "Week 1: Course Overview",
    "section": "Data Engineering: Levels 2 and 3",
    "text": "Data Engineering: Levels 2 and 3",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#architecture",
    "href": "w01/index.html#architecture",
    "title": "Week 1: Course Overview",
    "section": "Architecture",
    "text": "Architecture",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#storage",
    "href": "w01/index.html#storage",
    "title": "Week 1: Course Overview",
    "section": "Storage",
    "text": "Storage",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#source-control",
    "href": "w01/index.html#source-control",
    "title": "Week 1: Course Overview",
    "section": "Source control",
    "text": "Source control",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#orchestration",
    "href": "w01/index.html#orchestration",
    "title": "Week 1: Course Overview",
    "section": "Orchestration",
    "text": "Orchestration",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#processing",
    "href": "w01/index.html#processing",
    "title": "Week 1: Course Overview",
    "section": "Processing",
    "text": "Processing",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#analytics",
    "href": "w01/index.html#analytics",
    "title": "Week 1: Course Overview",
    "section": "Analytics",
    "text": "Analytics",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#machine-learning",
    "href": "w01/index.html#machine-learning",
    "title": "Week 1: Course Overview",
    "section": "Machine Learning",
    "text": "Machine Learning",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#governance",
    "href": "w01/index.html#governance",
    "title": "Week 1: Course Overview",
    "section": "Governance",
    "text": "Governance",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#the-terminal",
    "href": "w01/index.html#the-terminal",
    "title": "Week 1: Course Overview",
    "section": "The Terminal",
    "text": "The Terminal\n\n\n\n\nTerminal access was THE ONLY way to do programming\nNo GUIs! No Spyder, Jupyter, RStudio, etc.\nCoding is still more powerful than graphical interfaces for complex jobs\nCoding makes work repeatable",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#bash",
    "href": "w01/index.html#bash",
    "title": "Week 1: Course Overview",
    "section": "BASH",
    "text": "BASH\n\n\n\n\nCreated in 1989 by Brian Fox: ‚ÄúBourne-Again Shell‚Äù\nBrian Fox also built the first online interactive banking software\nBASH is a command processor\nConnection between you and the machine language and hardware",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#the-prompt",
    "href": "w01/index.html#the-prompt",
    "title": "Week 1: Course Overview",
    "section": "The Prompt",
    "text": "The Prompt\nusername@hostname:current_directory $\nWhat do we learn from the prompt?\n\nWho you are - username\nThe machine where your code is running - hostname\nThe directory where your code is running - current_directory\nThe shell type - $ - this symbol means BA$H",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#syntax",
    "href": "w01/index.html#syntax",
    "title": "Week 1: Course Overview",
    "section": "Syntax",
    "text": "Syntax\nCOMMAND -F --FLAG\n\nCOMMAND is the program, everything after that = arguments\nF is a single letter flag, FLAG is a single word or words connected by dashes. A space breaks things into a new argument.\nSometimes argument has single letter and long form versions (e.g.¬†F and FLAG)\n\nCOMMAND -F --FILE file1\n\nHere we pass a text argument \"file1\" as the value for the FILE flag\n-h flag is usually to get help. You can also run the man command and pass the name of the program as the argument to get the help page.\n\nLet‚Äôs try basic commands:\n\ndate to get the current date\nwhoami to get your user name\necho \"Hello World\" to print to the console",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#examining-files",
    "href": "w01/index.html#examining-files",
    "title": "Week 1: Course Overview",
    "section": "Examining Files",
    "text": "Examining Files\n\nFind out your Present Working Directory pwd\nExamine the contents of files and folders using ls\nMake new files from scratch using touch\nGlob: ‚ÄúMini-language‚Äù for selecting files with wildcards\n\n\\* for wild card any number of characters\n\\? for wild card for a single character\n[] for one of many character options\n! for exclusion\n[:alpha:], [:alnum:], [:digit:], [:lower:], [:upper:]\n\n\nReference material: Shell Lesson 1,2,4,5",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#navigating-directories",
    "href": "w01/index.html#navigating-directories",
    "title": "Week 1: Course Overview",
    "section": "Navigating Directories",
    "text": "Navigating Directories\n\nKnowing where your terminal is executing code ensures you are working with the right inputs/outputs.\nUse pwd to determine the Present Working Directory.\nChange to a folder called ‚Äúgit-repo‚Äù with cd git-repo.\n. refers to the current directory, such as ./git-repo\n.. can be used to move up one level (cd ..), and can be combined to move up multiple levels (cd ../../my_folder)\n/ is the root of the filesystem: contains core folders (system, users)\n~ is the home directory. Move to folders referenced relative to this path by including it at the start of your path, for example ~/projects.\nTo visualize the structure of your working directory, use tree\n\nReference link",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#interacting-with-files",
    "href": "w01/index.html#interacting-with-files",
    "title": "Week 1: Course Overview",
    "section": "Interacting with Files",
    "text": "Interacting with Files\nNow that we know how to navigate through directories, we need commands for interacting with files‚Ä¶\n\nmv to move files from one location to another\n\nCan use glob here - ?, *, [], ‚Ä¶\n\ncp to copy files instead of moving\n\nCan use glob here - ?, *, [], ‚Ä¶\n\nmkdir to make a directory\nrm to remove files\nrmdir to remove directories\nrm -rf to blast everything! WARNING!!! DO NOT USE UNLESS YOU KNOW WHAT YOU ARE DOING",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#using-bash-for-data-exploration",
    "href": "w01/index.html#using-bash-for-data-exploration",
    "title": "Week 1: Course Overview",
    "section": "Using BASH for Data Exploration",
    "text": "Using BASH for Data Exploration\n\nhead FILENAME / tail FILENAME - glimpsing the first / last few rows of data\nmore FILENAME / less FILENAME - viewing the data with basic up / (up & down) controls\ncat FILENAME - print entire file contents into terminal\nvim FILENAME - open (or edit!) the file in vim editor\ngrep FILENAME - search for lines within a file that match a regex expression\nwc FILENAME - count the number of lines (-l flag) or number of words (-w flag)\n\nReference material: Text Lesson 8,9,15,16",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#pipes-and-arrows",
    "href": "w01/index.html#pipes-and-arrows",
    "title": "Week 1: Course Overview",
    "section": "Pipes and Arrows",
    "text": "Pipes and Arrows\n\n| sends the stdout to another command (is the most powerful symbol in BASH!)\n&gt; sends stdout to a file and overwrites anything that was there before\n&gt;&gt; appends the stdout to the end of a file (or starts a new file from scratch if one does not exist yet)\n&lt; sends stdin into the command on the left\n\nReference material: Text Lesson 1,2,3,4,5",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#alias-and-user-files",
    "href": "w01/index.html#alias-and-user-files",
    "title": "Week 1: Course Overview",
    "section": "Alias and User Files",
    "text": "Alias and User Files\n\n/.bashrc is where your shell settings are located\nHow many processes? whoami | xargs ps -u | wc -l\nHard to remember full command! Let‚Äôs make an alias\nGeneral syntax:\nalias alias_name=\"command_to_run\"\nFor our case:\nalias nproc=\"whoami | xargs ps -u | wc -l\"\nNow we need to put this alias into the .bashrc\nalias nproc=\"whoami | xargs ps -u | wc -l\" &gt;&gt; ~/.bashrc\nYour commands get saved in ~/.bash_history",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#process-management",
    "href": "w01/index.html#process-management",
    "title": "Week 1: Course Overview",
    "section": "Process Management",
    "text": "Process Management\n\nUse ps to see your running processes\nUse top or even better htop to see all running processes\nInstall htop via sudo yum install htop -y\nTo kill a broken process: first find the process ID (PID)\nThen use kill [PID NUM] to ‚Äúask‚Äù the process to terminate. If things get really bad: kill -9 [PID NUM]\nTo kill a command in the terminal window it is running in, try using Ctrl + C or Ctrl + /\nRun cat on its own to let it stay open. Now open a new terminal to examine the processes and find the cat process.\n\nReference material: Text Lesson 1,2,3,7,9,10",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#try-playing-a-linux-game",
    "href": "w01/index.html#try-playing-a-linux-game",
    "title": "Week 1: Course Overview",
    "section": "Try playing a Linux game!",
    "text": "Try playing a Linux game!\nBash crawl is a game to help you practice your navigation and file access skills. Click on the binder link in this repo to launch a jupyter lab session and explore!",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/slides.html#agenda-for-todays-session",
    "href": "w01/slides.html#agenda-for-todays-session",
    "title": "Week 1: Course Overview",
    "section": "Agenda for today‚Äôs session",
    "text": "Agenda for today‚Äôs session\n\nCourse and syllabus overview\nBig Data Concepts\n\nDefinition\nChallenges\nApproaches\n\nData Engineering\nIntroduction to bash\n\nLab: Linux command line"
  },
  {
    "objectID": "w01/slides.html#bookmark-these-links",
    "href": "w01/slides.html#bookmark-these-links",
    "title": "Week 1: Course Overview",
    "section": "Bookmark these links!",
    "text": "Bookmark these links!\n\nCourse website: https://gu-dsan.github.io/6000-fall-2025/\nGitHub Organization for your deliverables: https://github.com/gu-dsan/\nGitHub Classroom: https://classroom.github.com/classrooms/34950344-georgetown-university-dsan6000-big-data-and-cloud-computing\nSlack Workspace: DSAN6000 Fall 2025 - https://dsan6000fall2025.slack.com\n\nJoin link: https://join.slack.com/t/dsan6000fall2025/shared_invite/zt-3b22qhque-GagQykwYYNiEzli9UXJn4w\n\nInstructors email: dsan-Fall-2025@georgetown.edu\nCanvas: https://georgetown.instructure.com/courses/TBA-2025\n\n\n\n\n\n\n\nThese are also pinned on the Slack main channel"
  },
  {
    "objectID": "w01/slides.html#instructional-team---professors",
    "href": "w01/slides.html#instructional-team---professors",
    "title": "Week 1: Course Overview",
    "section": "Instructional Team - Professors",
    "text": "Instructional Team - Professors\n\nAmit Arora, aa1603@georgetown.edu\nJeff Jacobs, jj1088@georgetown.edu"
  },
  {
    "objectID": "w01/slides.html#amit-arora-aa1603georgetown.edu",
    "href": "w01/slides.html#amit-arora-aa1603georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Amit Arora, aa1603@georgetown.edu",
    "text": "Amit Arora, aa1603@georgetown.edu\n\n\n\nPrincipal Solutions Architect - AI/ML at AWS\nAdjunct Professor at Georgetown University\nMultiple patents in telecommunications and applications of ML in telecommunications\n\nFun Facts\n\n\n\n\nI am a self-published author https://blueberriesinmysalad.com/\nMy book ‚ÄúBlueberries in my salad: my forever journey towards fitness & strength‚Äù is written as code in R and Markdown\nI love to read books about health and human performance, productivity, philosophy and Mathematics for ML. My reading list is online!"
  },
  {
    "objectID": "w01/slides.html#jeff-jacobs-jj1088georgetown.edu",
    "href": "w01/slides.html#jeff-jacobs-jj1088georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Jeff Jacobs, jj1088@georgetown.edu",
    "text": "Jeff Jacobs, jj1088@georgetown.edu\n\n\n\nFull-time Professor at Georgetown (DSAN and Public Policy)\nBackground in Computational Social Science (Comp Sci MS ‚Üí Political Economy PhD ‚Üí Labor Econ Postdoc)\n\nFun Facts\n\nUsed Apache Airflow daily for PhD projects! (Example)\n\n\n\n\n\nServer admin for lab server ‚Üí lab AWS account at Columbia (2015-2023) ‚Üí new DSAN server (!) (2025-)\nPassion project 1: Code for Palestine (2015-2022) ‚Üí YouthCode-Gaza (2023) ‚Üí Ukraine Ministry of Digital Transformation (2024)\nPassion projects 2+3 [ü§ì]: Sample-based music production, web app frameworks\nSleep disorder means lots of reading ‚Äì mainly history! ‚Äì at night\nAlso teaching PPOL6805 / DSAN 6750: GIS for Spatial Data Science this semester"
  },
  {
    "objectID": "w01/slides.html#instructional-team---teaching-assistants",
    "href": "w01/slides.html#instructional-team---teaching-assistants",
    "title": "Week 1: Course Overview",
    "section": "Instructional Team - Teaching Assistants",
    "text": "Instructional Team - Teaching Assistants\n\nBinhui Chen, bc928@georgetown.edu\nPranav Sudhir Patil, pp755@georgetown.edu\nOfure Udabor, au195@georgetown.edu\nYifei Wu, yw924@georgetown.edu\nNaomi Yamaguchi, ny159@georgetown.edu\nLeqi Ying, ly290@georgetown.edu\nXinyue (Monica) Zhang, xz646@georgetown.edu"
  },
  {
    "objectID": "w01/slides.html#binhui-chen-bc928georgetown.edu",
    "href": "w01/slides.html#binhui-chen-bc928georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Binhui Chen, bc928@georgetown.edu",
    "text": "Binhui Chen, bc928@georgetown.edu\n(Lead TA for the course!)"
  },
  {
    "objectID": "w01/slides.html#pranav-sudhir-patil-pp755georgetown.edu",
    "href": "w01/slides.html#pranav-sudhir-patil-pp755georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Pranav Sudhir Patil, pp755@georgetown.edu",
    "text": "Pranav Sudhir Patil, pp755@georgetown.edu"
  },
  {
    "objectID": "w01/slides.html#ofure-udabor-au195georgetown.edu",
    "href": "w01/slides.html#ofure-udabor-au195georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Ofure Udabor, au195@georgetown.edu",
    "text": "Ofure Udabor, au195@georgetown.edu"
  },
  {
    "objectID": "w01/slides.html#yifei-wu-yw924georgetown.edu",
    "href": "w01/slides.html#yifei-wu-yw924georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Yifei Wu, yw924@georgetown.edu",
    "text": "Yifei Wu, yw924@georgetown.edu"
  },
  {
    "objectID": "w01/slides.html#naomi-yamaguchi-ny159georgetown.edu",
    "href": "w01/slides.html#naomi-yamaguchi-ny159georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Naomi Yamaguchi, ny159@georgetown.edu",
    "text": "Naomi Yamaguchi, ny159@georgetown.edu"
  },
  {
    "objectID": "w01/slides.html#leqi-ying-ly290georgetown.edu",
    "href": "w01/slides.html#leqi-ying-ly290georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Leqi Ying, ly290@georgetown.edu",
    "text": "Leqi Ying, ly290@georgetown.edu"
  },
  {
    "objectID": "w01/slides.html#xinyue-monica-zhang-xz646georgetown.edu",
    "href": "w01/slides.html#xinyue-monica-zhang-xz646georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Xinyue (Monica) Zhang, xz646@georgetown.edu",
    "text": "Xinyue (Monica) Zhang, xz646@georgetown.edu"
  },
  {
    "objectID": "w01/slides.html#course-description",
    "href": "w01/slides.html#course-description",
    "title": "Week 1: Course Overview",
    "section": "Course Description",
    "text": "Course Description\n\nData is everywhere! Many times, it‚Äôs just too big to work with traditional tools. This is a hands-on, practical workshop style course about using cloud computing resources to do analysis and manipulation of datasets that are too large to fit on a single machine and/or analyzed with traditional tools. The course will focus on Spark, MapReduce, the Hadoop Ecosystem and other tools.\nYou will understand how to acquire and/or ingest the data, and then massage, clean, transform, analyze, and model it within the context of big data analytics. You will be able to think more programmatically and logically about your big data needs, tools and issues.\n\nAlways refer to the syllabus and calendar in the course website for class policies."
  },
  {
    "objectID": "w01/slides.html#learning-objectives",
    "href": "w01/slides.html#learning-objectives",
    "title": "Week 1: Course Overview",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nSetup, operate and manage big data tools and cloud infrastructure, including Spark, DuckDB, Polars, Athena, Snowflake, and orchestration tools like Airflow on Amazon Web Services\nUse ancillary tools that support big data processing, including git and the Linux command line\nExecute a big data analytics exercise from start to finish: ingest, wrangle, clean, analyze, store, and present\nDevelop strategies to break down large problems and datasets into manageable pieces\nIdentify broad spectrum resources and documentation to remain current with big data tools and developments\nCommunicate and interpret the big data analytics results through written and verbal methods"
  },
  {
    "objectID": "w01/slides.html#evaluation",
    "href": "w01/slides.html#evaluation",
    "title": "Week 1: Course Overview",
    "section": "Evaluation",
    "text": "Evaluation\n\nGroup project : 40%\nAssignments : 30%\nLab completions : 20%\nQuizzes : 10%"
  },
  {
    "objectID": "w01/slides.html#course-materials",
    "href": "w01/slides.html#course-materials",
    "title": "Week 1: Course Overview",
    "section": "Course Materials",
    "text": "Course Materials\n\nSlides/labs/assignment on Website/GitHub\nQuizzes and readings in Canvas"
  },
  {
    "objectID": "w01/slides.html#communication",
    "href": "w01/slides.html#communication",
    "title": "Week 1: Course Overview",
    "section": "Communication",
    "text": "Communication\n\nSlack is the primary form of communication\nInstructional team email: dsan-Fall-2025@georgetown.edu"
  },
  {
    "objectID": "w01/slides.html#slack-rules",
    "href": "w01/slides.html#slack-rules",
    "title": "Week 1: Course Overview",
    "section": "Slack rules",
    "text": "Slack rules\n\nPost any question/comment about the course, assignments or any technical issue.\nDMs are to be used sparingly\nYou may not DM multiple people in the instructional team at the same time for the same issue\nKeep an eye on the questions posted in Slack. Use the search function. It‚Äôs very possible that we have already answered a questions\nYou may DM us back only if we DM you first on a given issue\nLab/assignment/project questions will only be answered up to 6 hours before something is due (i.e.¬†6pm on Mondays)"
  },
  {
    "objectID": "w01/slides.html#midterm-project-new",
    "href": "w01/slides.html#midterm-project-new",
    "title": "Week 1: Course Overview",
    "section": "Midterm Project (NEW!)",
    "text": "Midterm Project (NEW!)\n\nIndividual assignment (not team-based)\nTiming: Around Week 5-6\nWeight: Equivalent to 2 homework assignments\nFormat:\n\nWe provide the dataset and problem statement\nYou apply big data tools and techniques learned in class\nEnd-to-end data pipeline implementation\n\nDetails: TBD (will be announced in Week 4)"
  },
  {
    "objectID": "w01/slides.html#final-project",
    "href": "w01/slides.html#final-project",
    "title": "Week 1: Course Overview",
    "section": "Final Project",
    "text": "Final Project\n\nGroups of 3-4 students\nUse an archive of Reddit data, augmented with external data\nExploratory analysis\nNLP\nMachine Learning\nWriteup\n\nData sourcing and ingesting\nExploratory analysis\nModeling\nChallenges and Learnings\nConclusions\nFuture work"
  },
  {
    "objectID": "w01/slides.html#in-one-minute-of-time-2018",
    "href": "w01/slides.html#in-one-minute-of-time-2018",
    "title": "Week 1: Course Overview",
    "section": "In one minute of time (2018)",
    "text": "In one minute of time (2018)"
  },
  {
    "objectID": "w01/slides.html#in-one-minute-of-time-2019",
    "href": "w01/slides.html#in-one-minute-of-time-2019",
    "title": "Week 1: Course Overview",
    "section": "In one minute of time (2019)",
    "text": "In one minute of time (2019)"
  },
  {
    "objectID": "w01/slides.html#in-one-minute-of-time-2020",
    "href": "w01/slides.html#in-one-minute-of-time-2020",
    "title": "Week 1: Course Overview",
    "section": "In one minute of time (2020)",
    "text": "In one minute of time (2020)"
  },
  {
    "objectID": "w01/slides.html#in-one-minute-of-time-2021",
    "href": "w01/slides.html#in-one-minute-of-time-2021",
    "title": "Week 1: Course Overview",
    "section": "In one minute of time (2021)",
    "text": "In one minute of time (2021)"
  },
  {
    "objectID": "w01/slides.html#in-one-minute-of-time-2025",
    "href": "w01/slides.html#in-one-minute-of-time-2025",
    "title": "Week 1: Course Overview",
    "section": "In one minute of time (2025)",
    "text": "In one minute of time (2025)\nEvery 60 seconds in 2025:\n\nChatGPT serves millions of requests (exact numbers proprietary)\n500 hours of video uploaded to YouTube\n1.04 million Slack messages sent\n362,000 hours watched on Netflix\n5.9-11.4 million Google searches\n$443,000 spent on Amazon\nAI-generated images created at massive scale (metrics not publicly available)\n347,200 posts on X (formerly Twitter)\n231-250 million emails sent"
  },
  {
    "objectID": "w01/slides.html#a-lot-of-it-is-hapenning-online.",
    "href": "w01/slides.html#a-lot-of-it-is-hapenning-online.",
    "title": "Week 1: Course Overview",
    "section": "A lot of it is hapenning online.",
    "text": "A lot of it is hapenning online.\n\n\nWe can record every:\n\nclick\nad impression\nbilling event\nvideo interaction\nserver request\ntransaction\nnetwork message\nfault\n‚Ä¶"
  },
  {
    "objectID": "w01/slides.html#it-can-also-be-user-generated-content",
    "href": "w01/slides.html#it-can-also-be-user-generated-content",
    "title": "Week 1: Course Overview",
    "section": "It can also be user-generated content:",
    "text": "It can also be user-generated content:\n\n\n\n\nInstagram posts & Reels\nX (Twitter) posts & Threads\nTikTok videos\nYouTube Shorts\nReddit discussions\nDiscord conversations\nAI-generated content (text, images, code)\n‚Ä¶"
  },
  {
    "objectID": "w01/slides.html#but-health-and-scientific-computing-create-a-lot-too",
    "href": "w01/slides.html#but-health-and-scientific-computing-create-a-lot-too",
    "title": "Week 1: Course Overview",
    "section": "But health and scientific computing create a lot too!",
    "text": "But health and scientific computing create a lot too!"
  },
  {
    "objectID": "w01/slides.html#theres-lots-of-graph-data-too",
    "href": "w01/slides.html#theres-lots-of-graph-data-too",
    "title": "Week 1: Course Overview",
    "section": "There‚Äôs lots of graph data too",
    "text": "There‚Äôs lots of graph data too\n\n\n\nMany interesting datasets have a graph structure:\n\nSocial networks\nGoogle‚Äôs knowledge graph\nTelecom networks\nComputer networks\nRoad networks\nCollaboration/relationships\n\nSome of these are HUGE"
  },
  {
    "objectID": "w01/slides.html#apache-web-server-log-files",
    "href": "w01/slides.html#apache-web-server-log-files",
    "title": "Week 1: Course Overview",
    "section": "Apache (web server) log files",
    "text": "Apache (web server) log files"
  },
  {
    "objectID": "w01/slides.html#system-log-files",
    "href": "w01/slides.html#system-log-files",
    "title": "Week 1: Course Overview",
    "section": "System log files",
    "text": "System log files"
  },
  {
    "objectID": "w01/slides.html#internet-of-things-iot-in-2025",
    "href": "w01/slides.html#internet-of-things-iot-in-2025",
    "title": "Week 1: Course Overview",
    "section": "Internet of Things (IoT) in 2025",
    "text": "Internet of Things (IoT) in 2025\n75 billion connected devices generating data:\n\n\n\n\nSmart home devices (Alexa, Google Home, Apple HomePod)\nWearables (Apple Watch, Fitbit, Oura rings)\nConnected vehicles & self-driving cars\nIndustrial IoT sensors\n\n\n\n\nSmart city infrastructure\nMedical devices & remote patient monitoring"
  },
  {
    "objectID": "w01/slides.html#smartphone-location-data",
    "href": "w01/slides.html#smartphone-location-data",
    "title": "Week 1: Course Overview",
    "section": "Smartphone Location Data",
    "text": "Smartphone Location Data"
  },
  {
    "objectID": "w01/slides.html#where-else",
    "href": "w01/slides.html#where-else",
    "title": "Week 1: Course Overview",
    "section": "Where else?",
    "text": "Where else?\n\nThe Internet\nTransactions\nDatabases\nExcel\nPDF Files\nAnything digital (music, movies, apps)\nSome old floppy disk lying around the house"
  },
  {
    "objectID": "w01/slides.html#typical-real-world-scenarios-in-2025",
    "href": "w01/slides.html#typical-real-world-scenarios-in-2025",
    "title": "Week 1: Course Overview",
    "section": "Typical Real-World Scenarios in 2025",
    "text": "Typical Real-World Scenarios in 2025\nScenario 1: Traditional Big Data\nYou have a laptop with 16GB of RAM and a 256GB SSD. You are given a 1TB dataset in text files. What do you do?\nScenario 2: AI/ML Pipeline\nYour company wants to build a RAG system using 10TB of internal documents. You need sub-second query response times. How do you architect this?\nScenario 3: Real-Time Analytics\nYou need to process 1 million events/second from IoT devices and provide real-time dashboards with &lt;1s latency. What‚Äôs your stack?"
  },
  {
    "objectID": "w01/slides.html#lets-discuss",
    "href": "w01/slides.html#lets-discuss",
    "title": "Week 1: Course Overview",
    "section": "Let‚Äôs discuss!",
    "text": "Let‚Äôs discuss!\n\nExponential data growth"
  },
  {
    "objectID": "w01/slides.html#big-data-definitions",
    "href": "w01/slides.html#big-data-definitions",
    "title": "Week 1: Course Overview",
    "section": "Big Data Definitions",
    "text": "Big Data Definitions\nWikipedia\n‚ÄúA collection of datasets so large and complex that it becomes difficult to process using traditional tools and applications. Big Data technologies describe a new generation of technologies and architectures designed to economically extract value from very large volumes of a wide variety of data, by enabling high-velocity capture, discover and/or analysis‚Äù\nO‚ÄôReilly\n‚ÄúBig data is when the size of the data itself becomes part of the problem‚Äù"
  },
  {
    "objectID": "w01/slides.html#frameworks-for-thinking-about-big-data",
    "href": "w01/slides.html#frameworks-for-thinking-about-big-data",
    "title": "Week 1: Course Overview",
    "section": "Frameworks for Thinking About Big Data",
    "text": "Frameworks for Thinking About Big Data\nIBM (The 3 V‚Äôs)\n\nVolume (Gigabytes ‚Üí Exabytes ‚Üí Zettabytes)\nVelocity (Batch ‚Üí Streaming ‚Üí Real-time AI inference)\nVariety (Structured, Unstructured, Embeddings)\n\nAdditional V‚Äôs for 2025\n\n\n\nVariability\nVeracity\nVisualization\n\n\n\nValue\nVectors (embeddings for AI/ML)\nVersatility (multi-modal data)"
  },
  {
    "objectID": "w01/slides.html#data-size",
    "href": "w01/slides.html#data-size",
    "title": "Week 1: Course Overview",
    "section": "Data ‚ÄúSize‚Äù",
    "text": "Data ‚ÄúSize‚Äù\n\\[\n\\text{``Size''} = f(\\text{Processing Ability}, \\text{Storage Space})\n\\]\n\nCan you analyze/process your data on a single machine?\nCan you store (or is it stored) on a single machine?\nCan you serve it fast enough for real-time AI applications?\n\nIf any of of the answers is no then you have a big-ish data problem!"
  },
  {
    "objectID": "w01/slides.html#the-new-data-landscape-2025",
    "href": "w01/slides.html#the-new-data-landscape-2025",
    "title": "Week 1: Course Overview",
    "section": "The New Data Landscape (2025)",
    "text": "The New Data Landscape (2025)\nTraining Foundation Models\n\nGPT-4: Trained on about 13 trillion tokens\nLlama 3: 15 trillion tokens\nGoogle Gemini: Multi-modal training (text, images, video)\nEach iteration requires petabytes of curated data\n\nData Requirements Have Exploded\n\n2020: BERT trained on 3.3 billion words\n2023: GPT-4 trained on ~13 trillion tokens\n2024: Llama 3 trained on 15+ trillion tokens"
  },
  {
    "objectID": "w01/slides.html#big-data-infrastructure-data-lakes-warehouses",
    "href": "w01/slides.html#big-data-infrastructure-data-lakes-warehouses",
    "title": "Week 1: Course Overview",
    "section": "Big Data Infrastructure: Data Lakes, Warehouses",
    "text": "Big Data Infrastructure: Data Lakes, Warehouses\nTraditional Use Cases:\n\nBusiness intelligence\nAnalytics & reporting\nHistorical data storage\n\nModern AI Use Cases:\n\nTraining data repositories\nVector embeddings storage\nRAG (Retrieval-Augmented Generation) context\nFine-tuning datasets\nEvaluation & benchmark data"
  },
  {
    "objectID": "w01/slides.html#rag-and-context-engineering-the-new-data-pipeline",
    "href": "w01/slides.html#rag-and-context-engineering-the-new-data-pipeline",
    "title": "Week 1: Course Overview",
    "section": "RAG and Context Engineering: The New Data Pipeline",
    "text": "RAG and Context Engineering: The New Data Pipeline\n\n\n\n\n\n\n\nG\n\n\n\nraw\n\nRaw Data\n\n\n\nlake\n\nData Lake\n\n\n\nraw-&gt;lake\n\n\n\n\n\nproc\n\nProcessing\n\n\n\nlake-&gt;proc\n\n\n\n\n\nvec\n\nVector DB\n\n\n\nproc-&gt;vec\n\n\n\n\n\ncontext\n\nLLM Context\n\n\n\nvec-&gt;context\n\n\n\n\n\n\n\n\n\n\nKey Components:\n\nData Lakes (S3, Azure Data Lake): Store massive unstructured data\nData Warehouses (Snowflake, BigQuery): Structured data for context\nVector Databases (Pinecone, Weaviate, Qdrant): Semantic search\nEmbedding Models: Convert data to vectors\nOrchestration (Airflow, Prefect): Manage the pipeline"
  },
  {
    "objectID": "w01/slides.html#mcp-servers-agentic-ai",
    "href": "w01/slides.html#mcp-servers-agentic-ai",
    "title": "Week 1: Course Overview",
    "section": "MCP Servers & Agentic AI",
    "text": "MCP Servers & Agentic AI\nModel Context Protocol (MCP)\n\nOpen protocol for connecting AI assistants to data sources\nStandardized way to expose tools and data to LLMs\nEnables ‚Äúagentic‚Äù behavior - AI that can act autonomously\n\nMCP in Production\n\n\n\n\n\n\n\nG\n\n\n\nware\n\nData Warehouse\n\n\n\nmcp\n\nMCP Server\n\n\n\nware-&gt;mcp\n\n\n\n\n\nagent\n\nAI Agent\n\n\n\nmcp-&gt;agent\n\n\n\n\n\naction\n\nAction\n\n\n\nagent-&gt;action\n\n\n\n\n\n\n\n\n\n\nExamples:\n\nAI agents querying Snowflake for real-time analytics\nAutonomous systems updating data lakes based on predictions\nMulti-agent systems coordinating through shared data contexts"
  },
  {
    "objectID": "w01/slides.html#data-quality-for-ai",
    "href": "w01/slides.html#data-quality-for-ai",
    "title": "Week 1: Course Overview",
    "section": "Data Quality for AI",
    "text": "Data Quality for AI\n(Why Data Quality Matters More Than Ever)\nGarbage In, Garbage Out - Amplified:\n\nBad training data ‚Üí Biased models\nIncorrect RAG data ‚Üí Hallucinations\nPoor data governance ‚Üí Compliance issues\n\nData Quality Challenges in 2025\n\nScale: Validating trillions of tokens\nDiversity: Multi-modal, multi-lingual data\nVelocity: Real-time data for online learning\nVeracity: Detecting AI-generated synthetic data"
  },
  {
    "objectID": "w01/slides.html#real-world-big-data-ai-examples",
    "href": "w01/slides.html#real-world-big-data-ai-examples",
    "title": "Week 1: Course Overview",
    "section": "Real-World Big Data / AI Examples",
    "text": "Real-World Big Data / AI Examples\nNetflix\n\nData Scale: 260+ million subscribers generating 100+ billion events/day\nAI Use: Personalization, content recommendations, thumbnail generation\nStack: S3 ‚Üí Spark ‚Üí Iceberg ‚Üí ML models ‚Üí Real-time serving\n\nUber\n\nData Scale: 35+ million trips per day, petabytes of location data\nAI Use: ETA prediction, surge pricing, driver-rider matching\nStack: Kafka ‚Üí Spark Streaming ‚Üí Feature Store ‚Üí ML Platform\n\nOpenAI\n\nData Scale: Trillions of tokens for training, millions of queries/day\nAI Use: GPT models, DALL-E, embeddings\nStack: Distributed training ‚Üí Vector DBs ‚Üí Inference clusters"
  },
  {
    "objectID": "w01/slides.html#emerging-trends-2025-2027",
    "href": "w01/slides.html#emerging-trends-2025-2027",
    "title": "Week 1: Course Overview",
    "section": "Emerging Trends (2025-2027)",
    "text": "Emerging Trends (2025-2027)\nUnified Platforms:\n\nData lakes becoming ‚ÄúAI lakes‚Äù\nIntegrated vector + relational databases\nOne-stop shops for data + AI (Databricks, Snowflake Cortex)\n\nEdge Computing + AI:\n\nProcessing at the data source\nFederated learning across devices\n5G enabling real-time edge AI\n\nSynthetic Data:\n\nAI generating training data for AI\nPrivacy-preserving synthetic datasets"
  },
  {
    "objectID": "w01/slides.html#relative-data-sizes",
    "href": "w01/slides.html#relative-data-sizes",
    "title": "Week 1: Course Overview",
    "section": "Relative Data Sizes",
    "text": "Relative Data Sizes\n¬†\n\n\n\n\nCan be processed on single machine?\nNo\nMedium(Parallel Processing)\nBig!Parallel + Distributed Processing\n\n\nYes\nSmall(Your Laptop)\nMedium(Data Streaming)\n\n\n\n\nYes\nNo\n\n\n\n\nCan be stored on single machine?"
  },
  {
    "objectID": "w01/slides.html#what-youll-learn-in-this-course",
    "href": "w01/slides.html#what-youll-learn-in-this-course",
    "title": "Week 1: Course Overview",
    "section": "What You‚Äôll Learn in This Course",
    "text": "What You‚Äôll Learn in This Course\nModern Big Data Stack (2025)\n\n\n\nQuery Engines:\n\nDuckDB - In-process analytical database\nPolars - Lightning-fast DataFrame library\n\nSpark - Distributed processing at scale\n\nData Warehouses & Lakes:\n\nSnowflake - Cloud-native data warehouse\nAthena - Serverless SQL on S3\nIceberg - Open table format\n\n\n\nAI/ML Integration:\n\nVector databases for embeddings\nRAG implementation patterns\nStreaming with Spark Structured Streaming\n\nOrchestration:\n\nAirflow for pipeline management\nServerless with AWS Lambda"
  },
  {
    "objectID": "w01/slides.html#data-types",
    "href": "w01/slides.html#data-types",
    "title": "Week 1: Course Overview",
    "section": "Data Types",
    "text": "Data Types\n\nStructured\nUnstructured\nNatural language\nMachine-generated\nGraph-based\nAudio, video, and images\nStreaming"
  },
  {
    "objectID": "w01/slides.html#big-data-vs.-small-data-i",
    "href": "w01/slides.html#big-data-vs.-small-data-i",
    "title": "Week 1: Course Overview",
    "section": "Big Data vs.¬†Small Data I",
    "text": "Big Data vs.¬†Small Data I\n\n\n\n\n\n\n\n\n\nSmall Data is usually‚Ä¶\nOn the other hand, Big Data‚Ä¶\n\n\n\n\nGoals\ngathered for a specific goal\nmay have a goal in mind when it‚Äôs first started, but things can evolve or take unexpected directions\n\n\nLocation\nin one place, and often in a single computer file\ncan be in multiple files in multiple servers on computers in different geographic locations\n\n\nStructure/Contents\nhighly structured like an Excel spreadsheet, and it‚Äôs got rows and columns of data\ncan be unstructured, it can have many formats in files involved across disciplines, and may link to other resources\n\n\nPreparation\nprepared by the end user for their own purposes\nis often prepared by one group of people, analyzed by a second group of people, and then used by a third group of people, and they may have different purposes, and they may have different disciplines"
  },
  {
    "objectID": "w01/slides.html#big-data-vs.-small-data-ii",
    "href": "w01/slides.html#big-data-vs.-small-data-ii",
    "title": "Week 1: Course Overview",
    "section": "Big Data vs.¬†Small Data II",
    "text": "Big Data vs.¬†Small Data II\n\n\n\n\n\n\n\n\n\nSmall Data is usually‚Ä¶\nOn the other hand, Big Data‚Ä¶\n\n\n\n\nLongevity\nkept for a specific amount of time after the project is over because there‚Äôs a clear ending point. In the academic world it‚Äôs maybe five or seven years and then you can throw it away\ncontains data that must be stored in perpetuity. Many big data projects extend into the past and future\n\n\nMeasurements\nmeasured with a single protocol using set units and it‚Äôs usually done at the same time\nis collected and measured using many sources, protocols, units, etc\n\n\nReproducibility\nbe reproduced in their entirety if something goes wrong in the process\nreplication is seldom feasible\n\n\nStakes\nif things go wrong the costs are limited, it‚Äôs not an enormous problem\ncan have high costs of failure in terms of money, time and labor\n\n\nAccess\nidentified by a location specified in a row/column\nunless it is exceptionally well designed, the organization can be inscrutable\n\n\nAnalysis\nanalyzed together, all at once\nis ordinarily analyzed in incremental steps"
  },
  {
    "objectID": "w01/slides.html#challenges-of-working-with-large-datasets",
    "href": "w01/slides.html#challenges-of-working-with-large-datasets",
    "title": "Week 1: Course Overview",
    "section": "Challenges of Working with Large Datasets",
    "text": "Challenges of Working with Large Datasets\n\n\n\nThe V\nThe Challenge\n\n\n\n\nVolume\ndata scale\n\n\nValue\ndata usefulness in decision making\n\n\nVelocity\ndata processing: batch or stream\n\n\nViscosity\ndata complexity\n\n\nVariability\ndata flow inconsistency\n\n\nVolatility\ndata durability\n\n\nViability\ndata activeness\n\n\nValidity\ndata properly understandable\n\n\nVariety\ndata heterogeneity"
  },
  {
    "objectID": "w01/slides.html#thinking-about-big-data-workflows",
    "href": "w01/slides.html#thinking-about-big-data-workflows",
    "title": "Week 1: Course Overview",
    "section": "Thinking About Big Data Workflows",
    "text": "Thinking About Big Data Workflows\nWilliam Cohen (Director, Research Engineering, Google):\n\nWorking with big data is not about‚Ä¶\n\nCode optimization\nLearning the details of today‚Äôs hardware/software (they are evolving‚Ä¶)\n\nWorking with big data is about understanding:\n\nThe cost of what you want to do\nWhat the tools that are available offer\nHow much can be accomplished with linear or nearly-linear operations\nHow to organize your computations so that they effectively use whatever‚Äôs fast\nHow to test/debug/verify with large data\n\nRecall that traditional tools like R and Python are single threaded (by default)"
  },
  {
    "objectID": "w01/slides.html#tools-at-a-glance",
    "href": "w01/slides.html#tools-at-a-glance",
    "title": "Week 1: Course Overview",
    "section": "Tools at-a-glance",
    "text": "Tools at-a-glance\n\n\nLanguages, libraries, and projects\n\nPython\n\npandas\npolars\nPySpark\nduckdb\ndask\nray\n\nApache Arrow\nApache Spark\nSQL\nApache Hadoop (briefly)\n\n\nCloud Services\n\nAmazon Web Services (AWS)\n\nAWS Sagemaker\nAmazon S3\n\nAzure\n\nAzure Blob\nAzure Machine Learning\n\n\nOther:\n\nAWS Elastic MapReduce (EMR)"
  },
  {
    "objectID": "w01/slides.html#additional-links-of-interest",
    "href": "w01/slides.html#additional-links-of-interest",
    "title": "Week 1: Course Overview",
    "section": "Additional links of interest",
    "text": "Additional links of interest\n\nMatt Turck‚Äôs Machine Learning, Artificial Intelligence & Data Landscape (MAD)\n\nArticle\nInteractive Landscape\n\nIs there life after Hadoop?\n10 Best Big Data Tools for 2023"
  },
  {
    "objectID": "w01/slides.html#data-scientist-vs.-data-engineer",
    "href": "w01/slides.html#data-scientist-vs.-data-engineer",
    "title": "Week 1: Course Overview",
    "section": "Data Scientist vs.¬†Data Engineer",
    "text": "Data Scientist vs.¬†Data Engineer\nIn this course, you‚Äôll augment your data scientist skills with data engineering skills!"
  },
  {
    "objectID": "w01/slides.html#data-engineer-responsibilities",
    "href": "w01/slides.html#data-engineer-responsibilities",
    "title": "Week 1: Course Overview",
    "section": "Data Engineer Responsibilities",
    "text": "Data Engineer Responsibilities"
  },
  {
    "objectID": "w01/slides.html#data-engineering-levels-2-and-3",
    "href": "w01/slides.html#data-engineering-levels-2-and-3",
    "title": "Week 1: Course Overview",
    "section": "Data Engineering: Levels 2 and 3",
    "text": "Data Engineering: Levels 2 and 3"
  },
  {
    "objectID": "w01/slides.html#architecture",
    "href": "w01/slides.html#architecture",
    "title": "Week 1: Course Overview",
    "section": "Architecture",
    "text": "Architecture"
  },
  {
    "objectID": "w01/slides.html#storage",
    "href": "w01/slides.html#storage",
    "title": "Week 1: Course Overview",
    "section": "Storage",
    "text": "Storage"
  },
  {
    "objectID": "w01/slides.html#source-control",
    "href": "w01/slides.html#source-control",
    "title": "Week 1: Course Overview",
    "section": "Source control",
    "text": "Source control"
  },
  {
    "objectID": "w01/slides.html#orchestration",
    "href": "w01/slides.html#orchestration",
    "title": "Week 1: Course Overview",
    "section": "Orchestration",
    "text": "Orchestration"
  },
  {
    "objectID": "w01/slides.html#processing",
    "href": "w01/slides.html#processing",
    "title": "Week 1: Course Overview",
    "section": "Processing",
    "text": "Processing"
  },
  {
    "objectID": "w01/slides.html#analytics",
    "href": "w01/slides.html#analytics",
    "title": "Week 1: Course Overview",
    "section": "Analytics",
    "text": "Analytics"
  },
  {
    "objectID": "w01/slides.html#machine-learning",
    "href": "w01/slides.html#machine-learning",
    "title": "Week 1: Course Overview",
    "section": "Machine Learning",
    "text": "Machine Learning"
  },
  {
    "objectID": "w01/slides.html#governance",
    "href": "w01/slides.html#governance",
    "title": "Week 1: Course Overview",
    "section": "Governance",
    "text": "Governance"
  },
  {
    "objectID": "w01/slides.html#the-terminal",
    "href": "w01/slides.html#the-terminal",
    "title": "Week 1: Course Overview",
    "section": "The Terminal",
    "text": "The Terminal\n\n\n\n\nTerminal access was THE ONLY way to do programming\nNo GUIs! No Spyder, Jupyter, RStudio, etc.\nCoding is still more powerful than graphical interfaces for complex jobs\nCoding makes work repeatable"
  },
  {
    "objectID": "w01/slides.html#bash",
    "href": "w01/slides.html#bash",
    "title": "Week 1: Course Overview",
    "section": "BASH",
    "text": "BASH\n\n\n\n\nCreated in 1989 by Brian Fox: ‚ÄúBourne-Again Shell‚Äù\nBrian Fox also built the first online interactive banking software\nBASH is a command processor\nConnection between you and the machine language and hardware"
  },
  {
    "objectID": "w01/slides.html#the-prompt",
    "href": "w01/slides.html#the-prompt",
    "title": "Week 1: Course Overview",
    "section": "The Prompt",
    "text": "The Prompt\nusername@hostname:current_directory $\nWhat do we learn from the prompt?\n\nWho you are - username\nThe machine where your code is running - hostname\nThe directory where your code is running - current_directory\nThe shell type - $ - this symbol means BA$H"
  },
  {
    "objectID": "w01/slides.html#syntax",
    "href": "w01/slides.html#syntax",
    "title": "Week 1: Course Overview",
    "section": "Syntax",
    "text": "Syntax\nCOMMAND -F --FLAG\n\nCOMMAND is the program, everything after that = arguments\nF is a single letter flag, FLAG is a single word or words connected by dashes. A space breaks things into a new argument.\nSometimes argument has single letter and long form versions (e.g.¬†F and FLAG)\n\nCOMMAND -F --FILE file1\n\nHere we pass a text argument \"file1\" as the value for the FILE flag\n-h flag is usually to get help. You can also run the man command and pass the name of the program as the argument to get the help page.\n\nLet‚Äôs try basic commands:\n\ndate to get the current date\nwhoami to get your user name\necho \"Hello World\" to print to the console"
  },
  {
    "objectID": "w01/slides.html#examining-files",
    "href": "w01/slides.html#examining-files",
    "title": "Week 1: Course Overview",
    "section": "Examining Files",
    "text": "Examining Files\n\nFind out your Present Working Directory pwd\nExamine the contents of files and folders using ls\nMake new files from scratch using touch\nGlob: ‚ÄúMini-language‚Äù for selecting files with wildcards\n\n\\* for wild card any number of characters\n\\? for wild card for a single character\n[] for one of many character options\n! for exclusion\n[:alpha:], [:alnum:], [:digit:], [:lower:], [:upper:]\n\n\nReference material: Shell Lesson 1,2,4,5"
  },
  {
    "objectID": "w01/slides.html#navigating-directories",
    "href": "w01/slides.html#navigating-directories",
    "title": "Week 1: Course Overview",
    "section": "Navigating Directories",
    "text": "Navigating Directories\n\nKnowing where your terminal is executing code ensures you are working with the right inputs/outputs.\nUse pwd to determine the Present Working Directory.\nChange to a folder called ‚Äúgit-repo‚Äù with cd git-repo.\n. refers to the current directory, such as ./git-repo\n.. can be used to move up one level (cd ..), and can be combined to move up multiple levels (cd ../../my_folder)\n/ is the root of the filesystem: contains core folders (system, users)\n~ is the home directory. Move to folders referenced relative to this path by including it at the start of your path, for example ~/projects.\nTo visualize the structure of your working directory, use tree\n\nReference link"
  },
  {
    "objectID": "w01/slides.html#interacting-with-files",
    "href": "w01/slides.html#interacting-with-files",
    "title": "Week 1: Course Overview",
    "section": "Interacting with Files",
    "text": "Interacting with Files\nNow that we know how to navigate through directories, we need commands for interacting with files‚Ä¶\n\nmv to move files from one location to another\n\nCan use glob here - ?, *, [], ‚Ä¶\n\ncp to copy files instead of moving\n\nCan use glob here - ?, *, [], ‚Ä¶\n\nmkdir to make a directory\nrm to remove files\nrmdir to remove directories\nrm -rf to blast everything! WARNING!!! DO NOT USE UNLESS YOU KNOW WHAT YOU ARE DOING"
  },
  {
    "objectID": "w01/slides.html#using-bash-for-data-exploration",
    "href": "w01/slides.html#using-bash-for-data-exploration",
    "title": "Week 1: Course Overview",
    "section": "Using BASH for Data Exploration",
    "text": "Using BASH for Data Exploration\n\nhead FILENAME / tail FILENAME - glimpsing the first / last few rows of data\nmore FILENAME / less FILENAME - viewing the data with basic up / (up & down) controls\ncat FILENAME - print entire file contents into terminal\nvim FILENAME - open (or edit!) the file in vim editor\ngrep FILENAME - search for lines within a file that match a regex expression\nwc FILENAME - count the number of lines (-l flag) or number of words (-w flag)\n\nReference material: Text Lesson 8,9,15,16"
  },
  {
    "objectID": "w01/slides.html#pipes-and-arrows",
    "href": "w01/slides.html#pipes-and-arrows",
    "title": "Week 1: Course Overview",
    "section": "Pipes and Arrows",
    "text": "Pipes and Arrows\n\n| sends the stdout to another command (is the most powerful symbol in BASH!)\n&gt; sends stdout to a file and overwrites anything that was there before\n&gt;&gt; appends the stdout to the end of a file (or starts a new file from scratch if one does not exist yet)\n&lt; sends stdin into the command on the left\n\nReference material: Text Lesson 1,2,3,4,5"
  },
  {
    "objectID": "w01/slides.html#alias-and-user-files",
    "href": "w01/slides.html#alias-and-user-files",
    "title": "Week 1: Course Overview",
    "section": "Alias and User Files",
    "text": "Alias and User Files\n\n/.bashrc is where your shell settings are located\nHow many processes? whoami | xargs ps -u | wc -l\nHard to remember full command! Let‚Äôs make an alias\nGeneral syntax:\nalias alias_name=\"command_to_run\"\nFor our case:\nalias nproc=\"whoami | xargs ps -u | wc -l\"\nNow we need to put this alias into the .bashrc\nalias nproc=\"whoami | xargs ps -u | wc -l\" &gt;&gt; ~/.bashrc\nYour commands get saved in ~/.bash_history"
  },
  {
    "objectID": "w01/slides.html#process-management",
    "href": "w01/slides.html#process-management",
    "title": "Week 1: Course Overview",
    "section": "Process Management",
    "text": "Process Management\n\nUse ps to see your running processes\nUse top or even better htop to see all running processes\nInstall htop via sudo yum install htop -y\nTo kill a broken process: first find the process ID (PID)\nThen use kill [PID NUM] to ‚Äúask‚Äù the process to terminate. If things get really bad: kill -9 [PID NUM]\nTo kill a command in the terminal window it is running in, try using Ctrl + C or Ctrl + /\nRun cat on its own to let it stay open. Now open a new terminal to examine the processes and find the cat process.\n\nReference material: Text Lesson 1,2,3,7,9,10"
  },
  {
    "objectID": "w01/slides.html#try-playing-a-linux-game",
    "href": "w01/slides.html#try-playing-a-linux-game",
    "title": "Week 1: Course Overview",
    "section": "Try playing a Linux game!",
    "text": "Try playing a Linux game!\nBash crawl is a game to help you practice your navigation and file access skills. Click on the binder link in this repo to launch a jupyter lab session and explore!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN 6000 Sections 01 and 02",
    "section": "",
    "text": "This webpage just serves as a hub collecting Jeff‚Äôs slides for DSAN 6000: Big Data and Cloud Computing, Fall 2025 at Georgetown University. It is not a replacement for the main course webpage!\nSection 01 of the course takes place on Wednesday from 3:30pm to 6:00pm in Walsh 394.\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nWeek 1: Course Overview\n\n\nAug 28\n\n\n\n\n\n\nWeek 2: Cloud Computing\n\n\nSep 2\n\n\n\n\n\n\nWeek 3: Parallelization Concepts\n\n\nSep 8\n\n\n\n\n\n\nWeek 4: DuckDB, Polars, File Formats\n\n\nSep 15\n\n\n\n\n\n\nWeek 5: Data Warehouse (Athena, Presto, Snowflake)\n\n\nSep 22\n\n\n\n\n\n\nWeek 6: Introduction to Spark, RDDs\n\n\nSep 29\n\n\n\n\n\n\nWeek 7: Spark DataFrames and Spark SQL\n\n\nOct 6\n\n\n\n\n\n\nWeek 8: Spark ML and Streaming\n\n\nOct 20\n\n\n\n\n\n\nWeek 9: Apache Iceberg, Table Formats\n\n\nOct 27\n\n\n\n\n\n\nWeek 10: Data Pipeline Orchestration with Airflow\n\n\nNov 3\n\n\n\n\n\n\nWeek 11: Vector Databases, RAG\n\n\nNov 10\n\n\n\n\n\n\nWeek 12: Modern Data Stack, Governance\n\n\nNov 17\n\n\n\n\n\n\nWeek 13: Serverless and Container Orchestration\n\n\nNov 24\n\n\n\n\n\n\nWeek 14: Final Topics, Review\n\n\nDec 1\n\n\n\n\n\n\nWeek 15: Final Project Presentations\n\n\nDec 8\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "w03/slides.html#looking-back",
    "href": "w03/slides.html#looking-back",
    "title": "Week 3: Parallelization Concepts",
    "section": "Looking back",
    "text": "Looking back\n\nContinued great use of Slack!\n\nNice interactions\n\nDue date reminders:\n\nAssignment 2: September 17, 2025\nLab 3: September 17, 2025\nAssignment 3: September 24, 2025"
  },
  {
    "objectID": "w03/slides.html#glossary",
    "href": "w03/slides.html#glossary",
    "title": "Week 3: Parallelization Concepts",
    "section": "Glossary",
    "text": "Glossary\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nLocal\nYour current workstation (laptop, desktop, etc.), wherever you start the terminal/console application.\n\n\nRemote\nAny machine you connect to via ssh or other means.\n\n\nEC2\nSingle virtual machine in the cloud where you can run computation\n\n\nSageMaker\nIntegrated Developer Environment where you can conduct data science on single machines or distributed training\n\n\nGPU\nGraphics Processing Unit - specialized hardware for parallel computation, essential for AI/ML\n\n\nTPU\nTensor Processing Unit - Google‚Äôs custom AI accelerator chips\n\n\nEphemeral\nLasting for a short time - any machine that will get turned off or place you will lose data\n\n\nPersistent\nLasting for a long time - any environment where your work is NOT lost when the timer goes off"
  },
  {
    "objectID": "w03/slides.html#quick-survey-question-for-intuition-building",
    "href": "w03/slides.html#quick-survey-question-for-intuition-building",
    "title": "Week 3: Parallelization Concepts",
    "section": "Quick Survey Question, for Intuition-Building",
    "text": "Quick Survey Question, for Intuition-Building\n\nAre humans capable of ‚Äútrue‚Äù multi-tasking?\n\nAs in, doing two things at the exact same time?\n\n(Or, do we instead rapidly switch back and forth between tasks?)"
  },
  {
    "objectID": "w03/slides.html#the-answer",
    "href": "w03/slides.html#the-answer",
    "title": "Week 3: Parallelization Concepts",
    "section": "The Answer",
    "text": "The Answer\n\n(From what we understand, at the moment, by way of studies in neuroscience/cognitive science/etc‚Ä¶)\nHumans are not capable of true multitasking! In CS terms, this would be called multiprocessing (more on this later)\nWe are capable, however, of various modes of concurrency!\n\n\n\n\n\n\n\n\n\n\nMultithreading\nAsynchronous Execution\n\n\n\n\nUnconsciously(you do it already, ‚Äúnaturally‚Äù)\nFocus on one speaker within a loud room, with tons of other conversations entering your ears\nPut something in oven, set alarm, go do something else, take out of oven once alarm goes off\n\n\nConsciously(you can do it with effort/practice)\nPat head (up and down) and rub stomach (circular motion) ‚Äúsimultaneously‚Äù\nThrow a ball in the air, clap 3 times, catch ball"
  },
  {
    "objectID": "w03/slides.html#helpful-specifically-for-programming",
    "href": "w03/slides.html#helpful-specifically-for-programming",
    "title": "Week 3: Parallelization Concepts",
    "section": "Helpful Specifically for Programming",
    "text": "Helpful Specifically for Programming\n\nCourse notes from MIT‚Äôs class on parallel computing phrases it like: if implemented thoughtfully, concurrency is a power multiplier for your code (do 10 things in 1 second instead of 10 seconds‚Ä¶)"
  },
  {
    "objectID": "w03/slides.html#helpful-in-general-as-a-way-of-thinking",
    "href": "w03/slides.html#helpful-in-general-as-a-way-of-thinking",
    "title": "Week 3: Parallelization Concepts",
    "section": "Helpful In General as a Way of Thinking!",
    "text": "Helpful In General as a Way of Thinking!\n\nSay you get hired as a Project Manager‚Ä¶\nPart of your job will fundamentally involve pipelines!\n\nNeed to know when Task \\(B\\) does/does not require Task \\(A\\) as a prerequisite\nNeed to know whether Task \\(A\\) and Task \\(B\\) can share one resource or need their own individual resources\nOnce Task \\(A\\) and \\(B\\) both complete, how do we merge their results together?"
  },
  {
    "objectID": "w03/slides.html#avoiding-the-rabbithole",
    "href": "w03/slides.html#avoiding-the-rabbithole",
    "title": "Week 3: Parallelization Concepts",
    "section": "Avoiding the Rabbithole",
    "text": "Avoiding the Rabbithole\n\nParallel computing is a rabbithole, but one you can safely avoid via simple heuristics (‚Äúrules of thumb‚Äù)!\n\n\nCheck for optimizations to serial code first,\nCheck for embarrassingly parallel code blocks\nUse map-reduce approach for more complicated cases"
  },
  {
    "objectID": "w03/slides.html#typical-real-world-scenarios",
    "href": "w03/slides.html#typical-real-world-scenarios",
    "title": "Week 3: Parallelization Concepts",
    "section": "Typical Real-World Scenarios",
    "text": "Typical Real-World Scenarios\n\nYou need to prepare training data for LLMs by cleaning and deduplicating 100TB of web-scraped text\nYou are building a RAG system that requires embedding and indexing millions of documents in parallel\nYou need to extract structured data from millions of PDFs using vision models for document AI\nYou are preprocessing multimodal datasets with billions of image-text pairs for foundation model training\nYou need to run quality filtering on petabytes of Common Crawl data for training dataset\nYou are generating synthetic training data using LLMs to augment limited real-world datasets\nYou need to transform and tokenize text across 100+ languages for multilingual AI\nYou are building real-time data pipelines that process streaming data for online learning"
  },
  {
    "objectID": "w03/slides.html#embarrassingly-parallel-pipelines",
    "href": "w03/slides.html#embarrassingly-parallel-pipelines",
    "title": "Week 3: Parallelization Concepts",
    "section": "‚ÄúEmbarrassingly Parallel‚Äù Pipelines",
    "text": "‚ÄúEmbarrassingly Parallel‚Äù Pipelines\n\nTechnical definition: tasks within pipeline can easily be parallelized bc no dependence and no need for communication (triple spatula!)"
  },
  {
    "objectID": "w03/slides.html#parallelizing-non-embarrassingly-parallel-pipelines",
    "href": "w03/slides.html#parallelizing-non-embarrassingly-parallel-pipelines",
    "title": "Week 3: Parallelization Concepts",
    "section": "Parallelizing Non-Embarrassingly-Parallel Pipelines",
    "text": "Parallelizing Non-Embarrassingly-Parallel Pipelines"
  },
  {
    "objectID": "w03/slides.html#buzzkill-complications-to-come",
    "href": "w03/slides.html#buzzkill-complications-to-come",
    "title": "Week 3: Parallelization Concepts",
    "section": "Buzzkill: Complications to Come üò∞",
    "text": "Buzzkill: Complications to Come üò∞\n\nIf it‚Äôs such a magical powerup, shouldn‚Äôt we just parallelize everything? Answer: No üòû bc overhead.\nOverhead source 1: Sending tasks to workers (processors), collecting results\nOverhead source 2: Even after setting up new stacks and heaps, threads may need to communicate with each other (e.g.¬†if they need to synchronize at some point(s))\nIn fact, probably the earliest super-popular parallelization library was created to handle Source 2, not Source 1: Message Passing Interface (C, C++, and Fortran)"
  },
  {
    "objectID": "w03/slides.html#rules-of-thumb-for-parallelization",
    "href": "w03/slides.html#rules-of-thumb-for-parallelization",
    "title": "Week 3: Parallelization Concepts",
    "section": "Rules of Thumb for Parallelization",
    "text": "Rules of Thumb for Parallelization\n\n\nYes - Parallelize These\n\nData Preparation:\n\nText extraction from documents\nTokenization of text corpora\nImage preprocessing\nEmbedding generation for documents\nData quality filtering and validation\nFormat conversions (audio features)\nWeb scraping and data collection\nSynthetic data generation\n\nData Processing:\n\nBatch inference on datasets\nFeature extraction at scale\nData deduplication (local)\n\n\n\nNo - Keep Sequential\n\nOrder-Dependent:\n\nConversation threading\nTime-series preprocessing\nSequential data validation\nCumulative statistics\n\nGlobal Operations:\n\nGlobal deduplication\nCross-dataset joins\nComputing exact quantiles\n\n\n\n\n\n\n\nFor data operations in the ‚ÄúNo‚Äù column, they often require global coordination or maintain strict ordering. However, many can be approximated with parallel algorithms (like approximate deduplication with locality-sensitive hashing)"
  },
  {
    "objectID": "w03/slides.html#in-action",
    "href": "w03/slides.html#in-action",
    "title": "Week 3: Parallelization Concepts",
    "section": "In Action",
    "text": "In Action\n\nimport time\nfrom sympy.ntheory import factorint\nfrom joblib import Parallel, delayed\nparallel_runner = Parallel(n_jobs=4)\nstart, end = 500, 580\ndef find_prime_factors(num):\n  time.sleep(.01)\n  return factorint(num, multiple=True)\ndisp_time = lambda start, end: print('{:.4f} s'.format(end - start))\n\n\n\n\nserial_start = time.time()\nresult = [\n  (i,find_prime_factors(i))\n  for i in range(start, end+1)\n]\nserial_end = time.time()\ndisp_time(serial_start, serial_end)\n\n0.9942 s\n\n\n\n\npar_start = time.time()\nresult = parallel_runner(\n  delayed(find_prime_factors)(i)\n  for i in range(start, end+1)\n)\npar_end = time.time()\ndisp_time(par_start, par_end)\n\n2.3756 s"
  },
  {
    "objectID": "w03/slides.html#what-happens-when-not-embarrassingly-parallel",
    "href": "w03/slides.html#what-happens-when-not-embarrassingly-parallel",
    "title": "Week 3: Parallelization Concepts",
    "section": "What Happens When Not Embarrassingly Parallel?",
    "text": "What Happens When Not Embarrassingly Parallel?\n\nThink of the difference between linear and quadratic equations in algebra:\n\\(3x - 1 = 0\\) is ‚Äúembarrassingly‚Äù solvable, on its own: you can solve it directly, by adding 3 to both sides \\(\\implies x = \\frac{1}{3}\\). Same for \\(2x + 3 = 0 \\implies x = -\\frac{3}{2}\\)\nNow consider \\(6x^2 + 7x - 3 = 0\\): Harder to solve ‚Äúdirectly‚Äù, so your instinct might be to turn to the laborious quadratic equation:\n\n\\[\n\\begin{align*}\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} = \\frac{-7 \\pm \\sqrt{49 - 4(6)(-3)}}{2(6)} = \\frac{-7 \\pm 11}{12} = \\left\\{\\frac{1}{3},-\\frac{3}{2}\\right\\}\n\\end{align*}\n\\]\n\nAnd yet, \\(6x^2 + 7x - 3 = (3x - 1)(2x + 3)\\), meaning that we could have split the problem into two ‚Äúembarrassingly‚Äù solvable pieces, then multiplied to get result!"
  },
  {
    "objectID": "w03/slides.html#the-analogy-to-map-reduce",
    "href": "w03/slides.html#the-analogy-to-map-reduce",
    "title": "Week 3: Parallelization Concepts",
    "section": "The Analogy to Map-Reduce",
    "text": "The Analogy to Map-Reduce\n\n\n\n\n\n\n\n\\(\\leadsto\\) If code is not embarrassingly parallel (instinctually requiring laborious serial execution),\n\\(\\underbrace{6x^2 + 7x - 3 = 0}_{\\text{Solve using Quadratic Eqn}}\\)\n\n\nBut can be split into‚Ä¶\n\\((3x - 1)(2x + 3) = 0\\)\n\n\nEmbarrassingly parallel pieces which combine to same result,\n\\(\\underbrace{3x - 1 = 0}_{\\text{Solve directly}}, \\underbrace{2x + 3 = 0}_{\\text{Solve directly}}\\)\n\n\nWe can use map-reduce to achieve ultra speedup (running ‚Äúpieces‚Äù on GPU!)\n\\(\\underbrace{(3x-1)(2x+3) = 0}_{\\text{Solutions satisfy this product}}\\)"
  },
  {
    "objectID": "w03/slides.html#the-direct-analogy-map-reduce",
    "href": "w03/slides.html#the-direct-analogy-map-reduce",
    "title": "Week 3: Parallelization Concepts",
    "section": "The Direct Analogy: Map-Reduce!",
    "text": "The Direct Analogy: Map-Reduce!\n\nProblem from DSAN 5000/5100: Computing SSR (Sum of Squared Residuals)\n\\(y = (1,3,2), \\widehat{y} = (2, 5, 0) \\implies \\text{SSR} = (1-2)^2 + (3-5)^2 + (2-0)^2 = 9\\)\nComputing pieces separately:\nmap(do_something_with_piece, list_of_pieces)\n\nmy_map = map(lambda input: input**2, [(1-2), (3-5), (2-0)])\nmap_result = list(my_map)\nmap_result\n\n[1, 4, 4]\n\n\nCombining solved pieces\nreduce(how_to_combine_pair_of_pieces, pieces_to_combine)\n\nfrom functools import reduce\nmy_reduce = reduce(lambda piece1, piece2: piece1 + piece2, map_result)\nmy_reduce\n\n9"
  },
  {
    "objectID": "w03/slides.html#functions-vs.-functionals",
    "href": "w03/slides.html#functions-vs.-functionals",
    "title": "Week 3: Parallelization Concepts",
    "section": "Functions vs.¬†Functionals",
    "text": "Functions vs.¬†Functionals\nYou may have noticed: map() and reduce() are ‚Äúmeta-functions‚Äù: functions that take other functions as inputs\n\n\n\ndef add_5(num):\n  return num + 5\nadd_5(10)\n\n15\n\n\n\n\ndef apply_twice(fn, arg):\n  return fn(fn(arg))\napply_twice(add_5, 10)\n\n20\n\n\n\nIn Python, functions can be used as vars (Hence lambda):\n\nadd_5 = lambda num: num + 5\napply_twice(add_5, 10)\n\n20\n\n\nThis relates to a whole paradigm, ‚Äúfunctional programming‚Äù: mostly outside scope of course, but lots of important+useful takeaways/rules-of-thumb!"
  },
  {
    "objectID": "w03/slides.html#train-your-brain-for-functional-approach-implies-master-debugging",
    "href": "w03/slides.html#train-your-brain-for-functional-approach-implies-master-debugging",
    "title": "Week 3: Parallelization Concepts",
    "section": "Train Your Brain for Functional Approach \\(\\implies\\) Master Debugging!",
    "text": "Train Your Brain for Functional Approach \\(\\implies\\) Master Debugging!\n\nIn CS Theory: enables formal proofs of correctness\nIn CS practice:\n\nWhen a program doesn‚Äôt work, each function is an interface point where you can check that the data are correct. You can look at the intermediate inputs and outputs to quickly isolate the function that‚Äôs responsible for a bug.(from Python‚Äôs ‚ÄúFunctional Programming HowTo‚Äù)"
  },
  {
    "objectID": "w03/slides.html#code-rightarrow-pipelines-rightarrow-debuggable-pipelines",
    "href": "w03/slides.html#code-rightarrow-pipelines-rightarrow-debuggable-pipelines",
    "title": "Week 3: Parallelization Concepts",
    "section": "Code \\(\\rightarrow\\) Pipelines \\(\\rightarrow\\) Debuggable Pipelines",
    "text": "Code \\(\\rightarrow\\) Pipelines \\(\\rightarrow\\) Debuggable Pipelines\n\n\nScenario: Run code, check the output, and‚Ä¶ it‚Äôs wrong üòµ what do you do?\nUsual approach: Read lines one-by-one, figuring out what they do, seeing if something pops out that seems wrong; adding comments like # Convert to lowercase\n\n\n\n\nEasy case: found typo in punctuation removal code. Fix the error, add comment like # Remove punctuation\n Rule 1 of FP: transform these comments into function names\n\n\n\nHard case: Something in load_text() modifies a variable that later on breaks remove_punct() (Called a side-effect)\n Rule 2 of FP: NO SIDE-EFFECTS!\n\n\n\n\n\n\n\n\n\nG\n\n\n\ninput\nin.txt\n\n\n\nload_text\nload_text\n(Verb)\n\n\n\n\ninput-&gt;load_text\n\n\n\n\n\nlowercase\nlowercase\n(Verb)\n\n\n\n\nload_text-&gt;lowercase\n\n\nüßê ‚úÖ\n\n\n\nremove_punct\nremove_punct\n(Verb)\n\n\n\n\nlowercase-&gt;remove_punct\n\n\nüßê ‚úÖ\n\n\n\nremove_stopwords\nremove_stopwords\n(Verb)\n\n\n\n\nremove_punct-&gt;remove_stopwords\n\n\nüßê ‚ùå‚ùóÔ∏è\n\n\n\noutput\nout.txt\n\n\n\nremove_stopwords-&gt;output\n\n\n\n\n\n\n (Does this way of diagramming a program look familiar?) \n\n\n\n\nWith side effects: ‚ùå \\(\\implies\\) issue is somewhere earlier in the chain üò©üèÉ‚Äç‚ôÇÔ∏è\nNo side effects: ‚ùå \\(\\implies\\) issue must be in remove_punct()!!! üòé ‚è±Ô∏è = üí∞"
  },
  {
    "objectID": "w03/slides.html#if-its-so-useful-why-doesnt-everyone-do-it",
    "href": "w03/slides.html#if-its-so-useful-why-doesnt-everyone-do-it",
    "title": "Week 3: Parallelization Concepts",
    "section": "If It‚Äôs So Useful, Why Doesn‚Äôt Everyone Do It?",
    "text": "If It‚Äôs So Useful, Why Doesn‚Äôt Everyone Do It?\n\nTrapped in imperative (sequential) coding mode: Path dependency / QWERTY\nWe need to start thinking like this bc, 1000x harder to debug parallel code! So we need to be less ad hoc in how we write+debug, from here on out! üôá‚Äç‚ôÇÔ∏èüôè\n\n\nFrom Leskovec, Rajaraman, and Ullman (2014)\nThe title relates to a classic Economics joke (the best kind of joke): ‚ÄúAn economist and a CEO are walking down the street, when the CEO points at the ground and tells the economist, ‚Äòlook! A $20 bill on the ground!‚Äô The economist keeps on walking, scoffing at the CEO: ‚Äòdon‚Äôt be silly, if there was a $20 bill on the ground, somebody would have picked it up already‚Äô.‚Äù"
  },
  {
    "objectID": "w03/slides.html#the-killer-application-matrix-multiplication",
    "href": "w03/slides.html#the-killer-application-matrix-multiplication",
    "title": "Week 3: Parallelization Concepts",
    "section": "The ‚ÄúKiller Application‚Äù: Matrix Multiplication",
    "text": "The ‚ÄúKiller Application‚Äù: Matrix Multiplication\n\n(I learned from Jeff Ullman, who did the obnoxious Stanford thing of mentioning in passing how ‚Äútwo previous students in the class did this for a cool final project on web crawling and, well, it escalated quickly‚Äù, aka became Google)\n\n\nFrom Leskovec, Rajaraman, and Ullman (2014), which is (legally) free online!"
  },
  {
    "objectID": "w03/slides.html#the-killer-way-to-learn-text-counts",
    "href": "w03/slides.html#the-killer-way-to-learn-text-counts",
    "title": "Week 3: Parallelization Concepts",
    "section": "The Killer Way-To-Learn: Text Counts!",
    "text": "The Killer Way-To-Learn: Text Counts!\n\n(2014): Text counts (2.2) \\(\\rightarrow\\) Matrix multiplication (2.3) \\(\\rightarrow \\cdots \\rightarrow\\) PageRank (5.1)\nThe goal: User searches ‚ÄúDenzel Curry‚Äù‚Ä¶ How relevant is a given webpage?\nScenario 1: Entire internet fits on CPU \\(\\implies\\) We can just make a big dict:\n\n\n\n\n\n\n\n\nG\n\n\n\ninternet\n\nScan in O(n):\nToday Denzel Washington\nate a big bowl of Yum's\ncurry. Denzel allegedly\nrubbed his tum and said\n\"yum yum yum\" when he\ntasted today's curry.\n\"Yum! It is me Denzel,\ncurry is my fav!\", he\nexclaimed. According to\nhis friend Steph, curry\nis indeed Denzel's fav.\nWe are live with Del\nCurry in Washington for\na Denzel curry update.\n\n\n\n\nccounts\n\nOverall Counts\n\n('according',1)\n('allegedly',1)\n('ate',1)\n('big',1)\n('bowl',1)\n('curry',6)\n('del',1)\n('denzel',5)\n('exclaimed',1)\n('fav',2)\n('friend',1)\n('indeed',1)\n('live',1)\n('rubbed',1)\n('said',1)\n('steph',1)\n('tasted',1)\n('today',2)\n('tum',1)\n('update',1)\n('washington',2)\n('yum',4)\n\n\n\n\ninternet-&gt;ccounts\n\n\nLoop Over Words"
  },
  {
    "objectID": "w03/slides.html#if-everything-doesnt-fit-on-cpu",
    "href": "w03/slides.html#if-everything-doesnt-fit-on-cpu",
    "title": "Week 3: Parallelization Concepts",
    "section": "If Everything Doesn‚Äôt Fit on CPU‚Ä¶",
    "text": "If Everything Doesn‚Äôt Fit on CPU‚Ä¶\n\nFrom Cornell Virtual Workshop, ‚ÄúUnderstanding GPU Architecture‚Äù"
  },
  {
    "objectID": "w03/slides.html#break-problem-into-chunks-for-the-green-bois",
    "href": "w03/slides.html#break-problem-into-chunks-for-the-green-bois",
    "title": "Week 3: Parallelization Concepts",
    "section": "Break Problem into Chunks for the Green Bois!",
    "text": "Break Problem into Chunks for the Green Bois!\n\n\n\n\n\n\n\nG\n\n\n\nchunked\n\nChunked Document\n\nToday Denzel Washington\nate a big bowl of Yum's\ncurry. Denzel allegedly\nrubbed his tum and said\n\n\"yum yum yum\" when he\ntasted today's curry.\n\"Yum! It is me Denzel,\ncurry is my fav!\", he\n\nexclaimed. According to\nhis friend Steph, curry\nis indeed Denzel's fav.\nWe are live with Del\nCurry in Washington for\na Denzel curry update.\n\n\n\n\nchcounts\n\nChunked Counts\n\n('today',1)\n('denzel',1)\n...\n('tum',1)\n('said',1)\n\n('yum',1)\n('yum',1)\n('yum',1)\n...\n('fav',1)\n\n('exclaimed',1)\n...\n('del',1)\n('curry',1)\n('washington',1)\n('denzel',1)\n('curry',1)\n('update',1)\n\n\n\n\nchunked:p1-&gt;chcounts:p1\n\n\nO(n/4)\n\n\n\nchunked:p2-&gt;chcounts:p2\n\n\nO(n/4)\n\n\n\nchunked:p3-&gt;chcounts:p3\n\n\nO(n/4)\n\n\n\nchunked:p4-&gt;chcounts:p4\n\n\nO(n/4)\n\n\n\nscounts\n\nHashed Counts\n\n('allegedly',1)\n...\n('curry',1)\n('denzel',2)\n...\n('yum',1)\n\n('curry',2)\n('denzel',1)\n...\n('yum',4)\n\n('according',1)\n('curry',1)\n('del',1)\n('denzel',1)\n...\n('curry',2)\n('denzel',1)\n('update',1)\n('washington',1)\n\n\n\n\nchcounts:p1-&gt;scounts:p1\n\n\nO(n/4)\n\n\n\nchcounts:p2-&gt;scounts:p2\n\n\nO(n/4)\n\n\n\nchcounts:p3-&gt;scounts:p3\n\n\nO(n/4)\n\n\n\nchcounts:p4-&gt;scounts:p4\n\n\nO(n/4)\n\n\n\nccounts\n\nOverall Counts\n('according',1)\n('allegedly',1)\n('ate',1)\n('big',1)\n('bowl',1)\n('curry',6)\n('del',1)\n('denzel',5)\n('exclaimed',1)\n('fav',2)\n('friend',1)\n('indeed',1)\n('live',1)\n('rubbed',1)\n('said',1)\n('steph',1)\n('tasted',1)\n('today',2)\n('tum',1)\n('update',1)\n('washington',2)\n('yum',4)\n\n\n\n\nscounts:p1-&gt;ccounts:p1\n\n\n\n\n\nscounts:p2-&gt;ccounts:p1\n\n\n\n\n\nscounts:p3-&gt;ccounts:p1\n\n\n\n\n\nscounts:p4-&gt;ccounts:p1\n\n\n\n\n\nscounts:p2-&gt;ccounts\n\n\n \n\nMerge in\nO(n)\n\n\n\n\n\n\n\n\n\n\\(\\implies\\) Total = \\(O(3n) = O(n)\\)\nBut also optimized in terms of constants, because of sequential memory reads"
  },
  {
    "objectID": "w03/slides.html#references",
    "href": "w03/slides.html#references",
    "title": "Week 3: Parallelization Concepts",
    "section": "References",
    "text": "References\n\n\nLeskovec, Jure, Anand Rajaraman, and Jeffrey David Ullman. 2014. Mining of Massive Datasets. Cambridge University Press."
  },
  {
    "objectID": "w03/index.html",
    "href": "w03/index.html",
    "title": "Week 3: Parallelization Concepts",
    "section": "",
    "text": "Open slides in new tab ‚Üí",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#looking-back",
    "href": "w03/index.html#looking-back",
    "title": "Week 3: Parallelization Concepts",
    "section": "Looking back",
    "text": "Looking back\n\nContinued great use of Slack!\n\nNice interactions\n\nDue date reminders:\n\nAssignment 2: September 17, 2025\nLab 3: September 17, 2025\nAssignment 3: September 24, 2025",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#glossary",
    "href": "w03/index.html#glossary",
    "title": "Week 3: Parallelization Concepts",
    "section": "Glossary",
    "text": "Glossary\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nLocal\nYour current workstation (laptop, desktop, etc.), wherever you start the terminal/console application.\n\n\nRemote\nAny machine you connect to via ssh or other means.\n\n\nEC2\nSingle virtual machine in the cloud where you can run computation\n\n\nSageMaker\nIntegrated Developer Environment where you can conduct data science on single machines or distributed training\n\n\nGPU\nGraphics Processing Unit - specialized hardware for parallel computation, essential for AI/ML\n\n\nTPU\nTensor Processing Unit - Google‚Äôs custom AI accelerator chips\n\n\nEphemeral\nLasting for a short time - any machine that will get turned off or place you will lose data\n\n\nPersistent\nLasting for a long time - any environment where your work is NOT lost when the timer goes off",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#quick-survey-question-for-intuition-building",
    "href": "w03/index.html#quick-survey-question-for-intuition-building",
    "title": "Week 3: Parallelization Concepts",
    "section": "Quick Survey Question, for Intuition-Building",
    "text": "Quick Survey Question, for Intuition-Building\n\nAre humans capable of ‚Äútrue‚Äù multi-tasking?\n\nAs in, doing two things at the exact same time?\n\n(Or, do we instead rapidly switch back and forth between tasks?)",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#the-answer",
    "href": "w03/index.html#the-answer",
    "title": "Week 3: Parallelization Concepts",
    "section": "The Answer",
    "text": "The Answer\n\n(From what we understand, at the moment, by way of studies in neuroscience/cognitive science/etc‚Ä¶)\nHumans are not capable of true multitasking! In CS terms, this would be called multiprocessing (more on this later)\nWe are capable, however, of various modes of concurrency!\n\n\n\n\n\n\n\n\n\n\nMultithreading\nAsynchronous Execution\n\n\n\n\nUnconsciously(you do it already, ‚Äúnaturally‚Äù)\nFocus on one speaker within a loud room, with tons of other conversations entering your ears\nPut something in oven, set alarm, go do something else, take out of oven once alarm goes off\n\n\nConsciously(you can do it with effort/practice)\nPat head (up and down) and rub stomach (circular motion) ‚Äúsimultaneously‚Äù\nThrow a ball in the air, clap 3 times, catch ball",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#helpful-specifically-for-programming",
    "href": "w03/index.html#helpful-specifically-for-programming",
    "title": "Week 3: Parallelization Concepts",
    "section": "Helpful Specifically for Programming",
    "text": "Helpful Specifically for Programming\n\nCourse notes from MIT‚Äôs class on parallel computing phrases it like: if implemented thoughtfully, concurrency is a power multiplier for your code (do 10 things in 1 second instead of 10 seconds‚Ä¶)",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#helpful-in-general-as-a-way-of-thinking",
    "href": "w03/index.html#helpful-in-general-as-a-way-of-thinking",
    "title": "Week 3: Parallelization Concepts",
    "section": "Helpful In General as a Way of Thinking!",
    "text": "Helpful In General as a Way of Thinking!\n\nSay you get hired as a Project Manager‚Ä¶\nPart of your job will fundamentally involve pipelines!\n\nNeed to know when Task \\(B\\) does/does not require Task \\(A\\) as a prerequisite\nNeed to know whether Task \\(A\\) and Task \\(B\\) can share one resource or need their own individual resources\nOnce Task \\(A\\) and \\(B\\) both complete, how do we merge their results together?",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#avoiding-the-rabbithole",
    "href": "w03/index.html#avoiding-the-rabbithole",
    "title": "Week 3: Parallelization Concepts",
    "section": "Avoiding the Rabbithole",
    "text": "Avoiding the Rabbithole\n\nParallel computing is a rabbithole, but one you can safely avoid via simple heuristics (‚Äúrules of thumb‚Äù)!\n\n\nCheck for optimizations to serial code first,\nCheck for embarrassingly parallel code blocks\nUse map-reduce approach for more complicated cases",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#typical-real-world-scenarios",
    "href": "w03/index.html#typical-real-world-scenarios",
    "title": "Week 3: Parallelization Concepts",
    "section": "Typical Real-World Scenarios",
    "text": "Typical Real-World Scenarios\n\nYou need to prepare training data for LLMs by cleaning and deduplicating 100TB of web-scraped text\nYou are building a RAG system that requires embedding and indexing millions of documents in parallel\nYou need to extract structured data from millions of PDFs using vision models for document AI\nYou are preprocessing multimodal datasets with billions of image-text pairs for foundation model training\nYou need to run quality filtering on petabytes of Common Crawl data for training dataset\nYou are generating synthetic training data using LLMs to augment limited real-world datasets\nYou need to transform and tokenize text across 100+ languages for multilingual AI\nYou are building real-time data pipelines that process streaming data for online learning",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#embarrassingly-parallel-pipelines",
    "href": "w03/index.html#embarrassingly-parallel-pipelines",
    "title": "Week 3: Parallelization Concepts",
    "section": "‚ÄúEmbarrassingly Parallel‚Äù Pipelines",
    "text": "‚ÄúEmbarrassingly Parallel‚Äù Pipelines\n\nTechnical definition: tasks within pipeline can easily be parallelized bc no dependence and no need for communication (triple spatula!)",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#parallelizing-non-embarrassingly-parallel-pipelines",
    "href": "w03/index.html#parallelizing-non-embarrassingly-parallel-pipelines",
    "title": "Week 3: Parallelization Concepts",
    "section": "Parallelizing Non-Embarrassingly-Parallel Pipelines",
    "text": "Parallelizing Non-Embarrassingly-Parallel Pipelines",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#buzzkill-complications-to-come",
    "href": "w03/index.html#buzzkill-complications-to-come",
    "title": "Week 3: Parallelization Concepts",
    "section": "Buzzkill: Complications to Come üò∞",
    "text": "Buzzkill: Complications to Come üò∞\n\nIf it‚Äôs such a magical powerup, shouldn‚Äôt we just parallelize everything? Answer: No üòû bc overhead.\nOverhead source 1: Sending tasks to workers (processors), collecting results\nOverhead source 2: Even after setting up new stacks and heaps, threads may need to communicate with each other (e.g.¬†if they need to synchronize at some point(s))\nIn fact, probably the earliest super-popular parallelization library was created to handle Source 2, not Source 1: Message Passing Interface (C, C++, and Fortran)",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#rules-of-thumb-for-parallelization",
    "href": "w03/index.html#rules-of-thumb-for-parallelization",
    "title": "Week 3: Parallelization Concepts",
    "section": "Rules of Thumb for Parallelization",
    "text": "Rules of Thumb for Parallelization\n\n\nYes - Parallelize These\n\nData Preparation:\n\nText extraction from documents\nTokenization of text corpora\nImage preprocessing\nEmbedding generation for documents\nData quality filtering and validation\nFormat conversions (audio features)\nWeb scraping and data collection\nSynthetic data generation\n\nData Processing:\n\nBatch inference on datasets\nFeature extraction at scale\nData deduplication (local)\n\n\n\n\nNo - Keep Sequential\n\nOrder-Dependent:\n\nConversation threading\nTime-series preprocessing\nSequential data validation\nCumulative statistics\n\nGlobal Operations:\n\nGlobal deduplication\nCross-dataset joins\nComputing exact quantiles\n\n\n\n\n\n\n\n\nFor data operations in the ‚ÄúNo‚Äù column, they often require global coordination or maintain strict ordering. However, many can be approximated with parallel algorithms (like approximate deduplication with locality-sensitive hashing)",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#in-action",
    "href": "w03/index.html#in-action",
    "title": "Week 3: Parallelization Concepts",
    "section": "In Action",
    "text": "In Action\n\nimport time\nfrom sympy.ntheory import factorint\nfrom joblib import Parallel, delayed\nparallel_runner = Parallel(n_jobs=4)\nstart, end = 500, 580\ndef find_prime_factors(num):\n  time.sleep(.01)\n  return factorint(num, multiple=True)\ndisp_time = lambda start, end: print('{:.4f} s'.format(end - start))\n\n\n\n\nserial_start = time.time()\nresult = [\n  (i,find_prime_factors(i))\n  for i in range(start, end+1)\n]\nserial_end = time.time()\ndisp_time(serial_start, serial_end)\n\n0.9903 s\n\n\n\n\npar_start = time.time()\nresult = parallel_runner(\n  delayed(find_prime_factors)(i)\n  for i in range(start, end+1)\n)\npar_end = time.time()\ndisp_time(par_start, par_end)\n\n2.3708 s",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#what-happens-when-not-embarrassingly-parallel",
    "href": "w03/index.html#what-happens-when-not-embarrassingly-parallel",
    "title": "Week 3: Parallelization Concepts",
    "section": "What Happens When Not Embarrassingly Parallel?",
    "text": "What Happens When Not Embarrassingly Parallel?\n\nThink of the difference between linear and quadratic equations in algebra:\n\\(3x - 1 = 0\\) is ‚Äúembarrassingly‚Äù solvable, on its own: you can solve it directly, by adding 3 to both sides \\(\\implies x = \\frac{1}{3}\\). Same for \\(2x + 3 = 0 \\implies x = -\\frac{3}{2}\\)\nNow consider \\(6x^2 + 7x - 3 = 0\\): Harder to solve ‚Äúdirectly‚Äù, so your instinct might be to turn to the laborious quadratic equation:\n\n\\[\n\\begin{align*}\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} = \\frac{-7 \\pm \\sqrt{49 - 4(6)(-3)}}{2(6)} = \\frac{-7 \\pm 11}{12} = \\left\\{\\frac{1}{3},-\\frac{3}{2}\\right\\}\n\\end{align*}\n\\]\n\nAnd yet, \\(6x^2 + 7x - 3 = (3x - 1)(2x + 3)\\), meaning that we could have split the problem into two ‚Äúembarrassingly‚Äù solvable pieces, then multiplied to get result!",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#the-analogy-to-map-reduce",
    "href": "w03/index.html#the-analogy-to-map-reduce",
    "title": "Week 3: Parallelization Concepts",
    "section": "The Analogy to Map-Reduce",
    "text": "The Analogy to Map-Reduce\n\n\n\n\n\n\n\n\\(\\leadsto\\) If code is not embarrassingly parallel (instinctually requiring laborious serial execution),\n\\(\\underbrace{6x^2 + 7x - 3 = 0}_{\\text{Solve using Quadratic Eqn}}\\)\n\n\nBut can be split into‚Ä¶\n\\((3x - 1)(2x + 3) = 0\\)\n\n\nEmbarrassingly parallel pieces which combine to same result,\n\\(\\underbrace{3x - 1 = 0}_{\\text{Solve directly}}, \\underbrace{2x + 3 = 0}_{\\text{Solve directly}}\\)\n\n\nWe can use map-reduce to achieve ultra speedup (running ‚Äúpieces‚Äù on GPU!)\n\\(\\underbrace{(3x-1)(2x+3) = 0}_{\\text{Solutions satisfy this product}}\\)",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#the-direct-analogy-map-reduce",
    "href": "w03/index.html#the-direct-analogy-map-reduce",
    "title": "Week 3: Parallelization Concepts",
    "section": "The Direct Analogy: Map-Reduce!",
    "text": "The Direct Analogy: Map-Reduce!\n\nProblem from DSAN 5000/5100: Computing SSR (Sum of Squared Residuals)\n\\(y = (1,3,2), \\widehat{y} = (2, 5, 0) \\implies \\text{SSR} = (1-2)^2 + (3-5)^2 + (2-0)^2 = 9\\)\nComputing pieces separately:\nmap(do_something_with_piece, list_of_pieces)\n\nmy_map = map(lambda input: input**2, [(1-2), (3-5), (2-0)])\nmap_result = list(my_map)\nmap_result\n\n[1, 4, 4]\n\n\nCombining solved pieces\nreduce(how_to_combine_pair_of_pieces, pieces_to_combine)\n\nfrom functools import reduce\nmy_reduce = reduce(lambda piece1, piece2: piece1 + piece2, map_result)\nmy_reduce\n\n9",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#functions-vs.-functionals",
    "href": "w03/index.html#functions-vs.-functionals",
    "title": "Week 3: Parallelization Concepts",
    "section": "Functions vs.¬†Functionals",
    "text": "Functions vs.¬†Functionals\nYou may have noticed: map() and reduce() are ‚Äúmeta-functions‚Äù: functions that take other functions as inputs\n\n\n\ndef add_5(num):\n  return num + 5\nadd_5(10)\n\n15\n\n\n\n\ndef apply_twice(fn, arg):\n  return fn(fn(arg))\napply_twice(add_5, 10)\n\n20\n\n\n\n\nIn Python, functions can be used as vars (Hence lambda):\n\nadd_5 = lambda num: num + 5\napply_twice(add_5, 10)\n\n20\n\n\nThis relates to a whole paradigm, ‚Äúfunctional programming‚Äù: mostly outside scope of course, but lots of important+useful takeaways/rules-of-thumb!",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#train-your-brain-for-functional-approach-implies-master-debugging",
    "href": "w03/index.html#train-your-brain-for-functional-approach-implies-master-debugging",
    "title": "Week 3: Parallelization Concepts",
    "section": "Train Your Brain for Functional Approach \\(\\implies\\) Master Debugging!",
    "text": "Train Your Brain for Functional Approach \\(\\implies\\) Master Debugging!\n\nIn CS Theory: enables formal proofs of correctness\nIn CS practice:\n\nWhen a program doesn‚Äôt work, each function is an interface point where you can check that the data are correct. You can look at the intermediate inputs and outputs to quickly isolate the function that‚Äôs responsible for a bug.(from Python‚Äôs ‚ÄúFunctional Programming HowTo‚Äù)",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#code-rightarrow-pipelines-rightarrow-debuggable-pipelines",
    "href": "w03/index.html#code-rightarrow-pipelines-rightarrow-debuggable-pipelines",
    "title": "Week 3: Parallelization Concepts",
    "section": "Code \\(\\rightarrow\\) Pipelines \\(\\rightarrow\\) Debuggable Pipelines",
    "text": "Code \\(\\rightarrow\\) Pipelines \\(\\rightarrow\\) Debuggable Pipelines\n\n\nScenario: Run code, check the output, and‚Ä¶ it‚Äôs wrong üòµ what do you do?\nUsual approach: Read lines one-by-one, figuring out what they do, seeing if something pops out that seems wrong; adding comments like # Convert to lowercase\n\n\n\n\nEasy case: found typo in punctuation removal code. Fix the error, add comment like # Remove punctuation\n Rule 1 of FP: transform these comments into function names\n\n\n\nHard case: Something in load_text() modifies a variable that later on breaks remove_punct() (Called a side-effect)\n Rule 2 of FP: NO SIDE-EFFECTS!\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\ninput\nin.txt\n\n\n\nload_text\nload_text\n(Verb)\n\n\n\n\ninput-&gt;load_text\n\n\n\n\n\nlowercase\nlowercase\n(Verb)\n\n\n\n\nload_text-&gt;lowercase\n\n\nüßê ‚úÖ\n\n\n\nremove_punct\nremove_punct\n(Verb)\n\n\n\n\nlowercase-&gt;remove_punct\n\n\nüßê ‚úÖ\n\n\n\nremove_stopwords\nremove_stopwords\n(Verb)\n\n\n\n\nremove_punct-&gt;remove_stopwords\n\n\nüßê ‚ùå‚ùóÔ∏è\n\n\n\noutput\nout.txt\n\n\n\nremove_stopwords-&gt;output\n\n\n\n\n\n\n (Does this way of diagramming a program look familiar?) \n\n\n\n\nWith side effects: ‚ùå \\(\\implies\\) issue is somewhere earlier in the chain üò©üèÉ‚Äç‚ôÇÔ∏è\nNo side effects: ‚ùå \\(\\implies\\) issue must be in remove_punct()!!! üòé ‚è±Ô∏è = üí∞",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#if-its-so-useful-why-doesnt-everyone-do-it",
    "href": "w03/index.html#if-its-so-useful-why-doesnt-everyone-do-it",
    "title": "Week 3: Parallelization Concepts",
    "section": "If It‚Äôs So Useful, Why Doesn‚Äôt Everyone Do It?",
    "text": "If It‚Äôs So Useful, Why Doesn‚Äôt Everyone Do It?\n\nTrapped in imperative (sequential) coding mode: Path dependency / QWERTY\nWe need to start thinking like this bc, 1000x harder to debug parallel code! So we need to be less ad hoc in how we write+debug, from here on out! üôá‚Äç‚ôÇÔ∏èüôè\n\n\n\n\nFrom Leskovec, Rajaraman, and Ullman (2014)\n\n\n\nThe title relates to a classic Economics joke (the best kind of joke): ‚ÄúAn economist and a CEO are walking down the street, when the CEO points at the ground and tells the economist, ‚Äòlook! A $20 bill on the ground!‚Äô The economist keeps on walking, scoffing at the CEO: ‚Äòdon‚Äôt be silly, if there was a $20 bill on the ground, somebody would have picked it up already‚Äô.‚Äù",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#the-killer-application-matrix-multiplication",
    "href": "w03/index.html#the-killer-application-matrix-multiplication",
    "title": "Week 3: Parallelization Concepts",
    "section": "The ‚ÄúKiller Application‚Äù: Matrix Multiplication",
    "text": "The ‚ÄúKiller Application‚Äù: Matrix Multiplication\n\n(I learned from Jeff Ullman, who did the obnoxious Stanford thing of mentioning in passing how ‚Äútwo previous students in the class did this for a cool final project on web crawling and, well, it escalated quickly‚Äù, aka became Google)\n\n\n\n\nFrom Leskovec, Rajaraman, and Ullman (2014), which is (legally) free online!",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#the-killer-way-to-learn-text-counts",
    "href": "w03/index.html#the-killer-way-to-learn-text-counts",
    "title": "Week 3: Parallelization Concepts",
    "section": "The Killer Way-To-Learn: Text Counts!",
    "text": "The Killer Way-To-Learn: Text Counts!\n\n(2014): Text counts (2.2) \\(\\rightarrow\\) Matrix multiplication (2.3) \\(\\rightarrow \\cdots \\rightarrow\\) PageRank (5.1)\nThe goal: User searches ‚ÄúDenzel Curry‚Äù‚Ä¶ How relevant is a given webpage?\nScenario 1: Entire internet fits on CPU \\(\\implies\\) We can just make a big dict:\n\n\n\n\n\n\n\n\nG\n\n\n\ninternet\n\nScan in O(n):\nToday Denzel Washington\nate a big bowl of Yum's\ncurry. Denzel allegedly\nrubbed his tum and said\n\"yum yum yum\" when he\ntasted today's curry.\n\"Yum! It is me Denzel,\ncurry is my fav!\", he\nexclaimed. According to\nhis friend Steph, curry\nis indeed Denzel's fav.\nWe are live with Del\nCurry in Washington for\na Denzel curry update.\n\n\n\n\nccounts\n\nOverall Counts\n\n('according',1)\n('allegedly',1)\n('ate',1)\n('big',1)\n('bowl',1)\n('curry',6)\n('del',1)\n('denzel',5)\n('exclaimed',1)\n('fav',2)\n('friend',1)\n('indeed',1)\n('live',1)\n('rubbed',1)\n('said',1)\n('steph',1)\n('tasted',1)\n('today',2)\n('tum',1)\n('update',1)\n('washington',2)\n('yum',4)\n\n\n\n\ninternet-&gt;ccounts\n\n\nLoop Over Words",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#if-everything-doesnt-fit-on-cpu",
    "href": "w03/index.html#if-everything-doesnt-fit-on-cpu",
    "title": "Week 3: Parallelization Concepts",
    "section": "If Everything Doesn‚Äôt Fit on CPU‚Ä¶",
    "text": "If Everything Doesn‚Äôt Fit on CPU‚Ä¶\n\n\n\nFrom Cornell Virtual Workshop, ‚ÄúUnderstanding GPU Architecture‚Äù",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#break-problem-into-chunks-for-the-green-bois",
    "href": "w03/index.html#break-problem-into-chunks-for-the-green-bois",
    "title": "Week 3: Parallelization Concepts",
    "section": "Break Problem into Chunks for the Green Bois!",
    "text": "Break Problem into Chunks for the Green Bois!\n\n\n\n\n\n\n\nG\n\n\n\nchunked\n\nChunked Document\n\nToday Denzel Washington\nate a big bowl of Yum's\ncurry. Denzel allegedly\nrubbed his tum and said\n\n\"yum yum yum\" when he\ntasted today's curry.\n\"Yum! It is me Denzel,\ncurry is my fav!\", he\n\nexclaimed. According to\nhis friend Steph, curry\nis indeed Denzel's fav.\nWe are live with Del\nCurry in Washington for\na Denzel curry update.\n\n\n\n\nchcounts\n\nChunked Counts\n\n('today',1)\n('denzel',1)\n...\n('tum',1)\n('said',1)\n\n('yum',1)\n('yum',1)\n('yum',1)\n...\n('fav',1)\n\n('exclaimed',1)\n...\n('del',1)\n('curry',1)\n('washington',1)\n('denzel',1)\n('curry',1)\n('update',1)\n\n\n\n\nchunked:p1-&gt;chcounts:p1\n\n\nO(n/4)\n\n\n\nchunked:p2-&gt;chcounts:p2\n\n\nO(n/4)\n\n\n\nchunked:p3-&gt;chcounts:p3\n\n\nO(n/4)\n\n\n\nchunked:p4-&gt;chcounts:p4\n\n\nO(n/4)\n\n\n\nscounts\n\nHashed Counts\n\n('allegedly',1)\n...\n('curry',1)\n('denzel',2)\n...\n('yum',1)\n\n('curry',2)\n('denzel',1)\n...\n('yum',4)\n\n('according',1)\n('curry',1)\n('del',1)\n('denzel',1)\n...\n('curry',2)\n('denzel',1)\n('update',1)\n('washington',1)\n\n\n\n\nchcounts:p1-&gt;scounts:p1\n\n\nO(n/4)\n\n\n\nchcounts:p2-&gt;scounts:p2\n\n\nO(n/4)\n\n\n\nchcounts:p3-&gt;scounts:p3\n\n\nO(n/4)\n\n\n\nchcounts:p4-&gt;scounts:p4\n\n\nO(n/4)\n\n\n\nccounts\n\nOverall Counts\n('according',1)\n('allegedly',1)\n('ate',1)\n('big',1)\n('bowl',1)\n('curry',6)\n('del',1)\n('denzel',5)\n('exclaimed',1)\n('fav',2)\n('friend',1)\n('indeed',1)\n('live',1)\n('rubbed',1)\n('said',1)\n('steph',1)\n('tasted',1)\n('today',2)\n('tum',1)\n('update',1)\n('washington',2)\n('yum',4)\n\n\n\n\nscounts:p1-&gt;ccounts:p1\n\n\n\n\n\nscounts:p2-&gt;ccounts:p1\n\n\n\n\n\nscounts:p3-&gt;ccounts:p1\n\n\n\n\n\nscounts:p4-&gt;ccounts:p1\n\n\n\n\n\nscounts:p2-&gt;ccounts\n\n\n \n\nMerge in\nO(n)\n\n\n\n\n\n\n\n\n\n\\(\\implies\\) Total = \\(O(3n) = O(n)\\)\nBut also optimized in terms of constants, because of sequential memory reads",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#references",
    "href": "w03/index.html#references",
    "title": "Week 3: Parallelization Concepts",
    "section": "References",
    "text": "References\n\n\nLeskovec, Jure, Anand Rajaraman, and Jeffrey David Ullman. 2014. Mining of Massive Datasets. Cambridge University Press.",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w04/index.html",
    "href": "w04/index.html",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "",
    "text": "Open slides in new tab ‚Üí",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#agenda-and-goals-for-today",
    "href": "w04/index.html#agenda-and-goals-for-today",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Agenda and Goals for Today",
    "text": "Agenda and Goals for Today\n\n\n\nLecture\n\nDistributed file systems\nModern file types\nWorking with large tabular data on a single node\n\nDuckDB\nPolars\n\n\n\n\n\nLab\n\nRun a similar task with Pandas, polars and duckdb",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#logistics-and-review",
    "href": "w04/index.html#logistics-and-review",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Logistics and Review",
    "text": "Logistics and Review\n\n\nDeadlines\n\nAssignment 1: Python Skills Due Sept 5 11:59pm\nLab 2: Cloud Tooling Due Sept 5 6pm\nAssignment 2: Shell & Linux Due Sept 11 11:59pm\nLab 3: Parallel Computing Due Sept 12 6pm\nAssignment 3: Parallelization Due Sept 18 11:59pm\nLab 4: Docker and Lambda Due Sept 19 6pm\nAssignment 4: Containers Due Sept 25 11:59pm\nLab 5: DuckDB & Polars Due Sept 26 6pm\n\n\nLook back and ahead\n\nContinue to use Slack for questions!\nDocker (containerization)\nLambda functions\nComing up: Spark and project",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#raw-ingredients-of-storage-systems",
    "href": "w04/index.html#raw-ingredients-of-storage-systems",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Raw Ingredients of Storage Systems",
    "text": "Raw Ingredients of Storage Systems\n\n\n\nDisk drives (magnetic HDDs or SSDs)\nRAM\nNetworking and CPU\nSerialization\nCompression\nCaching\n\n\n\n\n\nFrom Reis and Housley (2022)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#single-machine-vs.-distributed-storage",
    "href": "w04/index.html#single-machine-vs.-distributed-storage",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Single-Machine vs.¬†Distributed Storage",
    "text": "Single-Machine vs.¬†Distributed Storage\n\n\n\nFrom Reis and Housley (2022)\n\n\n\n\n\nSingle-Machine\n\n\nThey are commonly used for storing operating system files, application files, and user data files.\nFilesystems are also used in databases to store data files, transaction logs, and backups.\n\n\n\nDistributed Storage\n\n\nA distributed filesystem is a type of filesystem that spans multiple computers.\nIt provides a unified view of files across all the computers in the system.\nHave existed before cloud",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#file-storage-types",
    "href": "w04/index.html#file-storage-types",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "File Storage Types",
    "text": "File Storage Types\n\n\n\nLocal Disk\n\n\nOS-managed filesystems on local disk partition:\nNTFS (Windows)\nHFS+ (MacOS)\next4 (Linux)() on a local disk partition of SSD or magnetic disk\n\n\n\nNetwork-Attached (NAS)\n\n\nAccessed by clients over a network\nRedundancy and reliability, fine-grained control of resources, storage pooling across multiple disks for large virtual volumes, and file sharing across multiple machines\n\n\nCloud Filesystems\n\nNot object store (more on that later)\nNot the virtual hard drive attached to a virtual machine\nFully managed: Takes care of networking, managing disk clusters, failures, and configuration (Azure Files, Amazon Elastic Filesystem)\nBacked by Object Store\n\n\n\nBased on Reis and Housley (2022)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#object-stores",
    "href": "w04/index.html#object-stores",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Object Stores",
    "text": "Object Stores\n\nSomewhat confusing because object has several meanings in computer science.\nIn this context, we‚Äôre talking about a specialized file-like construct. It could be any type of file: TXT, CSV, JSON, images, videos, audio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContains objects of all shapes and sizes: each gets a unique identifier\nObjects are immutable: cannot be modified in place (unlike local FS)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#distributed-fs-vs-object-store",
    "href": "w04/index.html#distributed-fs-vs-object-store",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Distributed FS vs Object Store",
    "text": "Distributed FS vs Object Store\n\n\n\n\n\n\n\n\n\nDistributed File System\nObject Storage\n\n\n\n\nOrganization\nFiles in hierarchical directories\nFlat organization (though there can be overlays to provide hierarchical files structure)\n\n\nMethod\nPOSIX File Operations\nREST API\n\n\nImmutability\nNone: Random writes anywhere in file\nImmutable: need to replace/append entire object\n\n\nPerformance\nPerforms best for smaller files\nPerforms best for large files\n\n\nScalability\nMillions of files\nBillions of objects\n\n\n\nBoth provide:\n\nFault tolerance\nAvailability and consistency",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#before-data-locality-for-hadoop",
    "href": "w04/index.html#before-data-locality-for-hadoop",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Before: Data locality (for Hadoop)",
    "text": "Before: Data locality (for Hadoop)\n\n\n\nFrom White (2015)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#today-de-coupling-storage-from-compute",
    "href": "w04/index.html#today-de-coupling-storage-from-compute",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Today: De-Coupling Storage from Compute",
    "text": "Today: De-Coupling Storage from Compute\n\n\n\nFrom Gopalan (2022)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#data-on-disk-formats",
    "href": "w04/index.html#data-on-disk-formats",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Data-on-Disk Formats",
    "text": "Data-on-Disk Formats\n\nPlain Text (CSV, TSV, FWF)\nJSON\nBinary Files",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#plain-text-csv-tsv-fwf",
    "href": "w04/index.html#plain-text-csv-tsv-fwf",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Plain Text (CSV, TSV, FWF)",
    "text": "Plain Text (CSV, TSV, FWF)\n\n\n\n\n\n\nPay attention to encodings!\nLines end in linefeed, carriage-return, or both together depending on the OS that generated\nTypically, a single line of text contains a single record",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#json",
    "href": "w04/index.html#json",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "JSON",
    "text": "JSON\n\n\n\n\n\n\nWarning\n\n\n\nJSON files have two flavors: JSON Lines vs.¬†JSON. Typically when we say data is in JSON format, we imply it‚Äôs JSON Lines which means that there is a single JSON object per line, and there are multiple lines.\n\n\n\n\nJSON Lines\n4 records, one per line, no end comma\n{\"id\":1, \"name\":\"marck\", \"last_name\":\"vaisman\"}\n{\"id\":2, \"name\":\"anderson\", \"last_name\":\"monken\"}\n{\"id\":3, \"name\":\"amit\", \"last_name\":\"arora\"}\n{\"id\":4, \"name\":\"abhijit\", \"last_name\":\"dasgupta\"}\n\nJSON\n4 records enclosed in 1 JSON Array\n[\n  {\"id\":1, \"name\":\"marck\", \"last_name\":\"vaisman\"},\n  {\"id\":2, \"name\":\"anderson\", \"last_name\":\"monken\"},\n  {\"id\":3, \"name\":\"amit\", \"last_name\":\"arora\"},\n  {\"id\":4, \"name\":\"abhijit\", \"last_name\":\"dasgupta\"},\n]",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#binary-files",
    "href": "w04/index.html#binary-files",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Binary Files",
    "text": "Binary Files",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#issues-with-common-file-formats-especially-csv",
    "href": "w04/index.html#issues-with-common-file-formats-especially-csv",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Issues with Common File Formats (Especially CSV)",
    "text": "Issues with Common File Formats (Especially CSV)\n\nUbiquitous but highly error-prone\nDefault delimiter: familiar character in English, the comma\nAmbiguities:\n\nDelimiter (comma, tab, semi-colon, custom)\nQuote characters (single or doble quote)\nEscaping to appropriately handle string data\n\nDoesn‚Äôt natively encode schema information\nNo direct support for nested structures\nEncoding+schema must be configured on target system to ensure ingestion\nAutodetection provided in many cloud environments but is inappropriate for production ingestion (can be painfully slow)\nData engineers often forced to work with CSV data and then build robust exception handling and error detection to ensure data quality (Pydantic!)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#introducing-apache-parquet",
    "href": "w04/index.html#introducing-apache-parquet",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Introducing Apache Parquet",
    "text": "Introducing Apache Parquet\n\nFree, open-source, column-oriented data storage format created by Twitter and Cloudera (v1.0 released July 2013)\nData stored in columnar format (as opposed to row format), designed for read and write performance\nBuilds in schema information and natively supports nested data\nSupported by R and Python through Apache Arrow (more coming up!)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#traditional-row-store",
    "href": "w04/index.html#traditional-row-store",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Traditional Row-Store",
    "text": "Traditional Row-Store\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuery: ‚ÄúHow many balls did we sell?\nThe engine must scan each and every row until the end!",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#column-store",
    "href": "w04/index.html#column-store",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Column-Store",
    "text": "Column-Store\n\n\n\n\n\n\n\nParquet file format, everything you need to know",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#row-groups",
    "href": "w04/index.html#row-groups",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Row Groups",
    "text": "Row Groups\n\n\n\nData is stored in row groups!\n\n\n\nOnly required fields",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#metadata-compression-and-dictionary-encoding",
    "href": "w04/index.html#metadata-compression-and-dictionary-encoding",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Metadata, compression, and dictionary encoding",
    "text": "Metadata, compression, and dictionary encoding",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#apache-arrow-for-in-memory-analytics",
    "href": "w04/index.html#apache-arrow-for-in-memory-analytics",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Apache Arrow for In-Memory Analytics",
    "text": "Apache Arrow for In-Memory Analytics\n\nApache Arrow is a development platform for in-memory analytics. It contains a set of technologies that enable big data systems to process and move data fast. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. (Topol and McKinney 2024)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#before-arrow",
    "href": "w04/index.html#before-arrow",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Before Arrow",
    "text": "Before Arrow\n\n\n\nTopol and McKinney (2024)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#after-arrow",
    "href": "w04/index.html#after-arrow",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "After Arrow",
    "text": "After Arrow\n\n\n\nTopol and McKinney (2024)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#arrow-compatibility",
    "href": "w04/index.html#arrow-compatibility",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Arrow Compatibility",
    "text": "Arrow Compatibility\n\n\n\nTopol and McKinney (2024)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#arrow-performance",
    "href": "w04/index.html#arrow-performance",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Arrow Performance",
    "text": "Arrow Performance\n\n\n\n\n\nTopol and McKinney (2024)\n\n\n\n\n\n\nTopol and McKinney (2024)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#using-arrow-with-csv-and-parquet",
    "href": "w04/index.html#using-arrow-with-csv-and-parquet",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Using Arrow with CSV and Parquet",
    "text": "Using Arrow with CSV and Parquet\n\n\n\nPython\n\npyarrow or from pandas\nimport pandas as pd\npd.read_csv(engine = 'pyarrow')\npd.read_parquet()\n\nimport pyarrow.csv\npyarrow.csv.read_csv()\n\nimport pyarrow.parquet\npyarrow.parquet.read_table()\n\n\nR\n\nUse the arrow package\nlibrary(arrow)\n\nread_csv_arrow()\nread_parquet()\nread_json_arrow()\n\nwrite_csv_arrow()\nwrite_parquet()\n\n\nRecommendation: save your intermediate and analytical datasets as Parquet!",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#before-we-begin",
    "href": "w04/index.html#before-we-begin",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Before We Begin‚Ä¶",
    "text": "Before We Begin‚Ä¶\n\nPandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\n\nPandas is slow, but less slow if you use it the right way!\n\nApache Arrow and the ‚Äú10 Things I Hate About pandas‚Äù (A 2017 post from the creator of Pandas‚Ä¶))\n50x faster data loading in Pandas: no problem (an old 2019 article‚Ä¶)\nIs Pandas really that slow?\nPandas 2.0 and the arrow revolution",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#polars-1",
    "href": "w04/index.html#polars-1",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Polars",
    "text": "Polars",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#why-is-polars-faster-than-pandas",
    "href": "w04/index.html#why-is-polars-faster-than-pandas",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Why is Polars Faster than Pandas?",
    "text": "Why is Polars Faster than Pandas?\n\nPolars is written in Rust. Rust is compiled; Python is interpreted\n\nCompiled language: you generate the machine code only once then run it, subsequent runs do not need the compilation step.\nInterpreted language: code has to be parsed, interpreted and converted into machine code every single time.\n\nParallelization: Vectorized operations can be executed in parallel on multiple cores\nLazy evaluation: Polars supports two APIs lazy as well as eager evaluation (used by pandas). In lazy evaluation, a query is executed only when required. While in eager evaluation, a query is executed immediately.\nPolars uses Arrow for in-memory data representation. Similar to how pandas uses NumPy (Pandas 2 allows using Arrow as backend)\nPolars \\(\\approx\\) in-memory DataFrame library + query optimizer\n\n\n\n[Excerpt from this post from Ritchie Vink, author of Polars] Arrow provides the efficient data structures and some compute kernels, like a SUM, a FILTER, a MAX etc. Arrow is not a query engine. Polars is a DataFrame library on top of arrow that has implemented efficient algorithms for JOINS, GROUPBY, PIVOTs, MELTs, QUERY OPTIMIZATION, etc. (the things you expect from a DF lib).",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#ease-of-use",
    "href": "w04/index.html#ease-of-use",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Ease of Use",
    "text": "Ease of Use\n\nFamiliar API for users of Pandas: differences in syntax but still a Dataframe API making it straightforward to perform common operations such as filtering, aggregating, and joining data\nSee Migrating from Pandas\nReading data\n# must install s3fs -&gt; \"pip install s3fs\"\n# Using Polars\nimport polars as pl\npolars_df = pl.read_parquet(\"s3://nyc-tlc/trip data/yellow_tripdata_2023-06.parquet\")\n\n# using Pandas\nimport pandas as pd\npandas_df = pd.read_parquet(\"s3://nyc-tlc/trip data/yellow_tripdata_2023-06.parquet\")\nSelecting columns (see Pushdown optimization)\n# Using Polars\nselected_columns_polars = polars_df[['column1', 'column2']]\n\n# Using Pandas\nselected_columns_pandas = pandas_df[['column1', 'column2']]",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#ease-of-use-contd.",
    "href": "w04/index.html#ease-of-use-contd.",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Ease of Use (contd.)",
    "text": "Ease of Use (contd.)\n\nFiltering data\n# Using Polars\nfiltered_polars = polars_df[polars_df['column1'] &gt; 10]\n\n# Using Pandas\nfiltered_pandas = pandas_df[pandas_df['column1'] &gt; 10]\nThough you can write Polars code that looks like Pandas, better to write idiomatic Polars code that takes advantage of Polars‚Äô features\nMigrating from Apache Spark: Whereas Spark DataFrame is a collection of rows, Polars DataFrame is closer to a collection of columns",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#installation-data-loading-and-basic-operations",
    "href": "w04/index.html#installation-data-loading-and-basic-operations",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Installation, Data Loading, and Basic Operations",
    "text": "Installation, Data Loading, and Basic Operations\nInstall via pip:\npip install polars\nImport polars in your Python code and read data as usual:\nimport polars as pl\ndf = pl.read_parquet(\"s3://nyc-tlc/trip data/yellow_tripdata_2023-06.parquet\")\ndf.head()\n\nshape: (5, 19)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ VendorID ‚îÜ tpep_picku ‚îÜ tpep_dropoff ‚îÜ passenger_co ‚îÜ ... ‚îÜ improvement_ ‚îÜ total_amount ‚îÜ congestion_s ‚îÜ Airport_fee ‚îÇ\n‚îÇ ---      ‚îÜ p_datetime ‚îÜ _datetime    ‚îÜ unt          ‚îÜ     ‚îÜ surcharge    ‚îÜ ---          ‚îÜ urcharge     ‚îÜ ---         ‚îÇ\n‚îÇ i32      ‚îÜ ---        ‚îÜ ---          ‚îÜ ---          ‚îÜ     ‚îÜ ---          ‚îÜ f64          ‚îÜ ---          ‚îÜ f64         ‚îÇ\n‚îÇ          ‚îÜ datetime[n ‚îÜ datetime[ns] ‚îÜ i64          ‚îÜ     ‚îÜ f64          ‚îÜ              ‚îÜ f64          ‚îÜ             ‚îÇ\n‚îÇ          ‚îÜ s]         ‚îÜ              ‚îÜ              ‚îÜ     ‚îÜ              ‚îÜ              ‚îÜ              ‚îÜ             ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ 1        ‚îÜ 2023-06-01 ‚îÜ 2023-06-01   ‚îÜ 1            ‚îÜ ... ‚îÜ 1.0          ‚îÜ 33.6         ‚îÜ 2.5          ‚îÜ 0.0         ‚îÇ\n‚îÇ          ‚îÜ 00:08:48   ‚îÜ 00:29:41     ‚îÜ              ‚îÜ     ‚îÜ              ‚îÜ              ‚îÜ              ‚îÜ             ‚îÇ\n‚îÇ 1        ‚îÜ 2023-06-01 ‚îÜ 2023-06-01   ‚îÜ 0            ‚îÜ ... ‚îÜ 1.0          ‚îÜ 23.6         ‚îÜ 2.5          ‚îÜ 0.0         ‚îÇ\n‚îÇ          ‚îÜ 00:15:04   ‚îÜ 00:25:18     ‚îÜ              ‚îÜ     ‚îÜ              ‚îÜ              ‚îÜ              ‚îÜ             ‚îÇ\n‚îÇ 1        ‚îÜ 2023-06-01 ‚îÜ 2023-06-01   ‚îÜ 1            ‚îÜ ... ‚îÜ 1.0          ‚îÜ 60.05        ‚îÜ 0.0          ‚îÜ 1.75        ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#polars-pipeline-example",
    "href": "w04/index.html#polars-pipeline-example",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Polars Pipeline Example",
    "text": "Polars Pipeline Example\nWe‚Äôll run this as part of the lab in a little bit; think how you might code this in Pandas‚Ä¶\n\n\n\nPolars pipeline",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#further-reading",
    "href": "w04/index.html#further-reading",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Further reading",
    "text": "Further reading\n\nPolars\nUser guide ‚Üê MUST READ\nPolars GitHub repo\nPandas Vs Polars: a syntax and speed comparison\nTips & tricks for working with strings in Polars",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#duckdb-1",
    "href": "w04/index.html#duckdb-1",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "DuckDB",
    "text": "DuckDB\n\nDuckDB is an in-process SQL OLAP DB management system\nLike sqlite, but for analytics. What does this mean? It means that your database runs inside your process, there are no servers to manage, no remote system to connect to. Easy to experiment with SQL-like syntax.\nVectorized processing: Loads chunks of data into memory (tries to keep everything in the CPU‚Äôs L1 and L2 cache) and is thus able to handle datasets bigger than the amount of RAM available.\nSupports Python, R and a host of other languages\nImportant paper on DuckDB: Raasveldt and M√ºhleisen (2019)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#key-features",
    "href": "w04/index.html#key-features",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Key Features",
    "text": "Key Features\n\nColumnar Storage, Vectorized Query Processing: DuckDB contains a columnar-vectorized query execution engine, where queries are run on a large batch of values (a ‚Äúvector‚Äù) in one operation\n\nMost analytical queries (think group by and summarize) or even data retrieval for training ML models require retrieving a subset of columns and now the entire row, columnar storage make this faster\n\nIn-Memory Processing: All data needed for processing is brought within the process memory (recall that columnar storage format helps with this) making queries run faster (no DB call over the network)\nSQL Support: highly Postgres-compatible version of SQL1.\nACID Compliance: Transactional guarantees (ACID properties) through bulk-optimized Multi-Version Concurrency Control (MVCC).",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#use-cases-for-duckdb",
    "href": "w04/index.html#use-cases-for-duckdb",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Use Cases for DuckDB",
    "text": "Use Cases for DuckDB\n\nData Warehousing\nBusiness Intelligence\nReal-Time Analytics\nIoT Data Processing\n\nDuckDB in the Wild\n\nHow We Silently Switched Mode‚Äôs In-Memory Data Engine to DuckDB To Boost Visual Data Exploration Speed\nWhy we built Rill with DuckDB\nLeveraging DuckDB for enhanced performance in dbt projects",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#duckdb-diy",
    "href": "w04/index.html#duckdb-diy",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "DuckDB: DIY",
    "text": "DuckDB: DIY\n\n\n\nDatalake and DuckDB\n\n\nUsing DuckDB in AWS Lambda",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#duckdb-fully-managed",
    "href": "w04/index.html#duckdb-fully-managed",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "DuckDB: Fully-Managed",
    "text": "DuckDB: Fully-Managed\n\n\n\nMotherDuck Architecture\n\n\n\nArchitecture and capabilities\nSeamlessly analyze data, whether it sits on your laptop, in the cloud or split between.\nHybrid execution automatically plans each part of your query and determines where it‚Äôs best computed\nDuckDB Vs MotherDuck",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#setting-up-duckdb",
    "href": "w04/index.html#setting-up-duckdb",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Setting Up DuckDB",
    "text": "Setting Up DuckDB\nConfiguration and Initialization: DuckDB is integrated into Python and R for efficient interactive data analysis (APIs for Java, C, C++, Julia, Swift, and others)\npip install duckdb\nConnecting to DuckDB\nimport duckdb\n# directly query a Pandas DataFrame\nimport pandas as pd\ndata_url = \"https://raw.githubusercontent.com/anly503/datasets/main/EconomistData.csv\"\ndf = pd.read_csv(data_url)\nduckdb.sql('SELECT * FROM df')",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#setting-up-duckdb-contd.",
    "href": "w04/index.html#setting-up-duckdb-contd.",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Setting Up DuckDB (contd.)",
    "text": "Setting Up DuckDB (contd.)\nSupported data formats: DuckDB can ingest data from a wide variety of formats ‚Äì both on-disk and in-memory. See the data ingestion page for more information.\nimport duckdb\nduckdb.read_csv('example.csv')                # read a CSV file into a Relation\nduckdb.read_parquet('example.parquet')        # read a Parquet file into a Relation\nduckdb.read_json('example.json')              # read a JSON file into a Relation\n\nduckdb.sql('SELECT * FROM \"example.csv\"')     # directly query a CSV file\nduckdb.sql('SELECT * FROM \"example.parquet\"') # directly query a Parquet file\nduckdb.sql('SELECT * FROM \"example.json\"')    # directly query a JSON file",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#querying-duckdb",
    "href": "w04/index.html#querying-duckdb",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Querying DuckDB",
    "text": "Querying DuckDB\nEssential Reading\n\nFriendlier SQL with DuckDB\nSQL Introduction\n\nBasic SQL Queries\nimport duckdb\nimport pandas as pd\nbabynames = pd.read_parquet(\"https://github.com/anly503/datasets/raw/main/babynames.parquet.zstd\")\nduckdb.sql(\"select count(*)  from babynames where Name='John'\")\nAggregations and Grouping\nduckdb.sql(\"select State, Name, count(*) as count  from babynames group by State, Name order by State desc, count desc\")",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#querying-duckdb-contd.",
    "href": "w04/index.html#querying-duckdb-contd.",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Querying DuckDB (contd.)",
    "text": "Querying DuckDB (contd.)\nEssential reading: FROM and JOIN clauses\nJoins and Subqueries\n# Join two tables together\nduckdb.sql(\"SELECT * FROM table_name JOIN other_table ON (table_name.key = other_table.key\")\nWindow Functions\npowerplants = pd.read_csv(\"https://raw.githubusercontent.com/anly503/datasets/main/powerplants.csv\", parse_dates=[\"date\"])\nq = \"\"\"\nSELECT \"plant\", \"date\",\n    AVG(\"MWh\") OVER (\n        PARTITION BY \"plant\"\n        ORDER BY \"date\" ASC\n        RANGE BETWEEN INTERVAL 3 DAYS PRECEDING\n                  AND INTERVAL 3 DAYS FOLLOWING)\n        AS \"MWh 7-day Moving Average\"\nFROM powerplants\nORDER BY 1, 2;\n\"\"\"\nduckdb.sql(q)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#using-the-duckdb-cli-and-shell",
    "href": "w04/index.html#using-the-duckdb-cli-and-shell",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Using the DuckDB CLI and Shell",
    "text": "Using the DuckDB CLI and Shell\n\nInstall DuckDB CLI or use in browser via shell.duckdb.org for data exploration via SQL; Once installed, import a local file into the shell and run queries\nYou can download powerplants.csv here\nC:\\Users\\&lt;username&gt;\\Downloads\\duckdb_cli-windows-amd64&gt; duckdb\nv0.8.1 6536a77232\nEnter \".help\" for usage hints.\nConnected to a transient in-memory database.\nUse \".open FILENAME\" to reopen on a persistent database.\nD CREATE TABLE powerplants AS SELECT * FROM read_csv_auto('powerplants.csv');\nD DESCRIBE powerplants;\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ column_name ‚îÇ column_type ‚îÇ  null   ‚îÇ   key   ‚îÇ default ‚îÇ extra ‚îÇ\n‚îÇ   varchar   ‚îÇ   varchar   ‚îÇ varchar ‚îÇ varchar ‚îÇ varchar ‚îÇ int32 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ plant       ‚îÇ VARCHAR     ‚îÇ YES     ‚îÇ         ‚îÇ         ‚îÇ       ‚îÇ\n‚îÇ date        ‚îÇ DATE        ‚îÇ YES     ‚îÇ         ‚îÇ         ‚îÇ       ‚îÇ\n‚îÇ MWh         ‚îÇ BIGINT      ‚îÇ YES     ‚îÇ         ‚îÇ         ‚îÇ       ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\nD  SELECT * from powerplants where plant='Boston' and date='2019-01-02';\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  plant  ‚îÇ    date    ‚îÇ  MWh   ‚îÇ\n‚îÇ varchar ‚îÇ    date    ‚îÇ int64  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ Boston  ‚îÇ 2019-01-02 ‚îÇ 564337 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#profiling-in-duckdb",
    "href": "w04/index.html#profiling-in-duckdb",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Profiling in DuckDB",
    "text": "Profiling in DuckDB\nQuery Optimization: Use EXPLAIN and ANALYZE keywords to understand how your query is being executed and the time being spent in individual steps\nD EXPLAIN ANALYZE SELECT * from powerplants where plant='Boston' and date='2019-01-02';\nDuckDB will use all the cores available on the underlying compute, but you can adjust this (Full configuration details here)\nD select current_setting('threads');\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ current_setting('threads') ‚îÇ\n‚îÇ           int64            ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ                          8 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\nD SET threads=4;\nD select current_setting('threads');\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ current_setting('threads') ‚îÇ\n‚îÇ           int64            ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ                          4 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#benchmarks-and-comparisons",
    "href": "w04/index.html#benchmarks-and-comparisons",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Benchmarks and Comparisons",
    "text": "Benchmarks and Comparisons\n\n\n\n\nTricky topic! Can make your chosen solution look better by focusing on metrics on which it provides better results\nIn general, TPC-H and TPC-DS are considered the standard benchmarks for data processing.\n\n\n\n\n\n\nTPC-DS Homepage",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#further-reading-on-duckdb",
    "href": "w04/index.html#further-reading-on-duckdb",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Further Reading on DuckDB",
    "text": "Further Reading on DuckDB\n\nParallel Grouped Aggregation in DuckDB\nMeta queries\nProfiling queries in DuckDB\nDuckDB tutorial for beginners\nDuckDB CLI API\nUsing DuckDB in AWS Lambda\nRevisiting the Poor Man‚Äôs Data Lake with MotherDuck\nSupercharge your data processing with DuckDB\nFriendlier SQL with DuckDB\nBuilding and deploying data apps with DuckDB and Streamlit",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#references",
    "href": "w04/index.html#references",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "References",
    "text": "References\n\n\nGopalan, Rukmani. 2022. The Cloud Data Lake: A Guide to Building Robust Cloud Data Architecture. O‚ÄôReilly Media, Inc. https://www.dropbox.com/scl/fi/7u469ccc8y169us5hydk0/The-cloud-data-lake-a-guide-to-building-robust-cloud-data-Rukmani-Gopalan.pdf?rlkey=ey06a6zt9g90d4zq8tfncndta&dl=1.\n\n\nRaasveldt, Mark, and Hannes M√ºhleisen. 2019. ‚ÄúDuckDB: An Embeddable Analytical Database.‚Äù In Proceedings of the 2019 International Conference on Management of Data, 1981‚Äì84. SIGMOD ‚Äô19. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3299869.3320212.\n\n\nReis, Joe, and Matt Housley. 2022. Fundamentals of Data Engineering: Plan and Build Robust Data Systems. O‚ÄôReilly Media, Inc.\n\n\nTopol, Matthew, and Wes McKinney. 2024. In-Memory Analytics with Apache Arrow. Packt Publishing Ltd.\n\n\nWhite, Tom E. 2015. Hadoop: The Definitive Guide. O‚ÄôReilly Media, Inc.",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#footnotes",
    "href": "w04/index.html#footnotes",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFriendlier SQL with DuckDB‚Ü©Ô∏é",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/slides.html#agenda-and-goals-for-today",
    "href": "w04/slides.html#agenda-and-goals-for-today",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Agenda and Goals for Today",
    "text": "Agenda and Goals for Today\n\n\nLecture\n\nDistributed file systems\nModern file types\nWorking with large tabular data on a single node\n\nDuckDB\nPolars\n\n\n\nLab\n\nRun a similar task with Pandas, polars and duckdb"
  },
  {
    "objectID": "w04/slides.html#logistics-and-review",
    "href": "w04/slides.html#logistics-and-review",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Logistics and Review",
    "text": "Logistics and Review\n\n\nDeadlines\n\nAssignment 1: Python Skills Due Sept 5 11:59pm\nLab 2: Cloud Tooling Due Sept 5 6pm\nAssignment 2: Shell & Linux Due Sept 11 11:59pm\nLab 3: Parallel Computing Due Sept 12 6pm\nAssignment 3: Parallelization Due Sept 18 11:59pm\nLab 4: Docker and Lambda Due Sept 19 6pm\nAssignment 4: Containers Due Sept 25 11:59pm\nLab 5: DuckDB & Polars Due Sept 26 6pm\n\n\nLook back and ahead\n\nContinue to use Slack for questions!\nDocker (containerization)\nLambda functions\nComing up: Spark and project"
  },
  {
    "objectID": "w04/slides.html#raw-ingredients-of-storage-systems",
    "href": "w04/slides.html#raw-ingredients-of-storage-systems",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Raw Ingredients of Storage Systems",
    "text": "Raw Ingredients of Storage Systems\n\n\n\nDisk drives (magnetic HDDs or SSDs)\nRAM\nNetworking and CPU\nSerialization\nCompression\nCaching\n\n\n\n\n\nFrom Reis and Housley (2022)"
  },
  {
    "objectID": "w04/slides.html#single-machine-vs.-distributed-storage",
    "href": "w04/slides.html#single-machine-vs.-distributed-storage",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Single-Machine vs.¬†Distributed Storage",
    "text": "Single-Machine vs.¬†Distributed Storage\n\nFrom Reis and Housley (2022)\n\n\nSingle-Machine\n\n\nThey are commonly used for storing operating system files, application files, and user data files.\nFilesystems are also used in databases to store data files, transaction logs, and backups.\n\n\n\nDistributed Storage\n\n\nA distributed filesystem is a type of filesystem that spans multiple computers.\nIt provides a unified view of files across all the computers in the system.\nHave existed before cloud"
  },
  {
    "objectID": "w04/slides.html#file-storage-types",
    "href": "w04/slides.html#file-storage-types",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "File Storage Types",
    "text": "File Storage Types\n\n\n\nLocal Disk\n\n\nOS-managed filesystems on local disk partition:\nNTFS (Windows)\nHFS+ (MacOS)\next4 (Linux)() on a local disk partition of SSD or magnetic disk\n\n\n\nNetwork-Attached (NAS)\n\n\nAccessed by clients over a network\nRedundancy and reliability, fine-grained control of resources, storage pooling across multiple disks for large virtual volumes, and file sharing across multiple machines\n\n\nCloud Filesystems\n\nNot object store (more on that later)\nNot the virtual hard drive attached to a virtual machine\nFully managed: Takes care of networking, managing disk clusters, failures, and configuration (Azure Files, Amazon Elastic Filesystem)\nBacked by Object Store\n\n\nBased on Reis and Housley (2022)"
  },
  {
    "objectID": "w04/slides.html#object-stores",
    "href": "w04/slides.html#object-stores",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Object Stores",
    "text": "Object Stores\n\nSomewhat confusing because object has several meanings in computer science.\nIn this context, we‚Äôre talking about a specialized file-like construct. It could be any type of file: TXT, CSV, JSON, images, videos, audio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContains objects of all shapes and sizes: each gets a unique identifier\nObjects are immutable: cannot be modified in place (unlike local FS)"
  },
  {
    "objectID": "w04/slides.html#distributed-fs-vs-object-store",
    "href": "w04/slides.html#distributed-fs-vs-object-store",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Distributed FS vs Object Store",
    "text": "Distributed FS vs Object Store\n\n\n\n\n\n\n\n\n\nDistributed File System\nObject Storage\n\n\n\n\nOrganization\nFiles in hierarchical directories\nFlat organization (though there can be overlays to provide hierarchical files structure)\n\n\nMethod\nPOSIX File Operations\nREST API\n\n\nImmutability\nNone: Random writes anywhere in file\nImmutable: need to replace/append entire object\n\n\nPerformance\nPerforms best for smaller files\nPerforms best for large files\n\n\nScalability\nMillions of files\nBillions of objects\n\n\n\nBoth provide:\n\nFault tolerance\nAvailability and consistency"
  },
  {
    "objectID": "w04/slides.html#before-data-locality-for-hadoop",
    "href": "w04/slides.html#before-data-locality-for-hadoop",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Before: Data locality (for Hadoop)",
    "text": "Before: Data locality (for Hadoop)\n\n\n\nFrom White (2015)"
  },
  {
    "objectID": "w04/slides.html#today-de-coupling-storage-from-compute",
    "href": "w04/slides.html#today-de-coupling-storage-from-compute",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Today: De-Coupling Storage from Compute",
    "text": "Today: De-Coupling Storage from Compute\n\nFrom Gopalan (2022)"
  },
  {
    "objectID": "w04/slides.html#data-on-disk-formats",
    "href": "w04/slides.html#data-on-disk-formats",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Data-on-Disk Formats",
    "text": "Data-on-Disk Formats\n\nPlain Text (CSV, TSV, FWF)\nJSON\nBinary Files"
  },
  {
    "objectID": "w04/slides.html#plain-text-csv-tsv-fwf",
    "href": "w04/slides.html#plain-text-csv-tsv-fwf",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Plain Text (CSV, TSV, FWF)",
    "text": "Plain Text (CSV, TSV, FWF)\n\n\n\n\n\n\nPay attention to encodings!\nLines end in linefeed, carriage-return, or both together depending on the OS that generated\nTypically, a single line of text contains a single record"
  },
  {
    "objectID": "w04/slides.html#json",
    "href": "w04/slides.html#json",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "JSON",
    "text": "JSON\n\n\n\n\n\n\nWarning\n\n\nJSON files have two flavors: JSON Lines vs.¬†JSON. Typically when we say data is in JSON format, we imply it‚Äôs JSON Lines which means that there is a single JSON object per line, and there are multiple lines.\n\n\n\n\n\nJSON Lines\n4 records, one per line, no end comma\n{\"id\":1, \"name\":\"marck\", \"last_name\":\"vaisman\"}\n{\"id\":2, \"name\":\"anderson\", \"last_name\":\"monken\"}\n{\"id\":3, \"name\":\"amit\", \"last_name\":\"arora\"}\n{\"id\":4, \"name\":\"abhijit\", \"last_name\":\"dasgupta\"}\n\nJSON\n4 records enclosed in 1 JSON Array\n[\n  {\"id\":1, \"name\":\"marck\", \"last_name\":\"vaisman\"},\n  {\"id\":2, \"name\":\"anderson\", \"last_name\":\"monken\"},\n  {\"id\":3, \"name\":\"amit\", \"last_name\":\"arora\"},\n  {\"id\":4, \"name\":\"abhijit\", \"last_name\":\"dasgupta\"},\n]"
  },
  {
    "objectID": "w04/slides.html#binary-files",
    "href": "w04/slides.html#binary-files",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Binary Files",
    "text": "Binary Files"
  },
  {
    "objectID": "w04/slides.html#issues-with-common-file-formats-especially-csv",
    "href": "w04/slides.html#issues-with-common-file-formats-especially-csv",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Issues with Common File Formats (Especially CSV)",
    "text": "Issues with Common File Formats (Especially CSV)\n\nUbiquitous but highly error-prone\nDefault delimiter: familiar character in English, the comma\nAmbiguities:\n\nDelimiter (comma, tab, semi-colon, custom)\nQuote characters (single or doble quote)\nEscaping to appropriately handle string data\n\nDoesn‚Äôt natively encode schema information\nNo direct support for nested structures\nEncoding+schema must be configured on target system to ensure ingestion\nAutodetection provided in many cloud environments but is inappropriate for production ingestion (can be painfully slow)\nData engineers often forced to work with CSV data and then build robust exception handling and error detection to ensure data quality (Pydantic!)"
  },
  {
    "objectID": "w04/slides.html#introducing-apache-parquet",
    "href": "w04/slides.html#introducing-apache-parquet",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Introducing Apache Parquet",
    "text": "Introducing Apache Parquet\n\nFree, open-source, column-oriented data storage format created by Twitter and Cloudera (v1.0 released July 2013)\nData stored in columnar format (as opposed to row format), designed for read and write performance\nBuilds in schema information and natively supports nested data\nSupported by R and Python through Apache Arrow (more coming up!)"
  },
  {
    "objectID": "w04/slides.html#traditional-row-store",
    "href": "w04/slides.html#traditional-row-store",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Traditional Row-Store",
    "text": "Traditional Row-Store\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuery: ‚ÄúHow many balls did we sell?\nThe engine must scan each and every row until the end!"
  },
  {
    "objectID": "w04/slides.html#column-store",
    "href": "w04/slides.html#column-store",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Column-Store",
    "text": "Column-Store\n\n\n\n\n\n\n\nParquet file format, everything you need to know"
  },
  {
    "objectID": "w04/slides.html#row-groups",
    "href": "w04/slides.html#row-groups",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Row Groups",
    "text": "Row Groups\n\n\n\nData is stored in row groups!\n\n\n\nOnly required fields"
  },
  {
    "objectID": "w04/slides.html#metadata-compression-and-dictionary-encoding",
    "href": "w04/slides.html#metadata-compression-and-dictionary-encoding",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Metadata, compression, and dictionary encoding",
    "text": "Metadata, compression, and dictionary encoding"
  },
  {
    "objectID": "w04/slides.html#apache-arrow-for-in-memory-analytics",
    "href": "w04/slides.html#apache-arrow-for-in-memory-analytics",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Apache Arrow for In-Memory Analytics",
    "text": "Apache Arrow for In-Memory Analytics\n\nApache Arrow is a development platform for in-memory analytics. It contains a set of technologies that enable big data systems to process and move data fast. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. (Topol and McKinney 2024)"
  },
  {
    "objectID": "w04/slides.html#before-arrow",
    "href": "w04/slides.html#before-arrow",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Before Arrow",
    "text": "Before Arrow\n\nTopol and McKinney (2024)"
  },
  {
    "objectID": "w04/slides.html#after-arrow",
    "href": "w04/slides.html#after-arrow",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "After Arrow",
    "text": "After Arrow\n\nTopol and McKinney (2024)"
  },
  {
    "objectID": "w04/slides.html#arrow-compatibility",
    "href": "w04/slides.html#arrow-compatibility",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Arrow Compatibility",
    "text": "Arrow Compatibility\n\nTopol and McKinney (2024)"
  },
  {
    "objectID": "w04/slides.html#arrow-performance",
    "href": "w04/slides.html#arrow-performance",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Arrow Performance",
    "text": "Arrow Performance\n\n\n\n\n\nTopol and McKinney (2024)\n\n\n\n\n\n\nTopol and McKinney (2024)"
  },
  {
    "objectID": "w04/slides.html#using-arrow-with-csv-and-parquet",
    "href": "w04/slides.html#using-arrow-with-csv-and-parquet",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Using Arrow with CSV and Parquet",
    "text": "Using Arrow with CSV and Parquet\n\n\n\nPython\n\npyarrow or from pandas\nimport pandas as pd\npd.read_csv(engine = 'pyarrow')\npd.read_parquet()\n\nimport pyarrow.csv\npyarrow.csv.read_csv()\n\nimport pyarrow.parquet\npyarrow.parquet.read_table()\n\n\nR\n\nUse the arrow package\nlibrary(arrow)\n\nread_csv_arrow()\nread_parquet()\nread_json_arrow()\n\nwrite_csv_arrow()\nwrite_parquet()\n\nRecommendation: save your intermediate and analytical datasets as Parquet!"
  },
  {
    "objectID": "w04/slides.html#before-we-begin",
    "href": "w04/slides.html#before-we-begin",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Before We Begin‚Ä¶",
    "text": "Before We Begin‚Ä¶\n\nPandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\n\nPandas is slow, but less slow if you use it the right way!\n\nApache Arrow and the ‚Äú10 Things I Hate About pandas‚Äù (A 2017 post from the creator of Pandas‚Ä¶))\n50x faster data loading in Pandas: no problem (an old 2019 article‚Ä¶)\nIs Pandas really that slow?\nPandas 2.0 and the arrow revolution"
  },
  {
    "objectID": "w04/slides.html#polars-1",
    "href": "w04/slides.html#polars-1",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Polars",
    "text": "Polars"
  },
  {
    "objectID": "w04/slides.html#why-is-polars-faster-than-pandas",
    "href": "w04/slides.html#why-is-polars-faster-than-pandas",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Why is Polars Faster than Pandas?",
    "text": "Why is Polars Faster than Pandas?\n\nPolars is written in Rust. Rust is compiled; Python is interpreted\n\nCompiled language: you generate the machine code only once then run it, subsequent runs do not need the compilation step.\nInterpreted language: code has to be parsed, interpreted and converted into machine code every single time.\n\nParallelization: Vectorized operations can be executed in parallel on multiple cores\nLazy evaluation: Polars supports two APIs lazy as well as eager evaluation (used by pandas). In lazy evaluation, a query is executed only when required. While in eager evaluation, a query is executed immediately.\nPolars uses Arrow for in-memory data representation. Similar to how pandas uses NumPy (Pandas 2 allows using Arrow as backend)\nPolars \\(\\approx\\) in-memory DataFrame library + query optimizer\n\n\n\n[Excerpt from this post from Ritchie Vink, author of Polars] Arrow provides the efficient data structures and some compute kernels, like a SUM, a FILTER, a MAX etc. Arrow is not a query engine. Polars is a DataFrame library on top of arrow that has implemented efficient algorithms for JOINS, GROUPBY, PIVOTs, MELTs, QUERY OPTIMIZATION, etc. (the things you expect from a DF lib)."
  },
  {
    "objectID": "w04/slides.html#ease-of-use",
    "href": "w04/slides.html#ease-of-use",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Ease of Use",
    "text": "Ease of Use\n\nFamiliar API for users of Pandas: differences in syntax but still a Dataframe API making it straightforward to perform common operations such as filtering, aggregating, and joining data\nSee Migrating from Pandas\nReading data\n# must install s3fs -&gt; \"pip install s3fs\"\n# Using Polars\nimport polars as pl\npolars_df = pl.read_parquet(\"s3://nyc-tlc/trip data/yellow_tripdata_2023-06.parquet\")\n\n# using Pandas\nimport pandas as pd\npandas_df = pd.read_parquet(\"s3://nyc-tlc/trip data/yellow_tripdata_2023-06.parquet\")\nSelecting columns (see Pushdown optimization)\n# Using Polars\nselected_columns_polars = polars_df[['column1', 'column2']]\n\n# Using Pandas\nselected_columns_pandas = pandas_df[['column1', 'column2']]"
  },
  {
    "objectID": "w04/slides.html#ease-of-use-contd.",
    "href": "w04/slides.html#ease-of-use-contd.",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Ease of Use (contd.)",
    "text": "Ease of Use (contd.)\n\nFiltering data\n# Using Polars\nfiltered_polars = polars_df[polars_df['column1'] &gt; 10]\n\n# Using Pandas\nfiltered_pandas = pandas_df[pandas_df['column1'] &gt; 10]\nThough you can write Polars code that looks like Pandas, better to write idiomatic Polars code that takes advantage of Polars‚Äô features\nMigrating from Apache Spark: Whereas Spark DataFrame is a collection of rows, Polars DataFrame is closer to a collection of columns"
  },
  {
    "objectID": "w04/slides.html#installation-data-loading-and-basic-operations",
    "href": "w04/slides.html#installation-data-loading-and-basic-operations",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Installation, Data Loading, and Basic Operations",
    "text": "Installation, Data Loading, and Basic Operations\nInstall via pip:\npip install polars\nImport polars in your Python code and read data as usual:\nimport polars as pl\ndf = pl.read_parquet(\"s3://nyc-tlc/trip data/yellow_tripdata_2023-06.parquet\")\ndf.head()\n\nshape: (5, 19)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ VendorID ‚îÜ tpep_picku ‚îÜ tpep_dropoff ‚îÜ passenger_co ‚îÜ ... ‚îÜ improvement_ ‚îÜ total_amount ‚îÜ congestion_s ‚îÜ Airport_fee ‚îÇ\n‚îÇ ---      ‚îÜ p_datetime ‚îÜ _datetime    ‚îÜ unt          ‚îÜ     ‚îÜ surcharge    ‚îÜ ---          ‚îÜ urcharge     ‚îÜ ---         ‚îÇ\n‚îÇ i32      ‚îÜ ---        ‚îÜ ---          ‚îÜ ---          ‚îÜ     ‚îÜ ---          ‚îÜ f64          ‚îÜ ---          ‚îÜ f64         ‚îÇ\n‚îÇ          ‚îÜ datetime[n ‚îÜ datetime[ns] ‚îÜ i64          ‚îÜ     ‚îÜ f64          ‚îÜ              ‚îÜ f64          ‚îÜ             ‚îÇ\n‚îÇ          ‚îÜ s]         ‚îÜ              ‚îÜ              ‚îÜ     ‚îÜ              ‚îÜ              ‚îÜ              ‚îÜ             ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ 1        ‚îÜ 2023-06-01 ‚îÜ 2023-06-01   ‚îÜ 1            ‚îÜ ... ‚îÜ 1.0          ‚îÜ 33.6         ‚îÜ 2.5          ‚îÜ 0.0         ‚îÇ\n‚îÇ          ‚îÜ 00:08:48   ‚îÜ 00:29:41     ‚îÜ              ‚îÜ     ‚îÜ              ‚îÜ              ‚îÜ              ‚îÜ             ‚îÇ\n‚îÇ 1        ‚îÜ 2023-06-01 ‚îÜ 2023-06-01   ‚îÜ 0            ‚îÜ ... ‚îÜ 1.0          ‚îÜ 23.6         ‚îÜ 2.5          ‚îÜ 0.0         ‚îÇ\n‚îÇ          ‚îÜ 00:15:04   ‚îÜ 00:25:18     ‚îÜ              ‚îÜ     ‚îÜ              ‚îÜ              ‚îÜ              ‚îÜ             ‚îÇ\n‚îÇ 1        ‚îÜ 2023-06-01 ‚îÜ 2023-06-01   ‚îÜ 1            ‚îÜ ... ‚îÜ 1.0          ‚îÜ 60.05        ‚îÜ 0.0          ‚îÜ 1.75        ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "w04/slides.html#polars-pipeline-example",
    "href": "w04/slides.html#polars-pipeline-example",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Polars Pipeline Example",
    "text": "Polars Pipeline Example\nWe‚Äôll run this as part of the lab in a little bit; think how you might code this in Pandas‚Ä¶\n\nPolars pipeline"
  },
  {
    "objectID": "w04/slides.html#further-reading",
    "href": "w04/slides.html#further-reading",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Further reading",
    "text": "Further reading\n\nPolars\nUser guide ‚Üê MUST READ\nPolars GitHub repo\nPandas Vs Polars: a syntax and speed comparison\nTips & tricks for working with strings in Polars"
  },
  {
    "objectID": "w04/slides.html#duckdb-1",
    "href": "w04/slides.html#duckdb-1",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "DuckDB",
    "text": "DuckDB\n\nDuckDB is an in-process SQL OLAP DB management system\nLike sqlite, but for analytics. What does this mean? It means that your database runs inside your process, there are no servers to manage, no remote system to connect to. Easy to experiment with SQL-like syntax.\nVectorized processing: Loads chunks of data into memory (tries to keep everything in the CPU‚Äôs L1 and L2 cache) and is thus able to handle datasets bigger than the amount of RAM available.\nSupports Python, R and a host of other languages\nImportant paper on DuckDB: Raasveldt and M√ºhleisen (2019)"
  },
  {
    "objectID": "w04/slides.html#key-features",
    "href": "w04/slides.html#key-features",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Key Features",
    "text": "Key Features\n\nColumnar Storage, Vectorized Query Processing: DuckDB contains a columnar-vectorized query execution engine, where queries are run on a large batch of values (a ‚Äúvector‚Äù) in one operation\n\nMost analytical queries (think group by and summarize) or even data retrieval for training ML models require retrieving a subset of columns and now the entire row, columnar storage make this faster\n\nIn-Memory Processing: All data needed for processing is brought within the process memory (recall that columnar storage format helps with this) making queries run faster (no DB call over the network)\nSQL Support: highly Postgres-compatible version of SQL1.\nACID Compliance: Transactional guarantees (ACID properties) through bulk-optimized Multi-Version Concurrency Control (MVCC).\n\nFriendlier SQL with DuckDB"
  },
  {
    "objectID": "w04/slides.html#use-cases-for-duckdb",
    "href": "w04/slides.html#use-cases-for-duckdb",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Use Cases for DuckDB",
    "text": "Use Cases for DuckDB\n\nData Warehousing\nBusiness Intelligence\nReal-Time Analytics\nIoT Data Processing\n\nDuckDB in the Wild\n\nHow We Silently Switched Mode‚Äôs In-Memory Data Engine to DuckDB To Boost Visual Data Exploration Speed\nWhy we built Rill with DuckDB\nLeveraging DuckDB for enhanced performance in dbt projects"
  },
  {
    "objectID": "w04/slides.html#duckdb-diy",
    "href": "w04/slides.html#duckdb-diy",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "DuckDB: DIY",
    "text": "DuckDB: DIY\n\nDatalake and DuckDBUsing DuckDB in AWS Lambda"
  },
  {
    "objectID": "w04/slides.html#duckdb-fully-managed",
    "href": "w04/slides.html#duckdb-fully-managed",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "DuckDB: Fully-Managed",
    "text": "DuckDB: Fully-Managed\n\nMotherDuck Architecture\nArchitecture and capabilities\nSeamlessly analyze data, whether it sits on your laptop, in the cloud or split between.\nHybrid execution automatically plans each part of your query and determines where it‚Äôs best computed\nDuckDB Vs MotherDuck"
  },
  {
    "objectID": "w04/slides.html#setting-up-duckdb",
    "href": "w04/slides.html#setting-up-duckdb",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Setting Up DuckDB",
    "text": "Setting Up DuckDB\nConfiguration and Initialization: DuckDB is integrated into Python and R for efficient interactive data analysis (APIs for Java, C, C++, Julia, Swift, and others)\npip install duckdb\nConnecting to DuckDB\nimport duckdb\n# directly query a Pandas DataFrame\nimport pandas as pd\ndata_url = \"https://raw.githubusercontent.com/anly503/datasets/main/EconomistData.csv\"\ndf = pd.read_csv(data_url)\nduckdb.sql('SELECT * FROM df')"
  },
  {
    "objectID": "w04/slides.html#setting-up-duckdb-contd.",
    "href": "w04/slides.html#setting-up-duckdb-contd.",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Setting Up DuckDB (contd.)",
    "text": "Setting Up DuckDB (contd.)\nSupported data formats: DuckDB can ingest data from a wide variety of formats ‚Äì both on-disk and in-memory. See the data ingestion page for more information.\nimport duckdb\nduckdb.read_csv('example.csv')                # read a CSV file into a Relation\nduckdb.read_parquet('example.parquet')        # read a Parquet file into a Relation\nduckdb.read_json('example.json')              # read a JSON file into a Relation\n\nduckdb.sql('SELECT * FROM \"example.csv\"')     # directly query a CSV file\nduckdb.sql('SELECT * FROM \"example.parquet\"') # directly query a Parquet file\nduckdb.sql('SELECT * FROM \"example.json\"')    # directly query a JSON file"
  },
  {
    "objectID": "w04/slides.html#querying-duckdb",
    "href": "w04/slides.html#querying-duckdb",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Querying DuckDB",
    "text": "Querying DuckDB\nEssential Reading\n\nFriendlier SQL with DuckDB\nSQL Introduction\n\nBasic SQL Queries\nimport duckdb\nimport pandas as pd\nbabynames = pd.read_parquet(\"https://github.com/anly503/datasets/raw/main/babynames.parquet.zstd\")\nduckdb.sql(\"select count(*)  from babynames where Name='John'\")\nAggregations and Grouping\nduckdb.sql(\"select State, Name, count(*) as count  from babynames group by State, Name order by State desc, count desc\")"
  },
  {
    "objectID": "w04/slides.html#querying-duckdb-contd.",
    "href": "w04/slides.html#querying-duckdb-contd.",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Querying DuckDB (contd.)",
    "text": "Querying DuckDB (contd.)\nEssential reading: FROM and JOIN clauses\nJoins and Subqueries\n# Join two tables together\nduckdb.sql(\"SELECT * FROM table_name JOIN other_table ON (table_name.key = other_table.key\")\nWindow Functions\npowerplants = pd.read_csv(\"https://raw.githubusercontent.com/anly503/datasets/main/powerplants.csv\", parse_dates=[\"date\"])\nq = \"\"\"\nSELECT \"plant\", \"date\",\n    AVG(\"MWh\") OVER (\n        PARTITION BY \"plant\"\n        ORDER BY \"date\" ASC\n        RANGE BETWEEN INTERVAL 3 DAYS PRECEDING\n                  AND INTERVAL 3 DAYS FOLLOWING)\n        AS \"MWh 7-day Moving Average\"\nFROM powerplants\nORDER BY 1, 2;\n\"\"\"\nduckdb.sql(q)"
  },
  {
    "objectID": "w04/slides.html#using-the-duckdb-cli-and-shell",
    "href": "w04/slides.html#using-the-duckdb-cli-and-shell",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Using the DuckDB CLI and Shell",
    "text": "Using the DuckDB CLI and Shell\n\nInstall DuckDB CLI or use in browser via shell.duckdb.org for data exploration via SQL; Once installed, import a local file into the shell and run queries\nYou can download powerplants.csv here\nC:\\Users\\&lt;username&gt;\\Downloads\\duckdb_cli-windows-amd64&gt; duckdb\nv0.8.1 6536a77232\nEnter \".help\" for usage hints.\nConnected to a transient in-memory database.\nUse \".open FILENAME\" to reopen on a persistent database.\nD CREATE TABLE powerplants AS SELECT * FROM read_csv_auto('powerplants.csv');\nD DESCRIBE powerplants;\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ column_name ‚îÇ column_type ‚îÇ  null   ‚îÇ   key   ‚îÇ default ‚îÇ extra ‚îÇ\n‚îÇ   varchar   ‚îÇ   varchar   ‚îÇ varchar ‚îÇ varchar ‚îÇ varchar ‚îÇ int32 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ plant       ‚îÇ VARCHAR     ‚îÇ YES     ‚îÇ         ‚îÇ         ‚îÇ       ‚îÇ\n‚îÇ date        ‚îÇ DATE        ‚îÇ YES     ‚îÇ         ‚îÇ         ‚îÇ       ‚îÇ\n‚îÇ MWh         ‚îÇ BIGINT      ‚îÇ YES     ‚îÇ         ‚îÇ         ‚îÇ       ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\nD  SELECT * from powerplants where plant='Boston' and date='2019-01-02';\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  plant  ‚îÇ    date    ‚îÇ  MWh   ‚îÇ\n‚îÇ varchar ‚îÇ    date    ‚îÇ int64  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ Boston  ‚îÇ 2019-01-02 ‚îÇ 564337 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "w04/slides.html#profiling-in-duckdb",
    "href": "w04/slides.html#profiling-in-duckdb",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Profiling in DuckDB",
    "text": "Profiling in DuckDB\nQuery Optimization: Use EXPLAIN and ANALYZE keywords to understand how your query is being executed and the time being spent in individual steps\nD EXPLAIN ANALYZE SELECT * from powerplants where plant='Boston' and date='2019-01-02';\nDuckDB will use all the cores available on the underlying compute, but you can adjust this (Full configuration details here)\nD select current_setting('threads');\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ current_setting('threads') ‚îÇ\n‚îÇ           int64            ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ                          8 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\nD SET threads=4;\nD select current_setting('threads');\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ current_setting('threads') ‚îÇ\n‚îÇ           int64            ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ                          4 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "w04/slides.html#benchmarks-and-comparisons",
    "href": "w04/slides.html#benchmarks-and-comparisons",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Benchmarks and Comparisons",
    "text": "Benchmarks and Comparisons\n\n\n\n\nTricky topic! Can make your chosen solution look better by focusing on metrics on which it provides better results\nIn general, TPC-H and TPC-DS are considered the standard benchmarks for data processing.\n\n\n\n\n\n\nTPC-DS Homepage"
  },
  {
    "objectID": "w04/slides.html#further-reading-on-duckdb",
    "href": "w04/slides.html#further-reading-on-duckdb",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Further Reading on DuckDB",
    "text": "Further Reading on DuckDB\n\nParallel Grouped Aggregation in DuckDB\nMeta queries\nProfiling queries in DuckDB\nDuckDB tutorial for beginners\nDuckDB CLI API\nUsing DuckDB in AWS Lambda\nRevisiting the Poor Man‚Äôs Data Lake with MotherDuck\nSupercharge your data processing with DuckDB\nFriendlier SQL with DuckDB\nBuilding and deploying data apps with DuckDB and Streamlit"
  },
  {
    "objectID": "w04/slides.html#references",
    "href": "w04/slides.html#references",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "References",
    "text": "References\n\n\nGopalan, Rukmani. 2022. The Cloud Data Lake: A Guide to Building Robust Cloud Data Architecture. O‚ÄôReilly Media, Inc. https://www.dropbox.com/scl/fi/7u469ccc8y169us5hydk0/The-cloud-data-lake-a-guide-to-building-robust-cloud-data-Rukmani-Gopalan.pdf?rlkey=ey06a6zt9g90d4zq8tfncndta&dl=1.\n\n\nRaasveldt, Mark, and Hannes M√ºhleisen. 2019. ‚ÄúDuckDB: An Embeddable Analytical Database.‚Äù In Proceedings of the 2019 International Conference on Management of Data, 1981‚Äì84. SIGMOD ‚Äô19. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3299869.3320212.\n\n\nReis, Joe, and Matt Housley. 2022. Fundamentals of Data Engineering: Plan and Build Robust Data Systems. O‚ÄôReilly Media, Inc.\n\n\nTopol, Matthew, and Wes McKinney. 2024. In-Memory Analytics with Apache Arrow. Packt Publishing Ltd.\n\n\nWhite, Tom E. 2015. Hadoop: The Definitive Guide. O‚ÄôReilly Media, Inc."
  },
  {
    "objectID": "w02/index.html",
    "href": "w02/index.html",
    "title": "Week 2: Cloud Computing",
    "section": "",
    "text": "Open slides in new tab ‚Üí",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#jeffs-favorite-big-data-definition",
    "href": "w02/index.html#jeffs-favorite-big-data-definition",
    "title": "Week 2: Cloud Computing",
    "section": "Jeff‚Äôs Favorite ‚ÄúBig Data‚Äù Definition",
    "text": "Jeff‚Äôs Favorite ‚ÄúBig Data‚Äù Definition\n¬†\n\nBig data is when the size of the data itself becomes part of the problem (Loukides 2010)\n\n\nIn other words: Your problem becomes a ‚Äúbig data problem‚Äù when you hit one or both of the walls!",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#you-vs.-your-opps",
    "href": "w02/index.html#you-vs.-your-opps",
    "title": "Week 2: Cloud Computing",
    "section": "You vs.¬†Your Opps",
    "text": "You vs.¬†Your Opps",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#so-you-find-yourself-smashed-against-a-wall",
    "href": "w02/index.html#so-you-find-yourself-smashed-against-a-wall",
    "title": "Week 2: Cloud Computing",
    "section": "So you find yourself smashed against a wall‚Ä¶",
    "text": "So you find yourself smashed against a wall‚Ä¶\n(Some of yall would fold in this scenario)",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#the-pre-cloud-approach-make-the-one-computer-faster-and-faster",
    "href": "w02/index.html#the-pre-cloud-approach-make-the-one-computer-faster-and-faster",
    "title": "Week 2: Cloud Computing",
    "section": "The ‚ÄúPre-Cloud‚Äù Approach: Make The One Computer Faster and Faster",
    "text": "The ‚ÄúPre-Cloud‚Äù Approach: Make The One Computer Faster and Faster",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#it-worked-for-many-decades",
    "href": "w02/index.html#it-worked-for-many-decades",
    "title": "Week 2: Cloud Computing",
    "section": "It Worked For‚Ä¶ Many Decades!",
    "text": "It Worked For‚Ä¶ Many Decades!",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#is-moores-law-dead",
    "href": "w02/index.html#is-moores-law-dead",
    "title": "Week 2: Cloud Computing",
    "section": "Is Moore‚Äôs Law Dead?",
    "text": "Is Moore‚Äôs Law Dead?\n\n‚Ä¶Kind of?\nFocus nowadays: specialized hardware / compute architectures hyper-optimized for particular tasks\n\n[If you took DSAN 5500] Think of BLAS\n\nGraphic Processing Units (GPUs)\nField Programmable Gate Arrays (FPGAs)\nData Processing Units (DPUs)\nGoogle‚Äôs Tensor Processing Units (TPUs)",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#so-even-upgrading-our-hardware-didnt-help-now-what-do-we-do",
    "href": "w02/index.html#so-even-upgrading-our-hardware-didnt-help-now-what-do-we-do",
    "title": "Week 2: Cloud Computing",
    "section": "So, even upgrading our hardware didn‚Äôt help üò≠ now what do we do?",
    "text": "So, even upgrading our hardware didn‚Äôt help üò≠ now what do we do?\n\nWe distribute!\nMore CPUs, more memory, more storage!",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#my-favorite-example-ever",
    "href": "w02/index.html#my-favorite-example-ever",
    "title": "Week 2: Cloud Computing",
    "section": "My Favorite Example Ever",
    "text": "My Favorite Example Ever\n\nSpongebob is cooking Krabby Patties too slowly to satisfy ravenous customers‚Ä¶\nShould he upgrade his one spatula? How bout just many simple spatulas at once!",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#how-do-we-achieve-this-thats-exactly-what-the-cloud-is-for",
    "href": "w02/index.html#how-do-we-achieve-this-thats-exactly-what-the-cloud-is-for",
    "title": "Week 2: Cloud Computing",
    "section": "How Do We Achieve This? ‚Ä¶That‚Äôs Exactly What The Cloud is For!",
    "text": "How Do We Achieve This? ‚Ä¶That‚Äôs Exactly What The Cloud is For!",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#benefits",
    "href": "w02/index.html#benefits",
    "title": "Week 2: Cloud Computing",
    "section": "Benefits",
    "text": "Benefits\n\n\n\n\nProvides access to low-cost computing\nCosts are decreasing every year\nElastic\nPaaS works!\nMany other benefits‚Ä¶",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#q-what-is-the-cloud",
    "href": "w02/index.html#q-what-is-the-cloud",
    "title": "Week 2: Cloud Computing",
    "section": "Q: What is The Cloud?",
    "text": "Q: What is The Cloud?\n\nA: Using someone else‚Äôs computers",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#nist-definition",
    "href": "w02/index.html#nist-definition",
    "title": "Week 2: Cloud Computing",
    "section": "NIST Definition",
    "text": "NIST Definition\n\n\n\n\n\n\nBased on Mell and Grance (2011) (The ‚Äúofficial‚Äù NIST definition of ‚ÄúCloud Computing‚Äù)",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#service-models",
    "href": "w02/index.html#service-models",
    "title": "Week 2: Cloud Computing",
    "section": "Service Models",
    "text": "Service Models",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#hotel-analogy",
    "href": "w02/index.html#hotel-analogy",
    "title": "Week 2: Cloud Computing",
    "section": "Hotel Analogy",
    "text": "Hotel Analogy",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#infrastructure-as-a-service-iaas",
    "href": "w02/index.html#infrastructure-as-a-service-iaas",
    "title": "Week 2: Cloud Computing",
    "section": "Infrastructure as a Service (IaaS)",
    "text": "Infrastructure as a Service (IaaS)\n\nVirtualized computing resources delivered over the internet\nProvider manages: Physical hardware, virtualization, networking, storage\nYou manage: Operating systems, applications, runtime, data, middleware\n\nExamples and Use Cases\n\nAmazon EC2, Microsoft Azure VMs, Google Compute Engine\nPerfect for: Development environments, web hosting, backup & recovery\nBenefits: Rapid scaling, pay-as-you-go, global availability\n\n# Launch a virtual machine in seconds\naws ec2 run-instances --image-id ami-12345 --instance-type t3.xlarge",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#platform-as-a-service-paas",
    "href": "w02/index.html#platform-as-a-service-paas",
    "title": "Week 2: Cloud Computing",
    "section": "Platform as a Service (PaaS)",
    "text": "Platform as a Service (PaaS)\n\nWhat is PaaS?\n\nComplete development and deployment environment in the cloud\nProvider manages: Infrastructure, operating systems, runtime environments\nYou manage: Applications and data\n\nExamples and Use Cases\n\nAWS Elastic Beanstalk, Google App Engine, Microsoft Azure App Service\nPerfect for: Web applications, API development, microservices\nBenefits: Faster development, automatic scaling, integrated DevOps\n\n# Deploy your app with minimal configuration\ngit push heroku main  # App automatically deployed and scaled",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#software-as-a-service-saas",
    "href": "w02/index.html#software-as-a-service-saas",
    "title": "Week 2: Cloud Computing",
    "section": "Software as a Service (SaaS)",
    "text": "Software as a Service (SaaS)\n\nComplete applications delivered over the internet\nProvider manages: Everything (infrastructure, platform, software)\nYou manage: Your data and user access\n\nExamples and Use Cases\n\nGmail, Salesforce, Microsoft 365, Slack, Zoom\nPerfect for: Business applications, collaboration tools, CRM\nBenefits: No installation, automatic updates, accessible anywhere\n\nThe SaaS Revolution\n\n80% of companies use SaaS applications\n$195 billion market size (2023)",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#cloud-infrastructure-and-the-ai-revolution",
    "href": "w02/index.html#cloud-infrastructure-and-the-ai-revolution",
    "title": "Week 2: Cloud Computing",
    "section": "Cloud Infrastructure and the AI Revolution",
    "text": "Cloud Infrastructure and the AI Revolution\n\nMassive Computational Requirements\n\nTraining large language models requires thousands of GPUs\nCloud provides elastic access to specialized hardware\n\nData Storage and Processing\n\nAI models need petabytes of training data\nCloud offers unlimited, globally distributed storage\n\nCost Efficiency\n\nPay-per-use model for expensive AI hardware\nNo upfront investment in GPU clusters",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#gpu-infrastructure-the-ai-powerhouse",
    "href": "w02/index.html#gpu-infrastructure-the-ai-powerhouse",
    "title": "Week 2: Cloud Computing",
    "section": "GPU Infrastructure: The AI Powerhouse",
    "text": "GPU Infrastructure: The AI Powerhouse\nFrom Gaming to AI\n\nOriginally: Graphics processing for video games\nNow: Parallel processing powerhouse for AI/ML workloads\nArchitecture: Thousands of cores vs.¬†CPUs 8-64 cores\n\nCloud GPU Offerings\n# AWS GPU instances for different AI workloads\np4d.24xlarge   # 8x NVIDIA A100 (40GB) - $32/hour\np3.16xlarge    # 8x NVIDIA V100 (16GB) - $24/hour\ng4dn.xlarge    # 1x NVIDIA T4 (16GB)   - $0.50/hour\nThe Scale Challenge\n\nGPT-4 training: Estimated 25,000 A100 GPUs for 6 months\nMeta‚Äôs AI infrastructure: 350,000+ NVIDIA H100 chips\nCost: Single AI training run can cost $100M+",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#distributed-computing-and-large-clusters",
    "href": "w02/index.html#distributed-computing-and-large-clusters",
    "title": "Week 2: Cloud Computing",
    "section": "Distributed Computing and Large Clusters",
    "text": "Distributed Computing and Large Clusters\nModern AI Requires Massive Clusters\n\nModel parallelism: Split models across multiple GPUs\nData parallelism: Process different data batches simultaneously\n\nPipeline parallelism: Different layers on different GPUs\n\nTechnologies Enabling Scale\n\nKubernetes: Container orchestration for microservices\nApache Spark: Distributed data processing framework\nRay: Distributed AI/ML framework for Python\n\n# Distributed training with Ray\nimport ray\nfrom ray import tune\n# Scale across 1000+ machines automatically\ntune.run(train_model, resources_per_trial={\"gpu\": 8})",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#data-storage-evolution",
    "href": "w02/index.html#data-storage-evolution",
    "title": "Week 2: Cloud Computing",
    "section": "Data Storage Evolution",
    "text": "Data Storage Evolution\nThe Data Tsunami\n\n2025 projection: 181 zettabytes of data globally\nAI training datasets: CommonCrawl (800TB), ImageNet (150GB)\nReal-time requirements: 1-10ms latency for inference (some cases sub-millisecond)\n\nStorage Technologies\n\nObject Storage: Amazon S3, Google Cloud Storage (petabyte scale)\nHigh-performance: NVMe SSDs, persistent memory\nDistributed filesystems: HDFS, GlusterFS, Ceph\n\nThe Economics of Data\n\nStorage costs: Dropped 99.9% since 1980\nTransfer costs: Still significant for large datasets\nEdge computing: Bringing compute closer to data sources",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#evolution-of-the-cloud",
    "href": "w02/index.html#evolution-of-the-cloud",
    "title": "Week 2: Cloud Computing",
    "section": "Evolution of the Cloud",
    "text": "Evolution of the Cloud\n\n\n\n\n\n\n\n\nYesterday\nToday\nTomorrow\n\n\n\n\nLimited number of tools and vendors\nMany tools and vendors to work with\nIntegrated tools and vendors\n\n\nOne platform - few devices\nMultiple platforms - many devices\nConnected platforms and devices\n\n\nData is scarce but manageable\nOverabundance of data\nData is used for important business decisions\n\n\nIT has major influence and control\nIT has limited influence and control\nIT is strategic to the business\n\n\nPeople only work when they are at work\nPeople work wherever they want\nPeople have access to what they need wherever they are",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#what-does-the-cloud-look-like-microsoft-azure-data-center",
    "href": "w02/index.html#what-does-the-cloud-look-like-microsoft-azure-data-center",
    "title": "Week 2: Cloud Computing",
    "section": "What Does the Cloud Look Like? Microsoft Azure Data Center",
    "text": "What Does the Cloud Look Like? Microsoft Azure Data Center",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#loudon-county-va-cloudon",
    "href": "w02/index.html#loudon-county-va-cloudon",
    "title": "Week 2: Cloud Computing",
    "section": "Loudon County, VA: ‚ÄúCLoudon‚Äù",
    "text": "Loudon County, VA: ‚ÄúCLoudon‚Äù\n\n70% of the world‚Äôs internet traffic passes through Loudon\nHow data centers power VA‚Äôs Loudon County\nThe heart of ‚ÄúThe Cloud‚Äù is in Virginia\nCBS Sunday Morning Visits the Home of the Internet in Loudoun County",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#further-reading-saas-and-cloud-computing",
    "href": "w02/index.html#further-reading-saas-and-cloud-computing",
    "title": "Week 2: Cloud Computing",
    "section": "Further Reading: SaaS and Cloud Computing",
    "text": "Further Reading: SaaS and Cloud Computing\nSaaS and Cloud Computing Statistics:\n\n1. Zylo: 111 Unmissable SaaS Statistics for 2025\n2. Spendesk: 60+ eye-opening SaaS statistics (2025)\n3. Substly: The 28 most important SaaS statistics in 2023\n4. Harpa AI: SaaS Industry Key Statistics 2023\n5. Content Beta: 110+ SaaS Statistics and Trends\n6. Statista: Average number of SaaS apps used in the U.S. 2024\n7. Meetanshi: SaaS Statistics for 2025: Market Size, Growth, Trends & More",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#data-storage-and-ai-infrastructure",
    "href": "w02/index.html#data-storage-and-ai-infrastructure",
    "title": "Week 2: Cloud Computing",
    "section": "Data Storage and AI Infrastructure",
    "text": "Data Storage and AI Infrastructure\n\n2025 global data volume (181 zettabytes)\n\nrivery.io\nExplodingTopics.com\nPIT.edu\nForbes\n\nCommonCrawl and ImageNet dataset sizes\nAI inference latency and storage tech:\n\nIBM\nRCR Wireless",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#storage-economics-and-technology",
    "href": "w02/index.html#storage-economics-and-technology",
    "title": "Week 2: Cloud Computing",
    "section": "Storage Economics and Technology",
    "text": "Storage Economics and Technology\n\nStorage cost declines\n\nHumanProgress.org\nOur World In Data\n\nOngoing significance of transfer (egress) costs\nEdge computing trends",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#gpu-infrastructure-and-ai-training-costs",
    "href": "w02/index.html#gpu-infrastructure-and-ai-training-costs",
    "title": "Week 2: Cloud Computing",
    "section": "GPU Infrastructure and AI Training Costs",
    "text": "GPU Infrastructure and AI Training Costs\n\nGPT-4 GPU count and training duration\n\nGenspark.ai\nklu.ai\nacorn.io\n\nMeta‚Äôs NVIDIA H100 GPUs\nAI training cost estimates\n\nGenspark.ai\nacorn.io",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#references",
    "href": "w02/index.html#references",
    "title": "Week 2: Cloud Computing",
    "section": "References",
    "text": "References\n\n\nLoukides, Mike. 2010. ‚ÄúWhat Is Data Science?‚Äù O‚ÄôReilly Media. https://www.oreilly.com/radar/what-is-data-science/.\n\n\nMell, Peter, and Timothy Grance. 2011. ‚ÄúThe NIST Definition of Cloud Computing.‚Äù National Institute of Standards and Technology, Special Publication 800 (2011): 145. https://nvlpubs.nist.gov/nistpubs/legacy/sp/nistspecialpublication800-145.pdf.",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/slides.html#jeffs-favorite-big-data-definition",
    "href": "w02/slides.html#jeffs-favorite-big-data-definition",
    "title": "Week 2: Cloud Computing",
    "section": "Jeff‚Äôs Favorite ‚ÄúBig Data‚Äù Definition",
    "text": "Jeff‚Äôs Favorite ‚ÄúBig Data‚Äù Definition\n¬†\n\nBig data is when the size of the data itself becomes part of the problem (Loukides 2010)\n\n\nIn other words: Your problem becomes a ‚Äúbig data problem‚Äù when you hit one or both of the walls!"
  },
  {
    "objectID": "w02/slides.html#you-vs.-your-opps",
    "href": "w02/slides.html#you-vs.-your-opps",
    "title": "Week 2: Cloud Computing",
    "section": "You vs.¬†Your Opps",
    "text": "You vs.¬†Your Opps"
  },
  {
    "objectID": "w02/slides.html#so-you-find-yourself-smashed-against-a-wall",
    "href": "w02/slides.html#so-you-find-yourself-smashed-against-a-wall",
    "title": "Week 2: Cloud Computing",
    "section": "So you find yourself smashed against a wall‚Ä¶",
    "text": "So you find yourself smashed against a wall‚Ä¶\n(Some of yall would fold in this scenario)"
  },
  {
    "objectID": "w02/slides.html#the-pre-cloud-approach-make-the-one-computer-faster-and-faster",
    "href": "w02/slides.html#the-pre-cloud-approach-make-the-one-computer-faster-and-faster",
    "title": "Week 2: Cloud Computing",
    "section": "The ‚ÄúPre-Cloud‚Äù Approach: Make The One Computer Faster and Faster",
    "text": "The ‚ÄúPre-Cloud‚Äù Approach: Make The One Computer Faster and Faster"
  },
  {
    "objectID": "w02/slides.html#it-worked-for-many-decades",
    "href": "w02/slides.html#it-worked-for-many-decades",
    "title": "Week 2: Cloud Computing",
    "section": "It Worked For‚Ä¶ Many Decades!",
    "text": "It Worked For‚Ä¶ Many Decades!"
  },
  {
    "objectID": "w02/slides.html#is-moores-law-dead",
    "href": "w02/slides.html#is-moores-law-dead",
    "title": "Week 2: Cloud Computing",
    "section": "Is Moore‚Äôs Law Dead?",
    "text": "Is Moore‚Äôs Law Dead?\n\n‚Ä¶Kind of?\nFocus nowadays: specialized hardware / compute architectures hyper-optimized for particular tasks\n\n[If you took DSAN 5500] Think of BLAS\n\nGraphic Processing Units (GPUs)\nField Programmable Gate Arrays (FPGAs)\nData Processing Units (DPUs)\nGoogle‚Äôs Tensor Processing Units (TPUs)"
  },
  {
    "objectID": "w02/slides.html#so-even-upgrading-our-hardware-didnt-help-now-what-do-we-do",
    "href": "w02/slides.html#so-even-upgrading-our-hardware-didnt-help-now-what-do-we-do",
    "title": "Week 2: Cloud Computing",
    "section": "So, even upgrading our hardware didn‚Äôt help üò≠ now what do we do?",
    "text": "So, even upgrading our hardware didn‚Äôt help üò≠ now what do we do?\n\nWe distribute!\nMore CPUs, more memory, more storage!"
  },
  {
    "objectID": "w02/slides.html#my-favorite-example-ever",
    "href": "w02/slides.html#my-favorite-example-ever",
    "title": "Week 2: Cloud Computing",
    "section": "My Favorite Example Ever",
    "text": "My Favorite Example Ever\n\nSpongebob is cooking Krabby Patties too slowly to satisfy ravenous customers‚Ä¶\nShould he upgrade his one spatula? How bout just many simple spatulas at once!"
  },
  {
    "objectID": "w02/slides.html#how-do-we-achieve-this-thats-exactly-what-the-cloud-is-for",
    "href": "w02/slides.html#how-do-we-achieve-this-thats-exactly-what-the-cloud-is-for",
    "title": "Week 2: Cloud Computing",
    "section": "How Do We Achieve This? ‚Ä¶That‚Äôs Exactly What The Cloud is For!",
    "text": "How Do We Achieve This? ‚Ä¶That‚Äôs Exactly What The Cloud is For!"
  },
  {
    "objectID": "w02/slides.html#benefits",
    "href": "w02/slides.html#benefits",
    "title": "Week 2: Cloud Computing",
    "section": "Benefits",
    "text": "Benefits\n\n\n\n\nProvides access to low-cost computing\nCosts are decreasing every year\nElastic\nPaaS works!\nMany other benefits‚Ä¶"
  },
  {
    "objectID": "w02/slides.html#q-what-is-the-cloud",
    "href": "w02/slides.html#q-what-is-the-cloud",
    "title": "Week 2: Cloud Computing",
    "section": "Q: What is The Cloud?",
    "text": "Q: What is The Cloud?\n\nA: Using someone else‚Äôs computers"
  },
  {
    "objectID": "w02/slides.html#nist-definition",
    "href": "w02/slides.html#nist-definition",
    "title": "Week 2: Cloud Computing",
    "section": "NIST Definition",
    "text": "NIST Definition\n\n\n\n\n\n\nBased on Mell and Grance (2011) (The ‚Äúofficial‚Äù NIST definition of ‚ÄúCloud Computing‚Äù)"
  },
  {
    "objectID": "w02/slides.html#service-models",
    "href": "w02/slides.html#service-models",
    "title": "Week 2: Cloud Computing",
    "section": "Service Models",
    "text": "Service Models"
  },
  {
    "objectID": "w02/slides.html#hotel-analogy",
    "href": "w02/slides.html#hotel-analogy",
    "title": "Week 2: Cloud Computing",
    "section": "Hotel Analogy",
    "text": "Hotel Analogy"
  },
  {
    "objectID": "w02/slides.html#infrastructure-as-a-service-iaas",
    "href": "w02/slides.html#infrastructure-as-a-service-iaas",
    "title": "Week 2: Cloud Computing",
    "section": "Infrastructure as a Service (IaaS)",
    "text": "Infrastructure as a Service (IaaS)\n\nVirtualized computing resources delivered over the internet\nProvider manages: Physical hardware, virtualization, networking, storage\nYou manage: Operating systems, applications, runtime, data, middleware\n\nExamples and Use Cases\n\nAmazon EC2, Microsoft Azure VMs, Google Compute Engine\nPerfect for: Development environments, web hosting, backup & recovery\nBenefits: Rapid scaling, pay-as-you-go, global availability\n\n# Launch a virtual machine in seconds\naws ec2 run-instances --image-id ami-12345 --instance-type t3.xlarge"
  },
  {
    "objectID": "w02/slides.html#platform-as-a-service-paas",
    "href": "w02/slides.html#platform-as-a-service-paas",
    "title": "Week 2: Cloud Computing",
    "section": "Platform as a Service (PaaS)",
    "text": "Platform as a Service (PaaS)\nWhat is PaaS?\n\nComplete development and deployment environment in the cloud\nProvider manages: Infrastructure, operating systems, runtime environments\nYou manage: Applications and data\n\nExamples and Use Cases\n\nAWS Elastic Beanstalk, Google App Engine, Microsoft Azure App Service\nPerfect for: Web applications, API development, microservices\nBenefits: Faster development, automatic scaling, integrated DevOps\n\n# Deploy your app with minimal configuration\ngit push heroku main  # App automatically deployed and scaled"
  },
  {
    "objectID": "w02/slides.html#software-as-a-service-saas",
    "href": "w02/slides.html#software-as-a-service-saas",
    "title": "Week 2: Cloud Computing",
    "section": "Software as a Service (SaaS)",
    "text": "Software as a Service (SaaS)\n\nComplete applications delivered over the internet\nProvider manages: Everything (infrastructure, platform, software)\nYou manage: Your data and user access\n\nExamples and Use Cases\n\nGmail, Salesforce, Microsoft 365, Slack, Zoom\nPerfect for: Business applications, collaboration tools, CRM\nBenefits: No installation, automatic updates, accessible anywhere\n\nThe SaaS Revolution\n\n80% of companies use SaaS applications\n$195 billion market size (2023)"
  },
  {
    "objectID": "w02/slides.html#cloud-infrastructure-and-the-ai-revolution",
    "href": "w02/slides.html#cloud-infrastructure-and-the-ai-revolution",
    "title": "Week 2: Cloud Computing",
    "section": "Cloud Infrastructure and the AI Revolution",
    "text": "Cloud Infrastructure and the AI Revolution\n\nMassive Computational Requirements\n\nTraining large language models requires thousands of GPUs\nCloud provides elastic access to specialized hardware\n\nData Storage and Processing\n\nAI models need petabytes of training data\nCloud offers unlimited, globally distributed storage\n\nCost Efficiency\n\nPay-per-use model for expensive AI hardware\nNo upfront investment in GPU clusters"
  },
  {
    "objectID": "w02/slides.html#gpu-infrastructure-the-ai-powerhouse",
    "href": "w02/slides.html#gpu-infrastructure-the-ai-powerhouse",
    "title": "Week 2: Cloud Computing",
    "section": "GPU Infrastructure: The AI Powerhouse",
    "text": "GPU Infrastructure: The AI Powerhouse\nFrom Gaming to AI\n\nOriginally: Graphics processing for video games\nNow: Parallel processing powerhouse for AI/ML workloads\nArchitecture: Thousands of cores vs.¬†CPUs 8-64 cores\n\nCloud GPU Offerings\n# AWS GPU instances for different AI workloads\np4d.24xlarge   # 8x NVIDIA A100 (40GB) - $32/hour\np3.16xlarge    # 8x NVIDIA V100 (16GB) - $24/hour\ng4dn.xlarge    # 1x NVIDIA T4 (16GB)   - $0.50/hour\nThe Scale Challenge\n\nGPT-4 training: Estimated 25,000 A100 GPUs for 6 months\nMeta‚Äôs AI infrastructure: 350,000+ NVIDIA H100 chips\nCost: Single AI training run can cost $100M+"
  },
  {
    "objectID": "w02/slides.html#distributed-computing-and-large-clusters",
    "href": "w02/slides.html#distributed-computing-and-large-clusters",
    "title": "Week 2: Cloud Computing",
    "section": "Distributed Computing and Large Clusters",
    "text": "Distributed Computing and Large Clusters\nModern AI Requires Massive Clusters\n\nModel parallelism: Split models across multiple GPUs\nData parallelism: Process different data batches simultaneously\n\nPipeline parallelism: Different layers on different GPUs\n\nTechnologies Enabling Scale\n\nKubernetes: Container orchestration for microservices\nApache Spark: Distributed data processing framework\nRay: Distributed AI/ML framework for Python\n\n# Distributed training with Ray\nimport ray\nfrom ray import tune\n# Scale across 1000+ machines automatically\ntune.run(train_model, resources_per_trial={\"gpu\": 8})"
  },
  {
    "objectID": "w02/slides.html#data-storage-evolution",
    "href": "w02/slides.html#data-storage-evolution",
    "title": "Week 2: Cloud Computing",
    "section": "Data Storage Evolution",
    "text": "Data Storage Evolution\nThe Data Tsunami\n\n2025 projection: 181 zettabytes of data globally\nAI training datasets: CommonCrawl (800TB), ImageNet (150GB)\nReal-time requirements: 1-10ms latency for inference (some cases sub-millisecond)\n\nStorage Technologies\n\nObject Storage: Amazon S3, Google Cloud Storage (petabyte scale)\nHigh-performance: NVMe SSDs, persistent memory\nDistributed filesystems: HDFS, GlusterFS, Ceph\n\nThe Economics of Data\n\nStorage costs: Dropped 99.9% since 1980\nTransfer costs: Still significant for large datasets\nEdge computing: Bringing compute closer to data sources"
  },
  {
    "objectID": "w02/slides.html#evolution-of-the-cloud",
    "href": "w02/slides.html#evolution-of-the-cloud",
    "title": "Week 2: Cloud Computing",
    "section": "Evolution of the Cloud",
    "text": "Evolution of the Cloud\n\n\n\n\n\n\n\n\nYesterday\nToday\nTomorrow\n\n\n\n\nLimited number of tools and vendors\nMany tools and vendors to work with\nIntegrated tools and vendors\n\n\nOne platform - few devices\nMultiple platforms - many devices\nConnected platforms and devices\n\n\nData is scarce but manageable\nOverabundance of data\nData is used for important business decisions\n\n\nIT has major influence and control\nIT has limited influence and control\nIT is strategic to the business\n\n\nPeople only work when they are at work\nPeople work wherever they want\nPeople have access to what they need wherever they are"
  },
  {
    "objectID": "w02/slides.html#what-does-the-cloud-look-like-microsoft-azure-data-center",
    "href": "w02/slides.html#what-does-the-cloud-look-like-microsoft-azure-data-center",
    "title": "Week 2: Cloud Computing",
    "section": "What Does the Cloud Look Like? Microsoft Azure Data Center",
    "text": "What Does the Cloud Look Like? Microsoft Azure Data Center"
  },
  {
    "objectID": "w02/slides.html#loudon-county-va-cloudon",
    "href": "w02/slides.html#loudon-county-va-cloudon",
    "title": "Week 2: Cloud Computing",
    "section": "Loudon County, VA: ‚ÄúCLoudon‚Äù",
    "text": "Loudon County, VA: ‚ÄúCLoudon‚Äù\n\n70% of the world‚Äôs internet traffic passes through Loudon\nHow data centers power VA‚Äôs Loudon County\nThe heart of ‚ÄúThe Cloud‚Äù is in Virginia\nCBS Sunday Morning Visits the Home of the Internet in Loudoun County"
  },
  {
    "objectID": "w02/slides.html#further-reading-saas-and-cloud-computing",
    "href": "w02/slides.html#further-reading-saas-and-cloud-computing",
    "title": "Week 2: Cloud Computing",
    "section": "Further Reading: SaaS and Cloud Computing",
    "text": "Further Reading: SaaS and Cloud Computing\nSaaS and Cloud Computing Statistics:\n\n1. Zylo: 111 Unmissable SaaS Statistics for 2025\n2. Spendesk: 60+ eye-opening SaaS statistics (2025)\n3. Substly: The 28 most important SaaS statistics in 2023\n4. Harpa AI: SaaS Industry Key Statistics 2023\n5. Content Beta: 110+ SaaS Statistics and Trends\n6. Statista: Average number of SaaS apps used in the U.S. 2024\n7. Meetanshi: SaaS Statistics for 2025: Market Size, Growth, Trends & More"
  },
  {
    "objectID": "w02/slides.html#data-storage-and-ai-infrastructure",
    "href": "w02/slides.html#data-storage-and-ai-infrastructure",
    "title": "Week 2: Cloud Computing",
    "section": "Data Storage and AI Infrastructure",
    "text": "Data Storage and AI Infrastructure\n\n2025 global data volume (181 zettabytes)\n\nrivery.io\nExplodingTopics.com\nPIT.edu\nForbes\n\nCommonCrawl and ImageNet dataset sizes\nAI inference latency and storage tech:\n\nIBM\nRCR Wireless"
  },
  {
    "objectID": "w02/slides.html#storage-economics-and-technology",
    "href": "w02/slides.html#storage-economics-and-technology",
    "title": "Week 2: Cloud Computing",
    "section": "Storage Economics and Technology",
    "text": "Storage Economics and Technology\n\nStorage cost declines\n\nHumanProgress.org\nOur World In Data\n\nOngoing significance of transfer (egress) costs\nEdge computing trends"
  },
  {
    "objectID": "w02/slides.html#gpu-infrastructure-and-ai-training-costs",
    "href": "w02/slides.html#gpu-infrastructure-and-ai-training-costs",
    "title": "Week 2: Cloud Computing",
    "section": "GPU Infrastructure and AI Training Costs",
    "text": "GPU Infrastructure and AI Training Costs\n\nGPT-4 GPU count and training duration\n\nGenspark.ai\nklu.ai\nacorn.io\n\nMeta‚Äôs NVIDIA H100 GPUs\nAI training cost estimates\n\nGenspark.ai\nacorn.io"
  },
  {
    "objectID": "w02/slides.html#references",
    "href": "w02/slides.html#references",
    "title": "Week 2: Cloud Computing",
    "section": "References",
    "text": "References\n\n\nLoukides, Mike. 2010. ‚ÄúWhat Is Data Science?‚Äù O‚ÄôReilly Media. https://www.oreilly.com/radar/what-is-data-science/.\n\n\nMell, Peter, and Timothy Grance. 2011. ‚ÄúThe NIST Definition of Cloud Computing.‚Äù National Institute of Standards and Technology, Special Publication 800 (2011): 145. https://nvlpubs.nist.gov/nistpubs/legacy/sp/nistspecialpublication800-145.pdf."
  }
]