[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resource Hub",
    "section": "",
    "text": "Week 4 Polars Demo\nWeek 4 DuckDB Demo",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#in-class-demos",
    "href": "resources.html#in-class-demos",
    "title": "Resource Hub",
    "section": "",
    "text": "Week 4 Polars Demo\nWeek 4 DuckDB Demo",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#lab-.zips",
    "href": "resources.html#lab-.zips",
    "title": "Resource Hub",
    "section": "Lab .zips",
    "text": "Lab .zips\nAccess the ZIP files here",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#extra-writeups",
    "href": "resources.html#extra-writeups",
    "title": "Resource Hub",
    "section": "Extra Writeups",
    "text": "Extra Writeups\n\n\n\n\n\n\n\nFrom Pandas to SQL\n\n\nConverting Code into Queries\n\n\n\nExtra Writeups\n\n\n\n\nSep 20, 2025\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#key-books-by-week",
    "href": "resources.html#key-books-by-week",
    "title": "Resource Hub",
    "section": "Key Books By Week",
    "text": "Key Books By Week\n\nWeek 1: Course Overview and Week 2: Cloud Computing\n\nThe Boar Book: Kleppmann (2017), Designing Data-Intensive Applications\n\n\n\nWeek 3: Parallel Concepts\n\nThe Wolohan MapReduce Book: Wolohan (2020), Mastering Large Datasets with Python\n\n\n\nWeek 4: DuckDB, Polars, File Formats\n\nThe Lynx Book: Janssens and Nieuwdorp (2025), Python Polars: The Definitive Guide\nThe I-Need-Ham, Hunger Book: Needham, Hunger, and Simons (2024), DuckDB in Action\n\n\n\nWeek 5: Data Warehousing\nTwo Packt Books:\n\nGeneral Data Engineering on AWS: Eagar (2021), Data Engineering with AWS\nData Engineering with Athena: Virtuoso et al. (2021), Serverless Analytics with Amazon Athena",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#pdfs-of-books",
    "href": "resources.html#pdfs-of-books",
    "title": "Resource Hub",
    "section": "PDFs of Books",
    "text": "PDFs of Books\nThe remainder of this page is auto-generated from all of the references across the slides for each week: click on the name of a reference to download the ebook version, if available!\n\n\nBarber, David. 2012. Bayesian Reasoning and Machine Learning. Cambridge University Press.\n\n\nEagar, Gareth. 2021. Data Engineering with AWS: Learn How to Design and Build Cloud-Based Data Transformation Pipelines Using AWS. Packt Publishing Ltd.\n\n\nGopalan, Rukmani. 2022. The Cloud Data Lake: A Guide to Building Robust Cloud Data Architecture. O‚ÄôReilly Media, Inc.\n\n\nJanssens, Jeroen, and Thijs Nieuwdorp. 2025. Python Polars: The Definitive Guide: Transforming, Analyzing, and Visualizing Data with a Fast and Expressive DataFrame API. O‚ÄôReilly Media, Inc.\n\n\nKleppmann, Martin. 2017. Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems. O‚ÄôReilly Media, Inc.\n\n\nLeskovec, Jure, Anand Rajaraman, and Jeffrey David Ullman. 2014. Mining of Massive Datasets. Cambridge University Press.\n\n\nLoukides, Mike. 2010. ‚ÄúWhat Is Data Science?‚Äù O‚ÄôReilly Media.\n\n\nMell, Peter, and Timothy Grance. 2011. ‚ÄúThe NIST Definition of Cloud Computing.‚Äù National Institute of Standards and Technology, Special Publication 800 (2011): 145.\n\n\nNeedham, Mark, Michael Hunger, and Michael Simons. 2024. DuckDB in Action. Simon and Schuster.\n\n\nRaasveldt, Mark, and Hannes M√ºhleisen. 2019. ‚ÄúDuckDB: An Embeddable Analytical Database.‚Äù In Proceedings of the 2019 International Conference on Management of Data, 1981‚Äì84. SIGMOD ‚Äô19. New York, NY, USA: Association for Computing Machinery.\n\n\nReis, Joe, and Matt Housley. 2022. Fundamentals of Data Engineering: Plan and Build Robust Data Systems. O‚ÄôReilly Media, Inc.\n\n\nTopol, Matthew, and Wes McKinney. 2024. In-Memory Analytics with Apache Arrow. Packt Publishing Ltd.\n\n\nVirtuoso, Anthony, Mert Turkay Hocanin, Aaron Wishnick, and Rahul Pathak. 2021. Serverless Analytics with Amazon Athena: Query Structured, Unstructured, or Semi-Structured Data in Seconds Without Setting up Any Infrastructure. Packt Publishing Ltd.\n\n\nWhite, Tom E. 2015. Hadoop: The Definitive Guide. O‚ÄôReilly Media, Inc.\n\n\nWolohan, John. 2020. Mastering Large Datasets with Python: Parallelize and Distribute Your Python Code. Simon and Schuster.",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "w01/index.html",
    "href": "w01/index.html",
    "title": "Week 1: Course Overview",
    "section": "",
    "text": "Open slides in new tab ‚Üí",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#agenda-for-todays-session",
    "href": "w01/index.html#agenda-for-todays-session",
    "title": "Week 1: Course Overview",
    "section": "Agenda for today‚Äôs session",
    "text": "Agenda for today‚Äôs session\n\nCourse and syllabus overview\nBig Data Concepts\n\nDefinition\nChallenges\nApproaches\n\nData Engineering\nIntroduction to bash\n\nLab: Linux command line",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#bookmark-these-links",
    "href": "w01/index.html#bookmark-these-links",
    "title": "Week 1: Course Overview",
    "section": "Bookmark these links!",
    "text": "Bookmark these links!\n\nCourse website: https://gu-dsan.github.io/6000-fall-2025/\nGitHub Organization for your deliverables: https://github.com/gu-dsan/\nGitHub Classroom: https://classroom.github.com/classrooms/34950344-georgetown-university-dsan6000-big-data-and-cloud-computing\nSlack Workspace: DSAN6000 Fall 2025 - https://dsan6000fall2025.slack.com\n\nJoin link: https://join.slack.com/t/dsan6000fall2025/shared_invite/zt-3b22qhque-GagQykwYYNiEzli9UXJn4w\n\nInstructors email: dsan-Fall-2025@georgetown.edu\nCanvas: https://georgetown.instructure.com/courses/TBA-2025\n\n\n\n\n\n\n\nThese are also pinned on the Slack main channel",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#instructional-team---professors",
    "href": "w01/index.html#instructional-team---professors",
    "title": "Week 1: Course Overview",
    "section": "Instructional Team - Professors",
    "text": "Instructional Team - Professors\n\nAmit Arora, aa1603@georgetown.edu\nJeff Jacobs, jj1088@georgetown.edu",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#amit-arora-aa1603georgetown.edu",
    "href": "w01/index.html#amit-arora-aa1603georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Amit Arora, aa1603@georgetown.edu",
    "text": "Amit Arora, aa1603@georgetown.edu\n\n\n\nPrincipal Solutions Architect - AI/ML at AWS\nAdjunct Professor at Georgetown University\nMultiple patents in telecommunications and applications of ML in telecommunications\n\nFun Facts\n\n\n\n\n\nI am a self-published author https://blueberriesinmysalad.com/\nMy book ‚ÄúBlueberries in my salad: my forever journey towards fitness & strength‚Äù is written as code in R and Markdown\nI love to read books about health and human performance, productivity, philosophy and Mathematics for ML. My reading list is online!",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#jeff-jacobs-jj1088georgetown.edu",
    "href": "w01/index.html#jeff-jacobs-jj1088georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Jeff Jacobs, jj1088@georgetown.edu",
    "text": "Jeff Jacobs, jj1088@georgetown.edu\n\n\n\nFull-time Professor at Georgetown (DSAN and Public Policy)\nBackground in Computational Social Science (Comp Sci MS ‚Üí Political Economy PhD ‚Üí Labor Econ Postdoc)\n\nFun Facts\n\nUsed Apache Airflow daily for PhD projects! (Example)\n\n\n\n\n\n\nServer admin for lab server ‚Üí lab AWS account at Columbia (2015-2023) ‚Üí new DSAN server (!) (2025-)\nPassion project 1: Code for Palestine (2015-2022) ‚Üí YouthCode-Gaza (2023) ‚Üí Ukraine Ministry of Digital Transformation (2024)\nPassion projects 2+3 [ü§ì]: Sample-based music production, web app frameworks\nSleep disorder means lots of reading ‚Äì mainly history! ‚Äì at night\nAlso teaching PPOL6805 / DSAN 6750: GIS for Spatial Data Science this semester",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#instructional-team---teaching-assistants",
    "href": "w01/index.html#instructional-team---teaching-assistants",
    "title": "Week 1: Course Overview",
    "section": "Instructional Team - Teaching Assistants",
    "text": "Instructional Team - Teaching Assistants\n\nBinhui Chen, bc928@georgetown.edu\nPranav Sudhir Patil, pp755@georgetown.edu\nOfure Udabor, au195@georgetown.edu\nYifei Wu, yw924@georgetown.edu\nNaomi Yamaguchi, ny159@georgetown.edu\nLeqi Ying, ly290@georgetown.edu\nXinyue (Monica) Zhang, xz646@georgetown.edu",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#binhui-chen-bc928georgetown.edu",
    "href": "w01/index.html#binhui-chen-bc928georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Binhui Chen, bc928@georgetown.edu",
    "text": "Binhui Chen, bc928@georgetown.edu\n(Lead TA for the course!)",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#pranav-sudhir-patil-pp755georgetown.edu",
    "href": "w01/index.html#pranav-sudhir-patil-pp755georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Pranav Sudhir Patil, pp755@georgetown.edu",
    "text": "Pranav Sudhir Patil, pp755@georgetown.edu",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#ofure-udabor-au195georgetown.edu",
    "href": "w01/index.html#ofure-udabor-au195georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Ofure Udabor, au195@georgetown.edu",
    "text": "Ofure Udabor, au195@georgetown.edu",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#yifei-wu-yw924georgetown.edu",
    "href": "w01/index.html#yifei-wu-yw924georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Yifei Wu, yw924@georgetown.edu",
    "text": "Yifei Wu, yw924@georgetown.edu",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#naomi-yamaguchi-ny159georgetown.edu",
    "href": "w01/index.html#naomi-yamaguchi-ny159georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Naomi Yamaguchi, ny159@georgetown.edu",
    "text": "Naomi Yamaguchi, ny159@georgetown.edu",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#leqi-ying-ly290georgetown.edu",
    "href": "w01/index.html#leqi-ying-ly290georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Leqi Ying, ly290@georgetown.edu",
    "text": "Leqi Ying, ly290@georgetown.edu",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#xinyue-monica-zhang-xz646georgetown.edu",
    "href": "w01/index.html#xinyue-monica-zhang-xz646georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Xinyue (Monica) Zhang, xz646@georgetown.edu",
    "text": "Xinyue (Monica) Zhang, xz646@georgetown.edu",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#course-description",
    "href": "w01/index.html#course-description",
    "title": "Week 1: Course Overview",
    "section": "Course Description",
    "text": "Course Description\n\nData is everywhere! Many times, it‚Äôs just too big to work with traditional tools. This is a hands-on, practical workshop style course about using cloud computing resources to do analysis and manipulation of datasets that are too large to fit on a single machine and/or analyzed with traditional tools. The course will focus on Spark, MapReduce, the Hadoop Ecosystem and other tools.\nYou will understand how to acquire and/or ingest the data, and then massage, clean, transform, analyze, and model it within the context of big data analytics. You will be able to think more programmatically and logically about your big data needs, tools and issues.\n\nAlways refer to the syllabus and calendar in the course website for class policies.",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#learning-objectives",
    "href": "w01/index.html#learning-objectives",
    "title": "Week 1: Course Overview",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nSetup, operate and manage big data tools and cloud infrastructure, including Spark, DuckDB, Polars, Athena, Snowflake, and orchestration tools like Airflow on Amazon Web Services\nUse ancillary tools that support big data processing, including git and the Linux command line\nExecute a big data analytics exercise from start to finish: ingest, wrangle, clean, analyze, store, and present\nDevelop strategies to break down large problems and datasets into manageable pieces\nIdentify broad spectrum resources and documentation to remain current with big data tools and developments\nCommunicate and interpret the big data analytics results through written and verbal methods",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#evaluation",
    "href": "w01/index.html#evaluation",
    "title": "Week 1: Course Overview",
    "section": "Evaluation",
    "text": "Evaluation\n\nGroup project : 40%\nAssignments : 30%\nLab completions : 20%\nQuizzes : 10%",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#course-materials",
    "href": "w01/index.html#course-materials",
    "title": "Week 1: Course Overview",
    "section": "Course Materials",
    "text": "Course Materials\n\nSlides/labs/assignment on Website/GitHub\nQuizzes and readings in Canvas",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#communication",
    "href": "w01/index.html#communication",
    "title": "Week 1: Course Overview",
    "section": "Communication",
    "text": "Communication\n\nSlack is the primary form of communication\nInstructional team email: dsan-Fall-2025@georgetown.edu",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#slack-rules",
    "href": "w01/index.html#slack-rules",
    "title": "Week 1: Course Overview",
    "section": "Slack rules",
    "text": "Slack rules\n\nPost any question/comment about the course, assignments or any technical issue.\nDMs are to be used sparingly\nYou may not DM multiple people in the instructional team at the same time for the same issue\nKeep an eye on the questions posted in Slack. Use the search function. It‚Äôs very possible that we have already answered a questions\nYou may DM us back only if we DM you first on a given issue\nLab/assignment/project questions will only be answered up to 6 hours before something is due (i.e.¬†6pm on Mondays)",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#midterm-project-new",
    "href": "w01/index.html#midterm-project-new",
    "title": "Week 1: Course Overview",
    "section": "Midterm Project (NEW!)",
    "text": "Midterm Project (NEW!)\n\nIndividual assignment (not team-based)\nTiming: Around Week 5-6\nWeight: Equivalent to 2 homework assignments\nFormat:\n\nWe provide the dataset and problem statement\nYou apply big data tools and techniques learned in class\nEnd-to-end data pipeline implementation\n\nDetails: TBD (will be announced in Week 4)",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#final-project",
    "href": "w01/index.html#final-project",
    "title": "Week 1: Course Overview",
    "section": "Final Project",
    "text": "Final Project\n\nGroups of 3-4 students\nUse an archive of Reddit data, augmented with external data\nExploratory analysis\nNLP\nMachine Learning\nWriteup\n\nData sourcing and ingesting\nExploratory analysis\nModeling\nChallenges and Learnings\nConclusions\nFuture work",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#in-one-minute-of-time-2018",
    "href": "w01/index.html#in-one-minute-of-time-2018",
    "title": "Week 1: Course Overview",
    "section": "In one minute of time (2018)",
    "text": "In one minute of time (2018)",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#in-one-minute-of-time-2019",
    "href": "w01/index.html#in-one-minute-of-time-2019",
    "title": "Week 1: Course Overview",
    "section": "In one minute of time (2019)",
    "text": "In one minute of time (2019)",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#in-one-minute-of-time-2020",
    "href": "w01/index.html#in-one-minute-of-time-2020",
    "title": "Week 1: Course Overview",
    "section": "In one minute of time (2020)",
    "text": "In one minute of time (2020)",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#in-one-minute-of-time-2021",
    "href": "w01/index.html#in-one-minute-of-time-2021",
    "title": "Week 1: Course Overview",
    "section": "In one minute of time (2021)",
    "text": "In one minute of time (2021)",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#in-one-minute-of-time-2025",
    "href": "w01/index.html#in-one-minute-of-time-2025",
    "title": "Week 1: Course Overview",
    "section": "In one minute of time (2025)",
    "text": "In one minute of time (2025)\nEvery 60 seconds in 2025:\n\nChatGPT serves millions of requests (exact numbers proprietary)\n500 hours of video uploaded to YouTube\n1.04 million Slack messages sent\n362,000 hours watched on Netflix\n5.9-11.4 million Google searches\n$443,000 spent on Amazon\nAI-generated images created at massive scale (metrics not publicly available)\n347,200 posts on X (formerly Twitter)\n231-250 million emails sent",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#a-lot-of-it-is-hapenning-online.",
    "href": "w01/index.html#a-lot-of-it-is-hapenning-online.",
    "title": "Week 1: Course Overview",
    "section": "A lot of it is hapenning online.",
    "text": "A lot of it is hapenning online.\n\n\nWe can record every:\n\nclick\nad impression\nbilling event\nvideo interaction\nserver request\ntransaction\nnetwork message\nfault\n‚Ä¶",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#it-can-also-be-user-generated-content",
    "href": "w01/index.html#it-can-also-be-user-generated-content",
    "title": "Week 1: Course Overview",
    "section": "It can also be user-generated content:",
    "text": "It can also be user-generated content:\n\n\n\n\nInstagram posts & Reels\nX (Twitter) posts & Threads\nTikTok videos\nYouTube Shorts\nReddit discussions\nDiscord conversations\nAI-generated content (text, images, code)\n‚Ä¶",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#but-health-and-scientific-computing-create-a-lot-too",
    "href": "w01/index.html#but-health-and-scientific-computing-create-a-lot-too",
    "title": "Week 1: Course Overview",
    "section": "But health and scientific computing create a lot too!",
    "text": "But health and scientific computing create a lot too!",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#theres-lots-of-graph-data-too",
    "href": "w01/index.html#theres-lots-of-graph-data-too",
    "title": "Week 1: Course Overview",
    "section": "There‚Äôs lots of graph data too",
    "text": "There‚Äôs lots of graph data too\n\n\n\nMany interesting datasets have a graph structure:\n\nSocial networks\nGoogle‚Äôs knowledge graph\nTelecom networks\nComputer networks\nRoad networks\nCollaboration/relationships\n\nSome of these are HUGE",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#apache-web-server-log-files",
    "href": "w01/index.html#apache-web-server-log-files",
    "title": "Week 1: Course Overview",
    "section": "Apache (web server) log files",
    "text": "Apache (web server) log files",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#system-log-files",
    "href": "w01/index.html#system-log-files",
    "title": "Week 1: Course Overview",
    "section": "System log files",
    "text": "System log files",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#internet-of-things-iot-in-2025",
    "href": "w01/index.html#internet-of-things-iot-in-2025",
    "title": "Week 1: Course Overview",
    "section": "Internet of Things (IoT) in 2025",
    "text": "Internet of Things (IoT) in 2025\n75 billion connected devices generating data:\n\n\n\n\nSmart home devices (Alexa, Google Home, Apple HomePod)\nWearables (Apple Watch, Fitbit, Oura rings)\nConnected vehicles & self-driving cars\nIndustrial IoT sensors\n\n\n\n\nSmart city infrastructure\nMedical devices & remote patient monitoring",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#smartphone-location-data",
    "href": "w01/index.html#smartphone-location-data",
    "title": "Week 1: Course Overview",
    "section": "Smartphone Location Data",
    "text": "Smartphone Location Data",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#where-else",
    "href": "w01/index.html#where-else",
    "title": "Week 1: Course Overview",
    "section": "Where else?",
    "text": "Where else?\n\nThe Internet\nTransactions\nDatabases\nExcel\nPDF Files\nAnything digital (music, movies, apps)\nSome old floppy disk lying around the house",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#typical-real-world-scenarios-in-2025",
    "href": "w01/index.html#typical-real-world-scenarios-in-2025",
    "title": "Week 1: Course Overview",
    "section": "Typical Real-World Scenarios in 2025",
    "text": "Typical Real-World Scenarios in 2025\nScenario 1: Traditional Big Data\nYou have a laptop with 16GB of RAM and a 256GB SSD. You are given a 1TB dataset in text files. What do you do?\nScenario 2: AI/ML Pipeline\nYour company wants to build a RAG system using 10TB of internal documents. You need sub-second query response times. How do you architect this?\nScenario 3: Real-Time Analytics\nYou need to process 1 million events/second from IoT devices and provide real-time dashboards with &lt;1s latency. What‚Äôs your stack?",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#lets-discuss",
    "href": "w01/index.html#lets-discuss",
    "title": "Week 1: Course Overview",
    "section": "Let‚Äôs discuss!",
    "text": "Let‚Äôs discuss!\n\n\n\nExponential data growth",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#big-data-definitions",
    "href": "w01/index.html#big-data-definitions",
    "title": "Week 1: Course Overview",
    "section": "Big Data Definitions",
    "text": "Big Data Definitions\nWikipedia\n‚ÄúA collection of datasets so large and complex that it becomes difficult to process using traditional tools and applications. Big Data technologies describe a new generation of technologies and architectures designed to economically extract value from very large volumes of a wide variety of data, by enabling high-velocity capture, discover and/or analysis‚Äù\nO‚ÄôReilly\n‚ÄúBig data is when the size of the data itself becomes part of the problem‚Äù",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#frameworks-for-thinking-about-big-data",
    "href": "w01/index.html#frameworks-for-thinking-about-big-data",
    "title": "Week 1: Course Overview",
    "section": "Frameworks for Thinking About Big Data",
    "text": "Frameworks for Thinking About Big Data\nIBM (The 3 V‚Äôs)\n\nVolume (Gigabytes ‚Üí Exabytes ‚Üí Zettabytes)\nVelocity (Batch ‚Üí Streaming ‚Üí Real-time AI inference)\nVariety (Structured, Unstructured, Embeddings)\n\nAdditional V‚Äôs for 2025\n\n\n\nVariability\nVeracity\nVisualization\n\n\n\nValue\nVectors (embeddings for AI/ML)\nVersatility (multi-modal data)",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#data-size",
    "href": "w01/index.html#data-size",
    "title": "Week 1: Course Overview",
    "section": "Data ‚ÄúSize‚Äù",
    "text": "Data ‚ÄúSize‚Äù\n\\[\n\\text{``Size''} = f(\\text{Processing Ability}, \\text{Storage Space})\n\\]\n\nCan you analyze/process your data on a single machine?\nCan you store (or is it stored) on a single machine?\nCan you serve it fast enough for real-time AI applications?\n\nIf any of of the answers is no then you have a big-ish data problem!",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#the-new-data-landscape-2025",
    "href": "w01/index.html#the-new-data-landscape-2025",
    "title": "Week 1: Course Overview",
    "section": "The New Data Landscape (2025)",
    "text": "The New Data Landscape (2025)\nTraining Foundation Models\n\nGPT-4: Trained on about 13 trillion tokens\nLlama 3: 15 trillion tokens\nGoogle Gemini: Multi-modal training (text, images, video)\nEach iteration requires petabytes of curated data\n\nData Requirements Have Exploded\n\n2020: BERT trained on 3.3 billion words\n2023: GPT-4 trained on ~13 trillion tokens\n2024: Llama 3 trained on 15+ trillion tokens",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#big-data-infrastructure-data-lakes-warehouses",
    "href": "w01/index.html#big-data-infrastructure-data-lakes-warehouses",
    "title": "Week 1: Course Overview",
    "section": "Big Data Infrastructure: Data Lakes, Warehouses",
    "text": "Big Data Infrastructure: Data Lakes, Warehouses\nTraditional Use Cases:\n\nBusiness intelligence\nAnalytics & reporting\nHistorical data storage\n\nModern AI Use Cases:\n\nTraining data repositories\nVector embeddings storage\nRAG (Retrieval-Augmented Generation) context\nFine-tuning datasets\nEvaluation & benchmark data",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#rag-and-context-engineering-the-new-data-pipeline",
    "href": "w01/index.html#rag-and-context-engineering-the-new-data-pipeline",
    "title": "Week 1: Course Overview",
    "section": "RAG and Context Engineering: The New Data Pipeline",
    "text": "RAG and Context Engineering: The New Data Pipeline\n\n\n\n\n\n\n\nG\n\n\n\nraw\n\nRaw Data\n\n\n\nlake\n\nData Lake\n\n\n\nraw-&gt;lake\n\n\n\n\n\nproc\n\nProcessing\n\n\n\nlake-&gt;proc\n\n\n\n\n\nvec\n\nVector DB\n\n\n\nproc-&gt;vec\n\n\n\n\n\ncontext\n\nLLM Context\n\n\n\nvec-&gt;context\n\n\n\n\n\n\n\n\n\n\nKey Components:\n\nData Lakes (S3, Azure Data Lake): Store massive unstructured data\nData Warehouses (Snowflake, BigQuery): Structured data for context\nVector Databases (Pinecone, Weaviate, Qdrant): Semantic search\nEmbedding Models: Convert data to vectors\nOrchestration (Airflow, Prefect): Manage the pipeline",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#mcp-servers-agentic-ai",
    "href": "w01/index.html#mcp-servers-agentic-ai",
    "title": "Week 1: Course Overview",
    "section": "MCP Servers & Agentic AI",
    "text": "MCP Servers & Agentic AI\nModel Context Protocol (MCP)\n\nOpen protocol for connecting AI assistants to data sources\nStandardized way to expose tools and data to LLMs\nEnables ‚Äúagentic‚Äù behavior - AI that can act autonomously\n\nMCP in Production\n\n\n\n\n\n\n\nG\n\n\n\nware\n\nData Warehouse\n\n\n\nmcp\n\nMCP Server\n\n\n\nware-&gt;mcp\n\n\n\n\n\nagent\n\nAI Agent\n\n\n\nmcp-&gt;agent\n\n\n\n\n\naction\n\nAction\n\n\n\nagent-&gt;action\n\n\n\n\n\n\n\n\n\n\nExamples:\n\nAI agents querying Snowflake for real-time analytics\nAutonomous systems updating data lakes based on predictions\nMulti-agent systems coordinating through shared data contexts",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#data-quality-for-ai",
    "href": "w01/index.html#data-quality-for-ai",
    "title": "Week 1: Course Overview",
    "section": "Data Quality for AI",
    "text": "Data Quality for AI\n(Why Data Quality Matters More Than Ever)\nGarbage In, Garbage Out - Amplified:\n\nBad training data ‚Üí Biased models\nIncorrect RAG data ‚Üí Hallucinations\nPoor data governance ‚Üí Compliance issues\n\nData Quality Challenges in 2025\n\nScale: Validating trillions of tokens\nDiversity: Multi-modal, multi-lingual data\nVelocity: Real-time data for online learning\nVeracity: Detecting AI-generated synthetic data",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#real-world-big-data-ai-examples",
    "href": "w01/index.html#real-world-big-data-ai-examples",
    "title": "Week 1: Course Overview",
    "section": "Real-World Big Data / AI Examples",
    "text": "Real-World Big Data / AI Examples\nNetflix\n\nData Scale: 260+ million subscribers generating 100+ billion events/day\nAI Use: Personalization, content recommendations, thumbnail generation\nStack: S3 ‚Üí Spark ‚Üí Iceberg ‚Üí ML models ‚Üí Real-time serving\n\nUber\n\nData Scale: 35+ million trips per day, petabytes of location data\nAI Use: ETA prediction, surge pricing, driver-rider matching\nStack: Kafka ‚Üí Spark Streaming ‚Üí Feature Store ‚Üí ML Platform\n\nOpenAI\n\nData Scale: Trillions of tokens for training, millions of queries/day\nAI Use: GPT models, DALL-E, embeddings\nStack: Distributed training ‚Üí Vector DBs ‚Üí Inference clusters",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#emerging-trends-2025-2027",
    "href": "w01/index.html#emerging-trends-2025-2027",
    "title": "Week 1: Course Overview",
    "section": "Emerging Trends (2025-2027)",
    "text": "Emerging Trends (2025-2027)\nUnified Platforms:\n\nData lakes becoming ‚ÄúAI lakes‚Äù\nIntegrated vector + relational databases\nOne-stop shops for data + AI (Databricks, Snowflake Cortex)\n\nEdge Computing + AI:\n\nProcessing at the data source\nFederated learning across devices\n5G enabling real-time edge AI\n\nSynthetic Data:\n\nAI generating training data for AI\nPrivacy-preserving synthetic datasets",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#relative-data-sizes",
    "href": "w01/index.html#relative-data-sizes",
    "title": "Week 1: Course Overview",
    "section": "Relative Data Sizes",
    "text": "Relative Data Sizes\n¬†\n\n\n\n\nCan be processed on single machine?\nNo\nMedium(Parallel Processing)\nBig!Parallel + Distributed Processing\n\n\nYes\nSmall(Your Laptop)\nMedium(Data Streaming)\n\n\n\n\nYes\nNo\n\n\n\n\nCan be stored on single machine?",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#what-youll-learn-in-this-course",
    "href": "w01/index.html#what-youll-learn-in-this-course",
    "title": "Week 1: Course Overview",
    "section": "What You‚Äôll Learn in This Course",
    "text": "What You‚Äôll Learn in This Course\nModern Big Data Stack (2025)\n\n\n\nQuery Engines:\n\nDuckDB - In-process analytical database\nPolars - Lightning-fast DataFrame library\n\nSpark - Distributed processing at scale\n\nData Warehouses & Lakes:\n\nSnowflake - Cloud-native data warehouse\nAthena - Serverless SQL on S3\nIceberg - Open table format\n\n\n\nAI/ML Integration:\n\nVector databases for embeddings\nRAG implementation patterns\nStreaming with Spark Structured Streaming\n\nOrchestration:\n\nAirflow for pipeline management\nServerless with AWS Lambda",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#data-types",
    "href": "w01/index.html#data-types",
    "title": "Week 1: Course Overview",
    "section": "Data Types",
    "text": "Data Types\n\nStructured\nUnstructured\nNatural language\nMachine-generated\nGraph-based\nAudio, video, and images\nStreaming",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#big-data-vs.-small-data-i",
    "href": "w01/index.html#big-data-vs.-small-data-i",
    "title": "Week 1: Course Overview",
    "section": "Big Data vs.¬†Small Data I",
    "text": "Big Data vs.¬†Small Data I\n\n\n\n\n\n\n\n\n\nSmall Data is usually‚Ä¶\nOn the other hand, Big Data‚Ä¶\n\n\n\n\nGoals\ngathered for a specific goal\nmay have a goal in mind when it‚Äôs first started, but things can evolve or take unexpected directions\n\n\nLocation\nin one place, and often in a single computer file\ncan be in multiple files in multiple servers on computers in different geographic locations\n\n\nStructure/Contents\nhighly structured like an Excel spreadsheet, and it‚Äôs got rows and columns of data\ncan be unstructured, it can have many formats in files involved across disciplines, and may link to other resources\n\n\nPreparation\nprepared by the end user for their own purposes\nis often prepared by one group of people, analyzed by a second group of people, and then used by a third group of people, and they may have different purposes, and they may have different disciplines",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#big-data-vs.-small-data-ii",
    "href": "w01/index.html#big-data-vs.-small-data-ii",
    "title": "Week 1: Course Overview",
    "section": "Big Data vs.¬†Small Data II",
    "text": "Big Data vs.¬†Small Data II\n\n\n\n\n\n\n\n\n\nSmall Data is usually‚Ä¶\nOn the other hand, Big Data‚Ä¶\n\n\n\n\nLongevity\nkept for a specific amount of time after the project is over because there‚Äôs a clear ending point. In the academic world it‚Äôs maybe five or seven years and then you can throw it away\ncontains data that must be stored in perpetuity. Many big data projects extend into the past and future\n\n\nMeasurements\nmeasured with a single protocol using set units and it‚Äôs usually done at the same time\nis collected and measured using many sources, protocols, units, etc\n\n\nReproducibility\nbe reproduced in their entirety if something goes wrong in the process\nreplication is seldom feasible\n\n\nStakes\nif things go wrong the costs are limited, it‚Äôs not an enormous problem\ncan have high costs of failure in terms of money, time and labor\n\n\nAccess\nidentified by a location specified in a row/column\nunless it is exceptionally well designed, the organization can be inscrutable\n\n\nAnalysis\nanalyzed together, all at once\nis ordinarily analyzed in incremental steps",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#challenges-of-working-with-large-datasets",
    "href": "w01/index.html#challenges-of-working-with-large-datasets",
    "title": "Week 1: Course Overview",
    "section": "Challenges of Working with Large Datasets",
    "text": "Challenges of Working with Large Datasets\n\n\n\nThe V\nThe Challenge\n\n\n\n\nVolume\ndata scale\n\n\nValue\ndata usefulness in decision making\n\n\nVelocity\ndata processing: batch or stream\n\n\nViscosity\ndata complexity\n\n\nVariability\ndata flow inconsistency\n\n\nVolatility\ndata durability\n\n\nViability\ndata activeness\n\n\nValidity\ndata properly understandable\n\n\nVariety\ndata heterogeneity",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#thinking-about-big-data-workflows",
    "href": "w01/index.html#thinking-about-big-data-workflows",
    "title": "Week 1: Course Overview",
    "section": "Thinking About Big Data Workflows",
    "text": "Thinking About Big Data Workflows\nWilliam Cohen (Director, Research Engineering, Google):\n\nWorking with big data is not about‚Ä¶\n\nCode optimization\nLearning the details of today‚Äôs hardware/software (they are evolving‚Ä¶)\n\nWorking with big data is about understanding:\n\nThe cost of what you want to do\nWhat the tools that are available offer\nHow much can be accomplished with linear or nearly-linear operations\nHow to organize your computations so that they effectively use whatever‚Äôs fast\nHow to test/debug/verify with large data\n\nRecall that traditional tools like R and Python are single threaded (by default)",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#tools-at-a-glance",
    "href": "w01/index.html#tools-at-a-glance",
    "title": "Week 1: Course Overview",
    "section": "Tools at-a-glance",
    "text": "Tools at-a-glance\n\n\n\nLanguages, libraries, and projects\n\nPython\n\npandas\npolars\nPySpark\nduckdb\ndask\nray\n\nApache Arrow\nApache Spark\nSQL\nApache Hadoop (briefly)\n\n\n\n\nCloud Services\n\nAmazon Web Services (AWS)\n\nAWS Sagemaker\nAmazon S3\n\nAzure\n\nAzure Blob\nAzure Machine Learning\n\n\nOther:\n\nAWS Elastic MapReduce (EMR)",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#additional-links-of-interest",
    "href": "w01/index.html#additional-links-of-interest",
    "title": "Week 1: Course Overview",
    "section": "Additional links of interest",
    "text": "Additional links of interest\n\nMatt Turck‚Äôs Machine Learning, Artificial Intelligence & Data Landscape (MAD)\n\nArticle\nInteractive Landscape\n\nIs there life after Hadoop?\n10 Best Big Data Tools for 2023",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#data-scientist-vs.-data-engineer",
    "href": "w01/index.html#data-scientist-vs.-data-engineer",
    "title": "Week 1: Course Overview",
    "section": "Data Scientist vs.¬†Data Engineer",
    "text": "Data Scientist vs.¬†Data Engineer\nIn this course, you‚Äôll augment your data scientist skills with data engineering skills!",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#data-engineer-responsibilities",
    "href": "w01/index.html#data-engineer-responsibilities",
    "title": "Week 1: Course Overview",
    "section": "Data Engineer Responsibilities",
    "text": "Data Engineer Responsibilities",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#data-engineering-levels-2-and-3",
    "href": "w01/index.html#data-engineering-levels-2-and-3",
    "title": "Week 1: Course Overview",
    "section": "Data Engineering: Levels 2 and 3",
    "text": "Data Engineering: Levels 2 and 3",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#architecture",
    "href": "w01/index.html#architecture",
    "title": "Week 1: Course Overview",
    "section": "Architecture",
    "text": "Architecture",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#storage",
    "href": "w01/index.html#storage",
    "title": "Week 1: Course Overview",
    "section": "Storage",
    "text": "Storage",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#source-control",
    "href": "w01/index.html#source-control",
    "title": "Week 1: Course Overview",
    "section": "Source control",
    "text": "Source control",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#orchestration",
    "href": "w01/index.html#orchestration",
    "title": "Week 1: Course Overview",
    "section": "Orchestration",
    "text": "Orchestration",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#processing",
    "href": "w01/index.html#processing",
    "title": "Week 1: Course Overview",
    "section": "Processing",
    "text": "Processing",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#analytics",
    "href": "w01/index.html#analytics",
    "title": "Week 1: Course Overview",
    "section": "Analytics",
    "text": "Analytics",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#machine-learning",
    "href": "w01/index.html#machine-learning",
    "title": "Week 1: Course Overview",
    "section": "Machine Learning",
    "text": "Machine Learning",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#governance",
    "href": "w01/index.html#governance",
    "title": "Week 1: Course Overview",
    "section": "Governance",
    "text": "Governance",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#the-terminal",
    "href": "w01/index.html#the-terminal",
    "title": "Week 1: Course Overview",
    "section": "The Terminal",
    "text": "The Terminal\n\n\n\n\nTerminal access was THE ONLY way to do programming\nNo GUIs! No Spyder, Jupyter, RStudio, etc.\nCoding is still more powerful than graphical interfaces for complex jobs\nCoding makes work repeatable",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#bash",
    "href": "w01/index.html#bash",
    "title": "Week 1: Course Overview",
    "section": "BASH",
    "text": "BASH\n\n\n\n\nCreated in 1989 by Brian Fox: ‚ÄúBourne-Again Shell‚Äù\nBrian Fox also built the first online interactive banking software\nBASH is a command processor\nConnection between you and the machine language and hardware",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#the-prompt",
    "href": "w01/index.html#the-prompt",
    "title": "Week 1: Course Overview",
    "section": "The Prompt",
    "text": "The Prompt\nusername@hostname:current_directory $\nWhat do we learn from the prompt?\n\nWho you are - username\nThe machine where your code is running - hostname\nThe directory where your code is running - current_directory\nThe shell type - $ - this symbol means BA$H",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#syntax",
    "href": "w01/index.html#syntax",
    "title": "Week 1: Course Overview",
    "section": "Syntax",
    "text": "Syntax\nCOMMAND -F --FLAG\n\nCOMMAND is the program, everything after that = arguments\nF is a single letter flag, FLAG is a single word or words connected by dashes. A space breaks things into a new argument.\nSometimes argument has single letter and long form versions (e.g.¬†F and FLAG)\n\nCOMMAND -F --FILE file1\n\nHere we pass a text argument \"file1\" as the value for the FILE flag\n-h flag is usually to get help. You can also run the man command and pass the name of the program as the argument to get the help page.\n\nLet‚Äôs try basic commands:\n\ndate to get the current date\nwhoami to get your user name\necho \"Hello World\" to print to the console",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#examining-files",
    "href": "w01/index.html#examining-files",
    "title": "Week 1: Course Overview",
    "section": "Examining Files",
    "text": "Examining Files\n\nFind out your Present Working Directory pwd\nExamine the contents of files and folders using ls\nMake new files from scratch using touch\nGlob: ‚ÄúMini-language‚Äù for selecting files with wildcards\n\n\\* for wild card any number of characters\n\\? for wild card for a single character\n[] for one of many character options\n! for exclusion\n[:alpha:], [:alnum:], [:digit:], [:lower:], [:upper:]\n\n\nReference material: Shell Lesson 1,2,4,5",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#navigating-directories",
    "href": "w01/index.html#navigating-directories",
    "title": "Week 1: Course Overview",
    "section": "Navigating Directories",
    "text": "Navigating Directories\n\nKnowing where your terminal is executing code ensures you are working with the right inputs/outputs.\nUse pwd to determine the Present Working Directory.\nChange to a folder called ‚Äúgit-repo‚Äù with cd git-repo.\n. refers to the current directory, such as ./git-repo\n.. can be used to move up one level (cd ..), and can be combined to move up multiple levels (cd ../../my_folder)\n/ is the root of the filesystem: contains core folders (system, users)\n~ is the home directory. Move to folders referenced relative to this path by including it at the start of your path, for example ~/projects.\nTo visualize the structure of your working directory, use tree\n\nReference link",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#interacting-with-files",
    "href": "w01/index.html#interacting-with-files",
    "title": "Week 1: Course Overview",
    "section": "Interacting with Files",
    "text": "Interacting with Files\nNow that we know how to navigate through directories, we need commands for interacting with files‚Ä¶\n\nmv to move files from one location to another\n\nCan use glob here - ?, *, [], ‚Ä¶\n\ncp to copy files instead of moving\n\nCan use glob here - ?, *, [], ‚Ä¶\n\nmkdir to make a directory\nrm to remove files\nrmdir to remove directories\nrm -rf to blast everything! WARNING!!! DO NOT USE UNLESS YOU KNOW WHAT YOU ARE DOING",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#using-bash-for-data-exploration",
    "href": "w01/index.html#using-bash-for-data-exploration",
    "title": "Week 1: Course Overview",
    "section": "Using BASH for Data Exploration",
    "text": "Using BASH for Data Exploration\n\nhead FILENAME / tail FILENAME - glimpsing the first / last few rows of data\nmore FILENAME / less FILENAME - viewing the data with basic up / (up & down) controls\ncat FILENAME - print entire file contents into terminal\nvim FILENAME - open (or edit!) the file in vim editor\ngrep FILENAME - search for lines within a file that match a regex expression\nwc FILENAME - count the number of lines (-l flag) or number of words (-w flag)\n\nReference material: Text Lesson 8,9,15,16",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#pipes-and-arrows",
    "href": "w01/index.html#pipes-and-arrows",
    "title": "Week 1: Course Overview",
    "section": "Pipes and Arrows",
    "text": "Pipes and Arrows\n\n| sends the stdout to another command (is the most powerful symbol in BASH!)\n&gt; sends stdout to a file and overwrites anything that was there before\n&gt;&gt; appends the stdout to the end of a file (or starts a new file from scratch if one does not exist yet)\n&lt; sends stdin into the command on the left\n\nReference material: Text Lesson 1,2,3,4,5",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#alias-and-user-files",
    "href": "w01/index.html#alias-and-user-files",
    "title": "Week 1: Course Overview",
    "section": "Alias and User Files",
    "text": "Alias and User Files\n\n/.bashrc is where your shell settings are located\nHow many processes? whoami | xargs ps -u | wc -l\nHard to remember full command! Let‚Äôs make an alias\nGeneral syntax:\nalias alias_name=\"command_to_run\"\nFor our case:\nalias nproc=\"whoami | xargs ps -u | wc -l\"\nNow we need to put this alias into the .bashrc\nalias nproc=\"whoami | xargs ps -u | wc -l\" &gt;&gt; ~/.bashrc\nYour commands get saved in ~/.bash_history",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#process-management",
    "href": "w01/index.html#process-management",
    "title": "Week 1: Course Overview",
    "section": "Process Management",
    "text": "Process Management\n\nUse ps to see your running processes\nUse top or even better htop to see all running processes\nInstall htop via sudo yum install htop -y\nTo kill a broken process: first find the process ID (PID)\nThen use kill [PID NUM] to ‚Äúask‚Äù the process to terminate. If things get really bad: kill -9 [PID NUM]\nTo kill a command in the terminal window it is running in, try using Ctrl + C or Ctrl + /\nRun cat on its own to let it stay open. Now open a new terminal to examine the processes and find the cat process.\n\nReference material: Text Lesson 1,2,3,7,9,10",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/index.html#try-playing-a-linux-game",
    "href": "w01/index.html#try-playing-a-linux-game",
    "title": "Week 1: Course Overview",
    "section": "Try playing a Linux game!",
    "text": "Try playing a Linux game!\nBash crawl is a game to help you practice your navigation and file access skills. Click on the binder link in this repo to launch a jupyter lab session and explore!",
    "crumbs": [
      "Week 1: Aug 28"
    ]
  },
  {
    "objectID": "w01/slides.html#agenda-for-todays-session",
    "href": "w01/slides.html#agenda-for-todays-session",
    "title": "Week 1: Course Overview",
    "section": "Agenda for today‚Äôs session",
    "text": "Agenda for today‚Äôs session\n\nCourse and syllabus overview\nBig Data Concepts\n\nDefinition\nChallenges\nApproaches\n\nData Engineering\nIntroduction to bash\n\nLab: Linux command line"
  },
  {
    "objectID": "w01/slides.html#bookmark-these-links",
    "href": "w01/slides.html#bookmark-these-links",
    "title": "Week 1: Course Overview",
    "section": "Bookmark these links!",
    "text": "Bookmark these links!\n\nCourse website: https://gu-dsan.github.io/6000-fall-2025/\nGitHub Organization for your deliverables: https://github.com/gu-dsan/\nGitHub Classroom: https://classroom.github.com/classrooms/34950344-georgetown-university-dsan6000-big-data-and-cloud-computing\nSlack Workspace: DSAN6000 Fall 2025 - https://dsan6000fall2025.slack.com\n\nJoin link: https://join.slack.com/t/dsan6000fall2025/shared_invite/zt-3b22qhque-GagQykwYYNiEzli9UXJn4w\n\nInstructors email: dsan-Fall-2025@georgetown.edu\nCanvas: https://georgetown.instructure.com/courses/TBA-2025\n\n\n\n\n\n\n\nThese are also pinned on the Slack main channel"
  },
  {
    "objectID": "w01/slides.html#instructional-team---professors",
    "href": "w01/slides.html#instructional-team---professors",
    "title": "Week 1: Course Overview",
    "section": "Instructional Team - Professors",
    "text": "Instructional Team - Professors\n\nAmit Arora, aa1603@georgetown.edu\nJeff Jacobs, jj1088@georgetown.edu"
  },
  {
    "objectID": "w01/slides.html#amit-arora-aa1603georgetown.edu",
    "href": "w01/slides.html#amit-arora-aa1603georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Amit Arora, aa1603@georgetown.edu",
    "text": "Amit Arora, aa1603@georgetown.edu\n\n\n\nPrincipal Solutions Architect - AI/ML at AWS\nAdjunct Professor at Georgetown University\nMultiple patents in telecommunications and applications of ML in telecommunications\n\nFun Facts\n\n\n\n\nI am a self-published author https://blueberriesinmysalad.com/\nMy book ‚ÄúBlueberries in my salad: my forever journey towards fitness & strength‚Äù is written as code in R and Markdown\nI love to read books about health and human performance, productivity, philosophy and Mathematics for ML. My reading list is online!"
  },
  {
    "objectID": "w01/slides.html#jeff-jacobs-jj1088georgetown.edu",
    "href": "w01/slides.html#jeff-jacobs-jj1088georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Jeff Jacobs, jj1088@georgetown.edu",
    "text": "Jeff Jacobs, jj1088@georgetown.edu\n\n\n\nFull-time Professor at Georgetown (DSAN and Public Policy)\nBackground in Computational Social Science (Comp Sci MS ‚Üí Political Economy PhD ‚Üí Labor Econ Postdoc)\n\nFun Facts\n\nUsed Apache Airflow daily for PhD projects! (Example)\n\n\n\n\n\nServer admin for lab server ‚Üí lab AWS account at Columbia (2015-2023) ‚Üí new DSAN server (!) (2025-)\nPassion project 1: Code for Palestine (2015-2022) ‚Üí YouthCode-Gaza (2023) ‚Üí Ukraine Ministry of Digital Transformation (2024)\nPassion projects 2+3 [ü§ì]: Sample-based music production, web app frameworks\nSleep disorder means lots of reading ‚Äì mainly history! ‚Äì at night\nAlso teaching PPOL6805 / DSAN 6750: GIS for Spatial Data Science this semester"
  },
  {
    "objectID": "w01/slides.html#instructional-team---teaching-assistants",
    "href": "w01/slides.html#instructional-team---teaching-assistants",
    "title": "Week 1: Course Overview",
    "section": "Instructional Team - Teaching Assistants",
    "text": "Instructional Team - Teaching Assistants\n\nBinhui Chen, bc928@georgetown.edu\nPranav Sudhir Patil, pp755@georgetown.edu\nOfure Udabor, au195@georgetown.edu\nYifei Wu, yw924@georgetown.edu\nNaomi Yamaguchi, ny159@georgetown.edu\nLeqi Ying, ly290@georgetown.edu\nXinyue (Monica) Zhang, xz646@georgetown.edu"
  },
  {
    "objectID": "w01/slides.html#binhui-chen-bc928georgetown.edu",
    "href": "w01/slides.html#binhui-chen-bc928georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Binhui Chen, bc928@georgetown.edu",
    "text": "Binhui Chen, bc928@georgetown.edu\n(Lead TA for the course!)"
  },
  {
    "objectID": "w01/slides.html#pranav-sudhir-patil-pp755georgetown.edu",
    "href": "w01/slides.html#pranav-sudhir-patil-pp755georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Pranav Sudhir Patil, pp755@georgetown.edu",
    "text": "Pranav Sudhir Patil, pp755@georgetown.edu"
  },
  {
    "objectID": "w01/slides.html#ofure-udabor-au195georgetown.edu",
    "href": "w01/slides.html#ofure-udabor-au195georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Ofure Udabor, au195@georgetown.edu",
    "text": "Ofure Udabor, au195@georgetown.edu"
  },
  {
    "objectID": "w01/slides.html#yifei-wu-yw924georgetown.edu",
    "href": "w01/slides.html#yifei-wu-yw924georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Yifei Wu, yw924@georgetown.edu",
    "text": "Yifei Wu, yw924@georgetown.edu"
  },
  {
    "objectID": "w01/slides.html#naomi-yamaguchi-ny159georgetown.edu",
    "href": "w01/slides.html#naomi-yamaguchi-ny159georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Naomi Yamaguchi, ny159@georgetown.edu",
    "text": "Naomi Yamaguchi, ny159@georgetown.edu"
  },
  {
    "objectID": "w01/slides.html#leqi-ying-ly290georgetown.edu",
    "href": "w01/slides.html#leqi-ying-ly290georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Leqi Ying, ly290@georgetown.edu",
    "text": "Leqi Ying, ly290@georgetown.edu"
  },
  {
    "objectID": "w01/slides.html#xinyue-monica-zhang-xz646georgetown.edu",
    "href": "w01/slides.html#xinyue-monica-zhang-xz646georgetown.edu",
    "title": "Week 1: Course Overview",
    "section": "Xinyue (Monica) Zhang, xz646@georgetown.edu",
    "text": "Xinyue (Monica) Zhang, xz646@georgetown.edu"
  },
  {
    "objectID": "w01/slides.html#course-description",
    "href": "w01/slides.html#course-description",
    "title": "Week 1: Course Overview",
    "section": "Course Description",
    "text": "Course Description\n\nData is everywhere! Many times, it‚Äôs just too big to work with traditional tools. This is a hands-on, practical workshop style course about using cloud computing resources to do analysis and manipulation of datasets that are too large to fit on a single machine and/or analyzed with traditional tools. The course will focus on Spark, MapReduce, the Hadoop Ecosystem and other tools.\nYou will understand how to acquire and/or ingest the data, and then massage, clean, transform, analyze, and model it within the context of big data analytics. You will be able to think more programmatically and logically about your big data needs, tools and issues.\n\nAlways refer to the syllabus and calendar in the course website for class policies."
  },
  {
    "objectID": "w01/slides.html#learning-objectives",
    "href": "w01/slides.html#learning-objectives",
    "title": "Week 1: Course Overview",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nSetup, operate and manage big data tools and cloud infrastructure, including Spark, DuckDB, Polars, Athena, Snowflake, and orchestration tools like Airflow on Amazon Web Services\nUse ancillary tools that support big data processing, including git and the Linux command line\nExecute a big data analytics exercise from start to finish: ingest, wrangle, clean, analyze, store, and present\nDevelop strategies to break down large problems and datasets into manageable pieces\nIdentify broad spectrum resources and documentation to remain current with big data tools and developments\nCommunicate and interpret the big data analytics results through written and verbal methods"
  },
  {
    "objectID": "w01/slides.html#evaluation",
    "href": "w01/slides.html#evaluation",
    "title": "Week 1: Course Overview",
    "section": "Evaluation",
    "text": "Evaluation\n\nGroup project : 40%\nAssignments : 30%\nLab completions : 20%\nQuizzes : 10%"
  },
  {
    "objectID": "w01/slides.html#course-materials",
    "href": "w01/slides.html#course-materials",
    "title": "Week 1: Course Overview",
    "section": "Course Materials",
    "text": "Course Materials\n\nSlides/labs/assignment on Website/GitHub\nQuizzes and readings in Canvas"
  },
  {
    "objectID": "w01/slides.html#communication",
    "href": "w01/slides.html#communication",
    "title": "Week 1: Course Overview",
    "section": "Communication",
    "text": "Communication\n\nSlack is the primary form of communication\nInstructional team email: dsan-Fall-2025@georgetown.edu"
  },
  {
    "objectID": "w01/slides.html#slack-rules",
    "href": "w01/slides.html#slack-rules",
    "title": "Week 1: Course Overview",
    "section": "Slack rules",
    "text": "Slack rules\n\nPost any question/comment about the course, assignments or any technical issue.\nDMs are to be used sparingly\nYou may not DM multiple people in the instructional team at the same time for the same issue\nKeep an eye on the questions posted in Slack. Use the search function. It‚Äôs very possible that we have already answered a questions\nYou may DM us back only if we DM you first on a given issue\nLab/assignment/project questions will only be answered up to 6 hours before something is due (i.e.¬†6pm on Mondays)"
  },
  {
    "objectID": "w01/slides.html#midterm-project-new",
    "href": "w01/slides.html#midterm-project-new",
    "title": "Week 1: Course Overview",
    "section": "Midterm Project (NEW!)",
    "text": "Midterm Project (NEW!)\n\nIndividual assignment (not team-based)\nTiming: Around Week 5-6\nWeight: Equivalent to 2 homework assignments\nFormat:\n\nWe provide the dataset and problem statement\nYou apply big data tools and techniques learned in class\nEnd-to-end data pipeline implementation\n\nDetails: TBD (will be announced in Week 4)"
  },
  {
    "objectID": "w01/slides.html#final-project",
    "href": "w01/slides.html#final-project",
    "title": "Week 1: Course Overview",
    "section": "Final Project",
    "text": "Final Project\n\nGroups of 3-4 students\nUse an archive of Reddit data, augmented with external data\nExploratory analysis\nNLP\nMachine Learning\nWriteup\n\nData sourcing and ingesting\nExploratory analysis\nModeling\nChallenges and Learnings\nConclusions\nFuture work"
  },
  {
    "objectID": "w01/slides.html#in-one-minute-of-time-2018",
    "href": "w01/slides.html#in-one-minute-of-time-2018",
    "title": "Week 1: Course Overview",
    "section": "In one minute of time (2018)",
    "text": "In one minute of time (2018)"
  },
  {
    "objectID": "w01/slides.html#in-one-minute-of-time-2019",
    "href": "w01/slides.html#in-one-minute-of-time-2019",
    "title": "Week 1: Course Overview",
    "section": "In one minute of time (2019)",
    "text": "In one minute of time (2019)"
  },
  {
    "objectID": "w01/slides.html#in-one-minute-of-time-2020",
    "href": "w01/slides.html#in-one-minute-of-time-2020",
    "title": "Week 1: Course Overview",
    "section": "In one minute of time (2020)",
    "text": "In one minute of time (2020)"
  },
  {
    "objectID": "w01/slides.html#in-one-minute-of-time-2021",
    "href": "w01/slides.html#in-one-minute-of-time-2021",
    "title": "Week 1: Course Overview",
    "section": "In one minute of time (2021)",
    "text": "In one minute of time (2021)"
  },
  {
    "objectID": "w01/slides.html#in-one-minute-of-time-2025",
    "href": "w01/slides.html#in-one-minute-of-time-2025",
    "title": "Week 1: Course Overview",
    "section": "In one minute of time (2025)",
    "text": "In one minute of time (2025)\nEvery 60 seconds in 2025:\n\nChatGPT serves millions of requests (exact numbers proprietary)\n500 hours of video uploaded to YouTube\n1.04 million Slack messages sent\n362,000 hours watched on Netflix\n5.9-11.4 million Google searches\n$443,000 spent on Amazon\nAI-generated images created at massive scale (metrics not publicly available)\n347,200 posts on X (formerly Twitter)\n231-250 million emails sent"
  },
  {
    "objectID": "w01/slides.html#a-lot-of-it-is-hapenning-online.",
    "href": "w01/slides.html#a-lot-of-it-is-hapenning-online.",
    "title": "Week 1: Course Overview",
    "section": "A lot of it is hapenning online.",
    "text": "A lot of it is hapenning online.\n\n\nWe can record every:\n\nclick\nad impression\nbilling event\nvideo interaction\nserver request\ntransaction\nnetwork message\nfault\n‚Ä¶"
  },
  {
    "objectID": "w01/slides.html#it-can-also-be-user-generated-content",
    "href": "w01/slides.html#it-can-also-be-user-generated-content",
    "title": "Week 1: Course Overview",
    "section": "It can also be user-generated content:",
    "text": "It can also be user-generated content:\n\n\n\n\nInstagram posts & Reels\nX (Twitter) posts & Threads\nTikTok videos\nYouTube Shorts\nReddit discussions\nDiscord conversations\nAI-generated content (text, images, code)\n‚Ä¶"
  },
  {
    "objectID": "w01/slides.html#but-health-and-scientific-computing-create-a-lot-too",
    "href": "w01/slides.html#but-health-and-scientific-computing-create-a-lot-too",
    "title": "Week 1: Course Overview",
    "section": "But health and scientific computing create a lot too!",
    "text": "But health and scientific computing create a lot too!"
  },
  {
    "objectID": "w01/slides.html#theres-lots-of-graph-data-too",
    "href": "w01/slides.html#theres-lots-of-graph-data-too",
    "title": "Week 1: Course Overview",
    "section": "There‚Äôs lots of graph data too",
    "text": "There‚Äôs lots of graph data too\n\n\n\nMany interesting datasets have a graph structure:\n\nSocial networks\nGoogle‚Äôs knowledge graph\nTelecom networks\nComputer networks\nRoad networks\nCollaboration/relationships\n\nSome of these are HUGE"
  },
  {
    "objectID": "w01/slides.html#apache-web-server-log-files",
    "href": "w01/slides.html#apache-web-server-log-files",
    "title": "Week 1: Course Overview",
    "section": "Apache (web server) log files",
    "text": "Apache (web server) log files"
  },
  {
    "objectID": "w01/slides.html#system-log-files",
    "href": "w01/slides.html#system-log-files",
    "title": "Week 1: Course Overview",
    "section": "System log files",
    "text": "System log files"
  },
  {
    "objectID": "w01/slides.html#internet-of-things-iot-in-2025",
    "href": "w01/slides.html#internet-of-things-iot-in-2025",
    "title": "Week 1: Course Overview",
    "section": "Internet of Things (IoT) in 2025",
    "text": "Internet of Things (IoT) in 2025\n75 billion connected devices generating data:\n\n\n\n\nSmart home devices (Alexa, Google Home, Apple HomePod)\nWearables (Apple Watch, Fitbit, Oura rings)\nConnected vehicles & self-driving cars\nIndustrial IoT sensors\n\n\n\n\nSmart city infrastructure\nMedical devices & remote patient monitoring"
  },
  {
    "objectID": "w01/slides.html#smartphone-location-data",
    "href": "w01/slides.html#smartphone-location-data",
    "title": "Week 1: Course Overview",
    "section": "Smartphone Location Data",
    "text": "Smartphone Location Data"
  },
  {
    "objectID": "w01/slides.html#where-else",
    "href": "w01/slides.html#where-else",
    "title": "Week 1: Course Overview",
    "section": "Where else?",
    "text": "Where else?\n\nThe Internet\nTransactions\nDatabases\nExcel\nPDF Files\nAnything digital (music, movies, apps)\nSome old floppy disk lying around the house"
  },
  {
    "objectID": "w01/slides.html#typical-real-world-scenarios-in-2025",
    "href": "w01/slides.html#typical-real-world-scenarios-in-2025",
    "title": "Week 1: Course Overview",
    "section": "Typical Real-World Scenarios in 2025",
    "text": "Typical Real-World Scenarios in 2025\nScenario 1: Traditional Big Data\nYou have a laptop with 16GB of RAM and a 256GB SSD. You are given a 1TB dataset in text files. What do you do?\nScenario 2: AI/ML Pipeline\nYour company wants to build a RAG system using 10TB of internal documents. You need sub-second query response times. How do you architect this?\nScenario 3: Real-Time Analytics\nYou need to process 1 million events/second from IoT devices and provide real-time dashboards with &lt;1s latency. What‚Äôs your stack?"
  },
  {
    "objectID": "w01/slides.html#lets-discuss",
    "href": "w01/slides.html#lets-discuss",
    "title": "Week 1: Course Overview",
    "section": "Let‚Äôs discuss!",
    "text": "Let‚Äôs discuss!\n\nExponential data growth"
  },
  {
    "objectID": "w01/slides.html#big-data-definitions",
    "href": "w01/slides.html#big-data-definitions",
    "title": "Week 1: Course Overview",
    "section": "Big Data Definitions",
    "text": "Big Data Definitions\nWikipedia\n‚ÄúA collection of datasets so large and complex that it becomes difficult to process using traditional tools and applications. Big Data technologies describe a new generation of technologies and architectures designed to economically extract value from very large volumes of a wide variety of data, by enabling high-velocity capture, discover and/or analysis‚Äù\nO‚ÄôReilly\n‚ÄúBig data is when the size of the data itself becomes part of the problem‚Äù"
  },
  {
    "objectID": "w01/slides.html#frameworks-for-thinking-about-big-data",
    "href": "w01/slides.html#frameworks-for-thinking-about-big-data",
    "title": "Week 1: Course Overview",
    "section": "Frameworks for Thinking About Big Data",
    "text": "Frameworks for Thinking About Big Data\nIBM (The 3 V‚Äôs)\n\nVolume (Gigabytes ‚Üí Exabytes ‚Üí Zettabytes)\nVelocity (Batch ‚Üí Streaming ‚Üí Real-time AI inference)\nVariety (Structured, Unstructured, Embeddings)\n\nAdditional V‚Äôs for 2025\n\n\n\nVariability\nVeracity\nVisualization\n\n\n\nValue\nVectors (embeddings for AI/ML)\nVersatility (multi-modal data)"
  },
  {
    "objectID": "w01/slides.html#data-size",
    "href": "w01/slides.html#data-size",
    "title": "Week 1: Course Overview",
    "section": "Data ‚ÄúSize‚Äù",
    "text": "Data ‚ÄúSize‚Äù\n\\[\n\\text{``Size''} = f(\\text{Processing Ability}, \\text{Storage Space})\n\\]\n\nCan you analyze/process your data on a single machine?\nCan you store (or is it stored) on a single machine?\nCan you serve it fast enough for real-time AI applications?\n\nIf any of of the answers is no then you have a big-ish data problem!"
  },
  {
    "objectID": "w01/slides.html#the-new-data-landscape-2025",
    "href": "w01/slides.html#the-new-data-landscape-2025",
    "title": "Week 1: Course Overview",
    "section": "The New Data Landscape (2025)",
    "text": "The New Data Landscape (2025)\nTraining Foundation Models\n\nGPT-4: Trained on about 13 trillion tokens\nLlama 3: 15 trillion tokens\nGoogle Gemini: Multi-modal training (text, images, video)\nEach iteration requires petabytes of curated data\n\nData Requirements Have Exploded\n\n2020: BERT trained on 3.3 billion words\n2023: GPT-4 trained on ~13 trillion tokens\n2024: Llama 3 trained on 15+ trillion tokens"
  },
  {
    "objectID": "w01/slides.html#big-data-infrastructure-data-lakes-warehouses",
    "href": "w01/slides.html#big-data-infrastructure-data-lakes-warehouses",
    "title": "Week 1: Course Overview",
    "section": "Big Data Infrastructure: Data Lakes, Warehouses",
    "text": "Big Data Infrastructure: Data Lakes, Warehouses\nTraditional Use Cases:\n\nBusiness intelligence\nAnalytics & reporting\nHistorical data storage\n\nModern AI Use Cases:\n\nTraining data repositories\nVector embeddings storage\nRAG (Retrieval-Augmented Generation) context\nFine-tuning datasets\nEvaluation & benchmark data"
  },
  {
    "objectID": "w01/slides.html#rag-and-context-engineering-the-new-data-pipeline",
    "href": "w01/slides.html#rag-and-context-engineering-the-new-data-pipeline",
    "title": "Week 1: Course Overview",
    "section": "RAG and Context Engineering: The New Data Pipeline",
    "text": "RAG and Context Engineering: The New Data Pipeline\n\n\n\n\n\n\n\nG\n\n\n\nraw\n\nRaw Data\n\n\n\nlake\n\nData Lake\n\n\n\nraw-&gt;lake\n\n\n\n\n\nproc\n\nProcessing\n\n\n\nlake-&gt;proc\n\n\n\n\n\nvec\n\nVector DB\n\n\n\nproc-&gt;vec\n\n\n\n\n\ncontext\n\nLLM Context\n\n\n\nvec-&gt;context\n\n\n\n\n\n\n\n\n\n\nKey Components:\n\nData Lakes (S3, Azure Data Lake): Store massive unstructured data\nData Warehouses (Snowflake, BigQuery): Structured data for context\nVector Databases (Pinecone, Weaviate, Qdrant): Semantic search\nEmbedding Models: Convert data to vectors\nOrchestration (Airflow, Prefect): Manage the pipeline"
  },
  {
    "objectID": "w01/slides.html#mcp-servers-agentic-ai",
    "href": "w01/slides.html#mcp-servers-agentic-ai",
    "title": "Week 1: Course Overview",
    "section": "MCP Servers & Agentic AI",
    "text": "MCP Servers & Agentic AI\nModel Context Protocol (MCP)\n\nOpen protocol for connecting AI assistants to data sources\nStandardized way to expose tools and data to LLMs\nEnables ‚Äúagentic‚Äù behavior - AI that can act autonomously\n\nMCP in Production\n\n\n\n\n\n\n\nG\n\n\n\nware\n\nData Warehouse\n\n\n\nmcp\n\nMCP Server\n\n\n\nware-&gt;mcp\n\n\n\n\n\nagent\n\nAI Agent\n\n\n\nmcp-&gt;agent\n\n\n\n\n\naction\n\nAction\n\n\n\nagent-&gt;action\n\n\n\n\n\n\n\n\n\n\nExamples:\n\nAI agents querying Snowflake for real-time analytics\nAutonomous systems updating data lakes based on predictions\nMulti-agent systems coordinating through shared data contexts"
  },
  {
    "objectID": "w01/slides.html#data-quality-for-ai",
    "href": "w01/slides.html#data-quality-for-ai",
    "title": "Week 1: Course Overview",
    "section": "Data Quality for AI",
    "text": "Data Quality for AI\n(Why Data Quality Matters More Than Ever)\nGarbage In, Garbage Out - Amplified:\n\nBad training data ‚Üí Biased models\nIncorrect RAG data ‚Üí Hallucinations\nPoor data governance ‚Üí Compliance issues\n\nData Quality Challenges in 2025\n\nScale: Validating trillions of tokens\nDiversity: Multi-modal, multi-lingual data\nVelocity: Real-time data for online learning\nVeracity: Detecting AI-generated synthetic data"
  },
  {
    "objectID": "w01/slides.html#real-world-big-data-ai-examples",
    "href": "w01/slides.html#real-world-big-data-ai-examples",
    "title": "Week 1: Course Overview",
    "section": "Real-World Big Data / AI Examples",
    "text": "Real-World Big Data / AI Examples\nNetflix\n\nData Scale: 260+ million subscribers generating 100+ billion events/day\nAI Use: Personalization, content recommendations, thumbnail generation\nStack: S3 ‚Üí Spark ‚Üí Iceberg ‚Üí ML models ‚Üí Real-time serving\n\nUber\n\nData Scale: 35+ million trips per day, petabytes of location data\nAI Use: ETA prediction, surge pricing, driver-rider matching\nStack: Kafka ‚Üí Spark Streaming ‚Üí Feature Store ‚Üí ML Platform\n\nOpenAI\n\nData Scale: Trillions of tokens for training, millions of queries/day\nAI Use: GPT models, DALL-E, embeddings\nStack: Distributed training ‚Üí Vector DBs ‚Üí Inference clusters"
  },
  {
    "objectID": "w01/slides.html#emerging-trends-2025-2027",
    "href": "w01/slides.html#emerging-trends-2025-2027",
    "title": "Week 1: Course Overview",
    "section": "Emerging Trends (2025-2027)",
    "text": "Emerging Trends (2025-2027)\nUnified Platforms:\n\nData lakes becoming ‚ÄúAI lakes‚Äù\nIntegrated vector + relational databases\nOne-stop shops for data + AI (Databricks, Snowflake Cortex)\n\nEdge Computing + AI:\n\nProcessing at the data source\nFederated learning across devices\n5G enabling real-time edge AI\n\nSynthetic Data:\n\nAI generating training data for AI\nPrivacy-preserving synthetic datasets"
  },
  {
    "objectID": "w01/slides.html#relative-data-sizes",
    "href": "w01/slides.html#relative-data-sizes",
    "title": "Week 1: Course Overview",
    "section": "Relative Data Sizes",
    "text": "Relative Data Sizes\n¬†\n\n\n\n\nCan be processed on single machine?\nNo\nMedium(Parallel Processing)\nBig!Parallel + Distributed Processing\n\n\nYes\nSmall(Your Laptop)\nMedium(Data Streaming)\n\n\n\n\nYes\nNo\n\n\n\n\nCan be stored on single machine?"
  },
  {
    "objectID": "w01/slides.html#what-youll-learn-in-this-course",
    "href": "w01/slides.html#what-youll-learn-in-this-course",
    "title": "Week 1: Course Overview",
    "section": "What You‚Äôll Learn in This Course",
    "text": "What You‚Äôll Learn in This Course\nModern Big Data Stack (2025)\n\n\n\nQuery Engines:\n\nDuckDB - In-process analytical database\nPolars - Lightning-fast DataFrame library\n\nSpark - Distributed processing at scale\n\nData Warehouses & Lakes:\n\nSnowflake - Cloud-native data warehouse\nAthena - Serverless SQL on S3\nIceberg - Open table format\n\n\n\nAI/ML Integration:\n\nVector databases for embeddings\nRAG implementation patterns\nStreaming with Spark Structured Streaming\n\nOrchestration:\n\nAirflow for pipeline management\nServerless with AWS Lambda"
  },
  {
    "objectID": "w01/slides.html#data-types",
    "href": "w01/slides.html#data-types",
    "title": "Week 1: Course Overview",
    "section": "Data Types",
    "text": "Data Types\n\nStructured\nUnstructured\nNatural language\nMachine-generated\nGraph-based\nAudio, video, and images\nStreaming"
  },
  {
    "objectID": "w01/slides.html#big-data-vs.-small-data-i",
    "href": "w01/slides.html#big-data-vs.-small-data-i",
    "title": "Week 1: Course Overview",
    "section": "Big Data vs.¬†Small Data I",
    "text": "Big Data vs.¬†Small Data I\n\n\n\n\n\n\n\n\n\nSmall Data is usually‚Ä¶\nOn the other hand, Big Data‚Ä¶\n\n\n\n\nGoals\ngathered for a specific goal\nmay have a goal in mind when it‚Äôs first started, but things can evolve or take unexpected directions\n\n\nLocation\nin one place, and often in a single computer file\ncan be in multiple files in multiple servers on computers in different geographic locations\n\n\nStructure/Contents\nhighly structured like an Excel spreadsheet, and it‚Äôs got rows and columns of data\ncan be unstructured, it can have many formats in files involved across disciplines, and may link to other resources\n\n\nPreparation\nprepared by the end user for their own purposes\nis often prepared by one group of people, analyzed by a second group of people, and then used by a third group of people, and they may have different purposes, and they may have different disciplines"
  },
  {
    "objectID": "w01/slides.html#big-data-vs.-small-data-ii",
    "href": "w01/slides.html#big-data-vs.-small-data-ii",
    "title": "Week 1: Course Overview",
    "section": "Big Data vs.¬†Small Data II",
    "text": "Big Data vs.¬†Small Data II\n\n\n\n\n\n\n\n\n\nSmall Data is usually‚Ä¶\nOn the other hand, Big Data‚Ä¶\n\n\n\n\nLongevity\nkept for a specific amount of time after the project is over because there‚Äôs a clear ending point. In the academic world it‚Äôs maybe five or seven years and then you can throw it away\ncontains data that must be stored in perpetuity. Many big data projects extend into the past and future\n\n\nMeasurements\nmeasured with a single protocol using set units and it‚Äôs usually done at the same time\nis collected and measured using many sources, protocols, units, etc\n\n\nReproducibility\nbe reproduced in their entirety if something goes wrong in the process\nreplication is seldom feasible\n\n\nStakes\nif things go wrong the costs are limited, it‚Äôs not an enormous problem\ncan have high costs of failure in terms of money, time and labor\n\n\nAccess\nidentified by a location specified in a row/column\nunless it is exceptionally well designed, the organization can be inscrutable\n\n\nAnalysis\nanalyzed together, all at once\nis ordinarily analyzed in incremental steps"
  },
  {
    "objectID": "w01/slides.html#challenges-of-working-with-large-datasets",
    "href": "w01/slides.html#challenges-of-working-with-large-datasets",
    "title": "Week 1: Course Overview",
    "section": "Challenges of Working with Large Datasets",
    "text": "Challenges of Working with Large Datasets\n\n\n\nThe V\nThe Challenge\n\n\n\n\nVolume\ndata scale\n\n\nValue\ndata usefulness in decision making\n\n\nVelocity\ndata processing: batch or stream\n\n\nViscosity\ndata complexity\n\n\nVariability\ndata flow inconsistency\n\n\nVolatility\ndata durability\n\n\nViability\ndata activeness\n\n\nValidity\ndata properly understandable\n\n\nVariety\ndata heterogeneity"
  },
  {
    "objectID": "w01/slides.html#thinking-about-big-data-workflows",
    "href": "w01/slides.html#thinking-about-big-data-workflows",
    "title": "Week 1: Course Overview",
    "section": "Thinking About Big Data Workflows",
    "text": "Thinking About Big Data Workflows\nWilliam Cohen (Director, Research Engineering, Google):\n\nWorking with big data is not about‚Ä¶\n\nCode optimization\nLearning the details of today‚Äôs hardware/software (they are evolving‚Ä¶)\n\nWorking with big data is about understanding:\n\nThe cost of what you want to do\nWhat the tools that are available offer\nHow much can be accomplished with linear or nearly-linear operations\nHow to organize your computations so that they effectively use whatever‚Äôs fast\nHow to test/debug/verify with large data\n\nRecall that traditional tools like R and Python are single threaded (by default)"
  },
  {
    "objectID": "w01/slides.html#tools-at-a-glance",
    "href": "w01/slides.html#tools-at-a-glance",
    "title": "Week 1: Course Overview",
    "section": "Tools at-a-glance",
    "text": "Tools at-a-glance\n\n\nLanguages, libraries, and projects\n\nPython\n\npandas\npolars\nPySpark\nduckdb\ndask\nray\n\nApache Arrow\nApache Spark\nSQL\nApache Hadoop (briefly)\n\n\nCloud Services\n\nAmazon Web Services (AWS)\n\nAWS Sagemaker\nAmazon S3\n\nAzure\n\nAzure Blob\nAzure Machine Learning\n\n\nOther:\n\nAWS Elastic MapReduce (EMR)"
  },
  {
    "objectID": "w01/slides.html#additional-links-of-interest",
    "href": "w01/slides.html#additional-links-of-interest",
    "title": "Week 1: Course Overview",
    "section": "Additional links of interest",
    "text": "Additional links of interest\n\nMatt Turck‚Äôs Machine Learning, Artificial Intelligence & Data Landscape (MAD)\n\nArticle\nInteractive Landscape\n\nIs there life after Hadoop?\n10 Best Big Data Tools for 2023"
  },
  {
    "objectID": "w01/slides.html#data-scientist-vs.-data-engineer",
    "href": "w01/slides.html#data-scientist-vs.-data-engineer",
    "title": "Week 1: Course Overview",
    "section": "Data Scientist vs.¬†Data Engineer",
    "text": "Data Scientist vs.¬†Data Engineer\nIn this course, you‚Äôll augment your data scientist skills with data engineering skills!"
  },
  {
    "objectID": "w01/slides.html#data-engineer-responsibilities",
    "href": "w01/slides.html#data-engineer-responsibilities",
    "title": "Week 1: Course Overview",
    "section": "Data Engineer Responsibilities",
    "text": "Data Engineer Responsibilities"
  },
  {
    "objectID": "w01/slides.html#data-engineering-levels-2-and-3",
    "href": "w01/slides.html#data-engineering-levels-2-and-3",
    "title": "Week 1: Course Overview",
    "section": "Data Engineering: Levels 2 and 3",
    "text": "Data Engineering: Levels 2 and 3"
  },
  {
    "objectID": "w01/slides.html#architecture",
    "href": "w01/slides.html#architecture",
    "title": "Week 1: Course Overview",
    "section": "Architecture",
    "text": "Architecture"
  },
  {
    "objectID": "w01/slides.html#storage",
    "href": "w01/slides.html#storage",
    "title": "Week 1: Course Overview",
    "section": "Storage",
    "text": "Storage"
  },
  {
    "objectID": "w01/slides.html#source-control",
    "href": "w01/slides.html#source-control",
    "title": "Week 1: Course Overview",
    "section": "Source control",
    "text": "Source control"
  },
  {
    "objectID": "w01/slides.html#orchestration",
    "href": "w01/slides.html#orchestration",
    "title": "Week 1: Course Overview",
    "section": "Orchestration",
    "text": "Orchestration"
  },
  {
    "objectID": "w01/slides.html#processing",
    "href": "w01/slides.html#processing",
    "title": "Week 1: Course Overview",
    "section": "Processing",
    "text": "Processing"
  },
  {
    "objectID": "w01/slides.html#analytics",
    "href": "w01/slides.html#analytics",
    "title": "Week 1: Course Overview",
    "section": "Analytics",
    "text": "Analytics"
  },
  {
    "objectID": "w01/slides.html#machine-learning",
    "href": "w01/slides.html#machine-learning",
    "title": "Week 1: Course Overview",
    "section": "Machine Learning",
    "text": "Machine Learning"
  },
  {
    "objectID": "w01/slides.html#governance",
    "href": "w01/slides.html#governance",
    "title": "Week 1: Course Overview",
    "section": "Governance",
    "text": "Governance"
  },
  {
    "objectID": "w01/slides.html#the-terminal",
    "href": "w01/slides.html#the-terminal",
    "title": "Week 1: Course Overview",
    "section": "The Terminal",
    "text": "The Terminal\n\n\n\n\nTerminal access was THE ONLY way to do programming\nNo GUIs! No Spyder, Jupyter, RStudio, etc.\nCoding is still more powerful than graphical interfaces for complex jobs\nCoding makes work repeatable"
  },
  {
    "objectID": "w01/slides.html#bash",
    "href": "w01/slides.html#bash",
    "title": "Week 1: Course Overview",
    "section": "BASH",
    "text": "BASH\n\n\n\n\nCreated in 1989 by Brian Fox: ‚ÄúBourne-Again Shell‚Äù\nBrian Fox also built the first online interactive banking software\nBASH is a command processor\nConnection between you and the machine language and hardware"
  },
  {
    "objectID": "w01/slides.html#the-prompt",
    "href": "w01/slides.html#the-prompt",
    "title": "Week 1: Course Overview",
    "section": "The Prompt",
    "text": "The Prompt\nusername@hostname:current_directory $\nWhat do we learn from the prompt?\n\nWho you are - username\nThe machine where your code is running - hostname\nThe directory where your code is running - current_directory\nThe shell type - $ - this symbol means BA$H"
  },
  {
    "objectID": "w01/slides.html#syntax",
    "href": "w01/slides.html#syntax",
    "title": "Week 1: Course Overview",
    "section": "Syntax",
    "text": "Syntax\nCOMMAND -F --FLAG\n\nCOMMAND is the program, everything after that = arguments\nF is a single letter flag, FLAG is a single word or words connected by dashes. A space breaks things into a new argument.\nSometimes argument has single letter and long form versions (e.g.¬†F and FLAG)\n\nCOMMAND -F --FILE file1\n\nHere we pass a text argument \"file1\" as the value for the FILE flag\n-h flag is usually to get help. You can also run the man command and pass the name of the program as the argument to get the help page.\n\nLet‚Äôs try basic commands:\n\ndate to get the current date\nwhoami to get your user name\necho \"Hello World\" to print to the console"
  },
  {
    "objectID": "w01/slides.html#examining-files",
    "href": "w01/slides.html#examining-files",
    "title": "Week 1: Course Overview",
    "section": "Examining Files",
    "text": "Examining Files\n\nFind out your Present Working Directory pwd\nExamine the contents of files and folders using ls\nMake new files from scratch using touch\nGlob: ‚ÄúMini-language‚Äù for selecting files with wildcards\n\n\\* for wild card any number of characters\n\\? for wild card for a single character\n[] for one of many character options\n! for exclusion\n[:alpha:], [:alnum:], [:digit:], [:lower:], [:upper:]\n\n\nReference material: Shell Lesson 1,2,4,5"
  },
  {
    "objectID": "w01/slides.html#navigating-directories",
    "href": "w01/slides.html#navigating-directories",
    "title": "Week 1: Course Overview",
    "section": "Navigating Directories",
    "text": "Navigating Directories\n\nKnowing where your terminal is executing code ensures you are working with the right inputs/outputs.\nUse pwd to determine the Present Working Directory.\nChange to a folder called ‚Äúgit-repo‚Äù with cd git-repo.\n. refers to the current directory, such as ./git-repo\n.. can be used to move up one level (cd ..), and can be combined to move up multiple levels (cd ../../my_folder)\n/ is the root of the filesystem: contains core folders (system, users)\n~ is the home directory. Move to folders referenced relative to this path by including it at the start of your path, for example ~/projects.\nTo visualize the structure of your working directory, use tree\n\nReference link"
  },
  {
    "objectID": "w01/slides.html#interacting-with-files",
    "href": "w01/slides.html#interacting-with-files",
    "title": "Week 1: Course Overview",
    "section": "Interacting with Files",
    "text": "Interacting with Files\nNow that we know how to navigate through directories, we need commands for interacting with files‚Ä¶\n\nmv to move files from one location to another\n\nCan use glob here - ?, *, [], ‚Ä¶\n\ncp to copy files instead of moving\n\nCan use glob here - ?, *, [], ‚Ä¶\n\nmkdir to make a directory\nrm to remove files\nrmdir to remove directories\nrm -rf to blast everything! WARNING!!! DO NOT USE UNLESS YOU KNOW WHAT YOU ARE DOING"
  },
  {
    "objectID": "w01/slides.html#using-bash-for-data-exploration",
    "href": "w01/slides.html#using-bash-for-data-exploration",
    "title": "Week 1: Course Overview",
    "section": "Using BASH for Data Exploration",
    "text": "Using BASH for Data Exploration\n\nhead FILENAME / tail FILENAME - glimpsing the first / last few rows of data\nmore FILENAME / less FILENAME - viewing the data with basic up / (up & down) controls\ncat FILENAME - print entire file contents into terminal\nvim FILENAME - open (or edit!) the file in vim editor\ngrep FILENAME - search for lines within a file that match a regex expression\nwc FILENAME - count the number of lines (-l flag) or number of words (-w flag)\n\nReference material: Text Lesson 8,9,15,16"
  },
  {
    "objectID": "w01/slides.html#pipes-and-arrows",
    "href": "w01/slides.html#pipes-and-arrows",
    "title": "Week 1: Course Overview",
    "section": "Pipes and Arrows",
    "text": "Pipes and Arrows\n\n| sends the stdout to another command (is the most powerful symbol in BASH!)\n&gt; sends stdout to a file and overwrites anything that was there before\n&gt;&gt; appends the stdout to the end of a file (or starts a new file from scratch if one does not exist yet)\n&lt; sends stdin into the command on the left\n\nReference material: Text Lesson 1,2,3,4,5"
  },
  {
    "objectID": "w01/slides.html#alias-and-user-files",
    "href": "w01/slides.html#alias-and-user-files",
    "title": "Week 1: Course Overview",
    "section": "Alias and User Files",
    "text": "Alias and User Files\n\n/.bashrc is where your shell settings are located\nHow many processes? whoami | xargs ps -u | wc -l\nHard to remember full command! Let‚Äôs make an alias\nGeneral syntax:\nalias alias_name=\"command_to_run\"\nFor our case:\nalias nproc=\"whoami | xargs ps -u | wc -l\"\nNow we need to put this alias into the .bashrc\nalias nproc=\"whoami | xargs ps -u | wc -l\" &gt;&gt; ~/.bashrc\nYour commands get saved in ~/.bash_history"
  },
  {
    "objectID": "w01/slides.html#process-management",
    "href": "w01/slides.html#process-management",
    "title": "Week 1: Course Overview",
    "section": "Process Management",
    "text": "Process Management\n\nUse ps to see your running processes\nUse top or even better htop to see all running processes\nInstall htop via sudo yum install htop -y\nTo kill a broken process: first find the process ID (PID)\nThen use kill [PID NUM] to ‚Äúask‚Äù the process to terminate. If things get really bad: kill -9 [PID NUM]\nTo kill a command in the terminal window it is running in, try using Ctrl + C or Ctrl + /\nRun cat on its own to let it stay open. Now open a new terminal to examine the processes and find the cat process.\n\nReference material: Text Lesson 1,2,3,7,9,10"
  },
  {
    "objectID": "w01/slides.html#try-playing-a-linux-game",
    "href": "w01/slides.html#try-playing-a-linux-game",
    "title": "Week 1: Course Overview",
    "section": "Try playing a Linux game!",
    "text": "Try playing a Linux game!\nBash crawl is a game to help you practice your navigation and file access skills. Click on the binder link in this repo to launch a jupyter lab session and explore!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN 6000 Sections 01 and 02",
    "section": "",
    "text": "This webpage just serves as a hub collecting Jeff‚Äôs slides for DSAN 6000: Big Data and Cloud Computing, Fall 2025 at Georgetown University. It is not a replacement for the main course webpage!\nSection 01 of the course takes place on Wednesday from 3:30pm to 6:00pm in Walsh 394.\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nWeek 1: Course Overview\n\n\nAug 28\n\n\n\n\n\n\nWeek 2: Cloud Computing\n\n\nSep 2\n\n\n\n\n\n\nWeek 3: Parallelization Concepts\n\n\nSep 8\n\n\n\n\n\n\nWeek 4: DuckDB, Polars, File Formats\n\n\nSep 15\n\n\n\n\n\n\nWeek 5: Data Engineering\n\n\nSep 22\n\n\n\n\n\n\nWeek 6: Introduction to Spark\n\n\nSep 29\n\n\n\n\n\n\nWeek 7: Spark DataFrames and Spark SQL\n\n\nOct 6\n\n\n\n\n\n\nWeek 8: Spark ML and Streaming\n\n\nOct 20\n\n\n\n\n\n\nWeek 9: Apache Iceberg, Table Formats\n\n\nOct 27\n\n\n\n\n\n\nWeek 10: Data Pipeline Orchestration with Airflow\n\n\nNov 3\n\n\n\n\n\n\nWeek 11: Vector Databases, RAG\n\n\nNov 10\n\n\n\n\n\n\nWeek 12: Modern Data Stack, Governance\n\n\nNov 17\n\n\n\n\n\n\nWeek 13: Serverless and Container Orchestration\n\n\nNov 24\n\n\n\n\n\n\nWeek 14: Final Topics, Review\n\n\nDec 1\n\n\n\n\n\n\nWeek 15: Final Project Presentations\n\n\nDec 8\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "w02/slides.html#jeffs-favorite-big-data-definition",
    "href": "w02/slides.html#jeffs-favorite-big-data-definition",
    "title": "Week 2: Cloud Computing",
    "section": "Jeff‚Äôs Favorite ‚ÄúBig Data‚Äù Definition",
    "text": "Jeff‚Äôs Favorite ‚ÄúBig Data‚Äù Definition\n¬†\n\nBig data is when the size of the data itself becomes part of the problem (Loukides 2010)\n\n\nIn other words: Your problem becomes a ‚Äúbig data problem‚Äù when you hit one or both of the walls!"
  },
  {
    "objectID": "w02/slides.html#you-vs.-your-opps",
    "href": "w02/slides.html#you-vs.-your-opps",
    "title": "Week 2: Cloud Computing",
    "section": "You vs.¬†Your Opps",
    "text": "You vs.¬†Your Opps"
  },
  {
    "objectID": "w02/slides.html#so-you-find-yourself-smashed-against-a-wall",
    "href": "w02/slides.html#so-you-find-yourself-smashed-against-a-wall",
    "title": "Week 2: Cloud Computing",
    "section": "So you find yourself smashed against a wall‚Ä¶",
    "text": "So you find yourself smashed against a wall‚Ä¶\n(Some of yall would fold in this scenario)"
  },
  {
    "objectID": "w02/slides.html#the-pre-cloud-approach-make-the-one-computer-faster-and-faster",
    "href": "w02/slides.html#the-pre-cloud-approach-make-the-one-computer-faster-and-faster",
    "title": "Week 2: Cloud Computing",
    "section": "The ‚ÄúPre-Cloud‚Äù Approach: Make The One Computer Faster and Faster",
    "text": "The ‚ÄúPre-Cloud‚Äù Approach: Make The One Computer Faster and Faster"
  },
  {
    "objectID": "w02/slides.html#it-worked-for-many-decades",
    "href": "w02/slides.html#it-worked-for-many-decades",
    "title": "Week 2: Cloud Computing",
    "section": "It Worked For‚Ä¶ Many Decades!",
    "text": "It Worked For‚Ä¶ Many Decades!"
  },
  {
    "objectID": "w02/slides.html#is-moores-law-dead",
    "href": "w02/slides.html#is-moores-law-dead",
    "title": "Week 2: Cloud Computing",
    "section": "Is Moore‚Äôs Law Dead?",
    "text": "Is Moore‚Äôs Law Dead?\n\n‚Ä¶Kind of?\nFocus nowadays: specialized hardware / compute architectures hyper-optimized for particular tasks\n\n[If you took DSAN 5500] Think of BLAS\n\nGraphic Processing Units (GPUs)\nField Programmable Gate Arrays (FPGAs)\nData Processing Units (DPUs)\nGoogle‚Äôs Tensor Processing Units (TPUs)"
  },
  {
    "objectID": "w02/slides.html#so-even-upgrading-our-hardware-didnt-help-now-what-do-we-do",
    "href": "w02/slides.html#so-even-upgrading-our-hardware-didnt-help-now-what-do-we-do",
    "title": "Week 2: Cloud Computing",
    "section": "So, even upgrading our hardware didn‚Äôt help üò≠ now what do we do?",
    "text": "So, even upgrading our hardware didn‚Äôt help üò≠ now what do we do?\n\nWe distribute!\nMore CPUs, more memory, more storage!"
  },
  {
    "objectID": "w02/slides.html#my-favorite-example-ever",
    "href": "w02/slides.html#my-favorite-example-ever",
    "title": "Week 2: Cloud Computing",
    "section": "My Favorite Example Ever",
    "text": "My Favorite Example Ever\n\nSpongebob is cooking Krabby Patties too slowly to satisfy ravenous customers‚Ä¶\nShould he upgrade his one spatula? How bout just many simple spatulas at once!"
  },
  {
    "objectID": "w02/slides.html#how-do-we-achieve-this-thats-exactly-what-the-cloud-is-for",
    "href": "w02/slides.html#how-do-we-achieve-this-thats-exactly-what-the-cloud-is-for",
    "title": "Week 2: Cloud Computing",
    "section": "How Do We Achieve This? ‚Ä¶That‚Äôs Exactly What The Cloud is For!",
    "text": "How Do We Achieve This? ‚Ä¶That‚Äôs Exactly What The Cloud is For!"
  },
  {
    "objectID": "w02/slides.html#benefits",
    "href": "w02/slides.html#benefits",
    "title": "Week 2: Cloud Computing",
    "section": "Benefits",
    "text": "Benefits\n\n\n\n\nProvides access to low-cost computing\nCosts are decreasing every year\nElastic\nPaaS works!\nMany other benefits‚Ä¶"
  },
  {
    "objectID": "w02/slides.html#q-what-is-the-cloud",
    "href": "w02/slides.html#q-what-is-the-cloud",
    "title": "Week 2: Cloud Computing",
    "section": "Q: What is The Cloud?",
    "text": "Q: What is The Cloud?\n\nA: Using someone else‚Äôs computers"
  },
  {
    "objectID": "w02/slides.html#nist-definition",
    "href": "w02/slides.html#nist-definition",
    "title": "Week 2: Cloud Computing",
    "section": "NIST Definition",
    "text": "NIST Definition\n\n\n\n\n\n\nBased on Mell and Grance (2011) (The ‚Äúofficial‚Äù NIST definition of ‚ÄúCloud Computing‚Äù)"
  },
  {
    "objectID": "w02/slides.html#service-models",
    "href": "w02/slides.html#service-models",
    "title": "Week 2: Cloud Computing",
    "section": "Service Models",
    "text": "Service Models"
  },
  {
    "objectID": "w02/slides.html#hotel-analogy",
    "href": "w02/slides.html#hotel-analogy",
    "title": "Week 2: Cloud Computing",
    "section": "Hotel Analogy",
    "text": "Hotel Analogy"
  },
  {
    "objectID": "w02/slides.html#infrastructure-as-a-service-iaas",
    "href": "w02/slides.html#infrastructure-as-a-service-iaas",
    "title": "Week 2: Cloud Computing",
    "section": "Infrastructure as a Service (IaaS)",
    "text": "Infrastructure as a Service (IaaS)\n\nVirtualized computing resources delivered over the internet\nProvider manages: Physical hardware, virtualization, networking, storage\nYou manage: Operating systems, applications, runtime, data, middleware\n\nExamples and Use Cases\n\nAmazon EC2, Microsoft Azure VMs, Google Compute Engine\nPerfect for: Development environments, web hosting, backup & recovery\nBenefits: Rapid scaling, pay-as-you-go, global availability\n\n# Launch a virtual machine in seconds\naws ec2 run-instances --image-id ami-12345 --instance-type t3.xlarge"
  },
  {
    "objectID": "w02/slides.html#platform-as-a-service-paas",
    "href": "w02/slides.html#platform-as-a-service-paas",
    "title": "Week 2: Cloud Computing",
    "section": "Platform as a Service (PaaS)",
    "text": "Platform as a Service (PaaS)\nWhat is PaaS?\n\nComplete development and deployment environment in the cloud\nProvider manages: Infrastructure, operating systems, runtime environments\nYou manage: Applications and data\n\nExamples and Use Cases\n\nAWS Elastic Beanstalk, Google App Engine, Microsoft Azure App Service\nPerfect for: Web applications, API development, microservices\nBenefits: Faster development, automatic scaling, integrated DevOps\n\n# Deploy your app with minimal configuration\ngit push heroku main  # App automatically deployed and scaled"
  },
  {
    "objectID": "w02/slides.html#software-as-a-service-saas",
    "href": "w02/slides.html#software-as-a-service-saas",
    "title": "Week 2: Cloud Computing",
    "section": "Software as a Service (SaaS)",
    "text": "Software as a Service (SaaS)\n\nComplete applications delivered over the internet\nProvider manages: Everything (infrastructure, platform, software)\nYou manage: Your data and user access\n\nExamples and Use Cases\n\nGmail, Salesforce, Microsoft 365, Slack, Zoom\nPerfect for: Business applications, collaboration tools, CRM\nBenefits: No installation, automatic updates, accessible anywhere\n\nThe SaaS Revolution\n\n80% of companies use SaaS applications\n$195 billion market size (2023)"
  },
  {
    "objectID": "w02/slides.html#cloud-infrastructure-and-the-ai-revolution",
    "href": "w02/slides.html#cloud-infrastructure-and-the-ai-revolution",
    "title": "Week 2: Cloud Computing",
    "section": "Cloud Infrastructure and the AI Revolution",
    "text": "Cloud Infrastructure and the AI Revolution\n\nMassive Computational Requirements\n\nTraining large language models requires thousands of GPUs\nCloud provides elastic access to specialized hardware\n\nData Storage and Processing\n\nAI models need petabytes of training data\nCloud offers unlimited, globally distributed storage\n\nCost Efficiency\n\nPay-per-use model for expensive AI hardware\nNo upfront investment in GPU clusters"
  },
  {
    "objectID": "w02/slides.html#gpu-infrastructure-the-ai-powerhouse",
    "href": "w02/slides.html#gpu-infrastructure-the-ai-powerhouse",
    "title": "Week 2: Cloud Computing",
    "section": "GPU Infrastructure: The AI Powerhouse",
    "text": "GPU Infrastructure: The AI Powerhouse\nFrom Gaming to AI\n\nOriginally: Graphics processing for video games\nNow: Parallel processing powerhouse for AI/ML workloads\nArchitecture: Thousands of cores vs.¬†CPUs 8-64 cores\n\nCloud GPU Offerings\n# AWS GPU instances for different AI workloads\np4d.24xlarge   # 8x NVIDIA A100 (40GB) - $32/hour\np3.16xlarge    # 8x NVIDIA V100 (16GB) - $24/hour\ng4dn.xlarge    # 1x NVIDIA T4 (16GB)   - $0.50/hour\nThe Scale Challenge\n\nGPT-4 training: Estimated 25,000 A100 GPUs for 6 months\nMeta‚Äôs AI infrastructure: 350,000+ NVIDIA H100 chips\nCost: Single AI training run can cost $100M+"
  },
  {
    "objectID": "w02/slides.html#distributed-computing-and-large-clusters",
    "href": "w02/slides.html#distributed-computing-and-large-clusters",
    "title": "Week 2: Cloud Computing",
    "section": "Distributed Computing and Large Clusters",
    "text": "Distributed Computing and Large Clusters\nModern AI Requires Massive Clusters\n\nModel parallelism: Split models across multiple GPUs\nData parallelism: Process different data batches simultaneously\n\nPipeline parallelism: Different layers on different GPUs\n\nTechnologies Enabling Scale\n\nKubernetes: Container orchestration for microservices\nApache Spark: Distributed data processing framework\nRay: Distributed AI/ML framework for Python\n\n# Distributed training with Ray\nimport ray\nfrom ray import tune\n# Scale across 1000+ machines automatically\ntune.run(train_model, resources_per_trial={\"gpu\": 8})"
  },
  {
    "objectID": "w02/slides.html#data-storage-evolution",
    "href": "w02/slides.html#data-storage-evolution",
    "title": "Week 2: Cloud Computing",
    "section": "Data Storage Evolution",
    "text": "Data Storage Evolution\nThe Data Tsunami\n\n2025 projection: 181 zettabytes of data globally\nAI training datasets: CommonCrawl (800TB), ImageNet (150GB)\nReal-time requirements: 1-10ms latency for inference (some cases sub-millisecond)\n\nStorage Technologies\n\nObject Storage: Amazon S3, Google Cloud Storage (petabyte scale)\nHigh-performance: NVMe SSDs, persistent memory\nDistributed filesystems: HDFS, GlusterFS, Ceph\n\nThe Economics of Data\n\nStorage costs: Dropped 99.9% since 1980\nTransfer costs: Still significant for large datasets\nEdge computing: Bringing compute closer to data sources"
  },
  {
    "objectID": "w02/slides.html#evolution-of-the-cloud",
    "href": "w02/slides.html#evolution-of-the-cloud",
    "title": "Week 2: Cloud Computing",
    "section": "Evolution of the Cloud",
    "text": "Evolution of the Cloud\n\n\n\n\n\n\n\n\nYesterday\nToday\nTomorrow\n\n\n\n\nLimited number of tools and vendors\nMany tools and vendors to work with\nIntegrated tools and vendors\n\n\nOne platform - few devices\nMultiple platforms - many devices\nConnected platforms and devices\n\n\nData is scarce but manageable\nOverabundance of data\nData is used for important business decisions\n\n\nIT has major influence and control\nIT has limited influence and control\nIT is strategic to the business\n\n\nPeople only work when they are at work\nPeople work wherever they want\nPeople have access to what they need wherever they are"
  },
  {
    "objectID": "w02/slides.html#what-does-the-cloud-look-like-microsoft-azure-data-center",
    "href": "w02/slides.html#what-does-the-cloud-look-like-microsoft-azure-data-center",
    "title": "Week 2: Cloud Computing",
    "section": "What Does the Cloud Look Like? Microsoft Azure Data Center",
    "text": "What Does the Cloud Look Like? Microsoft Azure Data Center"
  },
  {
    "objectID": "w02/slides.html#loudon-county-va-cloudon",
    "href": "w02/slides.html#loudon-county-va-cloudon",
    "title": "Week 2: Cloud Computing",
    "section": "Loudon County, VA: ‚ÄúCLoudon‚Äù",
    "text": "Loudon County, VA: ‚ÄúCLoudon‚Äù\n\n70% of the world‚Äôs internet traffic passes through Loudon\nHow data centers power VA‚Äôs Loudon County\nThe heart of ‚ÄúThe Cloud‚Äù is in Virginia\nCBS Sunday Morning Visits the Home of the Internet in Loudoun County"
  },
  {
    "objectID": "w02/slides.html#further-reading-saas-and-cloud-computing",
    "href": "w02/slides.html#further-reading-saas-and-cloud-computing",
    "title": "Week 2: Cloud Computing",
    "section": "Further Reading: SaaS and Cloud Computing",
    "text": "Further Reading: SaaS and Cloud Computing\nSaaS and Cloud Computing Statistics:\n\n1. Zylo: 111 Unmissable SaaS Statistics for 2025\n2. Spendesk: 60+ eye-opening SaaS statistics (2025)\n3. Substly: The 28 most important SaaS statistics in 2023\n4. Harpa AI: SaaS Industry Key Statistics 2023\n5. Content Beta: 110+ SaaS Statistics and Trends\n6. Statista: Average number of SaaS apps used in the U.S. 2024\n7. Meetanshi: SaaS Statistics for 2025: Market Size, Growth, Trends & More"
  },
  {
    "objectID": "w02/slides.html#data-storage-and-ai-infrastructure",
    "href": "w02/slides.html#data-storage-and-ai-infrastructure",
    "title": "Week 2: Cloud Computing",
    "section": "Data Storage and AI Infrastructure",
    "text": "Data Storage and AI Infrastructure\n\n2025 global data volume (181 zettabytes)\n\nrivery.io\nExplodingTopics.com\nPIT.edu\nForbes\n\nCommonCrawl and ImageNet dataset sizes\nAI inference latency and storage tech:\n\nIBM\nRCR Wireless"
  },
  {
    "objectID": "w02/slides.html#storage-economics-and-technology",
    "href": "w02/slides.html#storage-economics-and-technology",
    "title": "Week 2: Cloud Computing",
    "section": "Storage Economics and Technology",
    "text": "Storage Economics and Technology\n\nStorage cost declines\n\nHumanProgress.org\nOur World In Data\n\nOngoing significance of transfer (egress) costs\nEdge computing trends"
  },
  {
    "objectID": "w02/slides.html#gpu-infrastructure-and-ai-training-costs",
    "href": "w02/slides.html#gpu-infrastructure-and-ai-training-costs",
    "title": "Week 2: Cloud Computing",
    "section": "GPU Infrastructure and AI Training Costs",
    "text": "GPU Infrastructure and AI Training Costs\n\nGPT-4 GPU count and training duration\n\nGenspark.ai\nklu.ai\nacorn.io\n\nMeta‚Äôs NVIDIA H100 GPUs\nAI training cost estimates\n\nGenspark.ai\nacorn.io"
  },
  {
    "objectID": "w02/slides.html#references",
    "href": "w02/slides.html#references",
    "title": "Week 2: Cloud Computing",
    "section": "References",
    "text": "References\n\n\nLoukides, Mike. 2010. ‚ÄúWhat Is Data Science?‚Äù O‚ÄôReilly Media. https://www.oreilly.com/radar/what-is-data-science/.\n\n\nMell, Peter, and Timothy Grance. 2011. ‚ÄúThe NIST Definition of Cloud Computing.‚Äù National Institute of Standards and Technology, Special Publication 800 (2011): 145. https://nvlpubs.nist.gov/nistpubs/legacy/sp/nistspecialpublication800-145.pdf."
  },
  {
    "objectID": "w02/index.html",
    "href": "w02/index.html",
    "title": "Week 2: Cloud Computing",
    "section": "",
    "text": "Open slides in new tab ‚Üí",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#jeffs-favorite-big-data-definition",
    "href": "w02/index.html#jeffs-favorite-big-data-definition",
    "title": "Week 2: Cloud Computing",
    "section": "Jeff‚Äôs Favorite ‚ÄúBig Data‚Äù Definition",
    "text": "Jeff‚Äôs Favorite ‚ÄúBig Data‚Äù Definition\n¬†\n\nBig data is when the size of the data itself becomes part of the problem (Loukides 2010)\n\n\nIn other words: Your problem becomes a ‚Äúbig data problem‚Äù when you hit one or both of the walls!",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#you-vs.-your-opps",
    "href": "w02/index.html#you-vs.-your-opps",
    "title": "Week 2: Cloud Computing",
    "section": "You vs.¬†Your Opps",
    "text": "You vs.¬†Your Opps",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#so-you-find-yourself-smashed-against-a-wall",
    "href": "w02/index.html#so-you-find-yourself-smashed-against-a-wall",
    "title": "Week 2: Cloud Computing",
    "section": "So you find yourself smashed against a wall‚Ä¶",
    "text": "So you find yourself smashed against a wall‚Ä¶\n(Some of yall would fold in this scenario)",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#the-pre-cloud-approach-make-the-one-computer-faster-and-faster",
    "href": "w02/index.html#the-pre-cloud-approach-make-the-one-computer-faster-and-faster",
    "title": "Week 2: Cloud Computing",
    "section": "The ‚ÄúPre-Cloud‚Äù Approach: Make The One Computer Faster and Faster",
    "text": "The ‚ÄúPre-Cloud‚Äù Approach: Make The One Computer Faster and Faster",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#it-worked-for-many-decades",
    "href": "w02/index.html#it-worked-for-many-decades",
    "title": "Week 2: Cloud Computing",
    "section": "It Worked For‚Ä¶ Many Decades!",
    "text": "It Worked For‚Ä¶ Many Decades!",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#is-moores-law-dead",
    "href": "w02/index.html#is-moores-law-dead",
    "title": "Week 2: Cloud Computing",
    "section": "Is Moore‚Äôs Law Dead?",
    "text": "Is Moore‚Äôs Law Dead?\n\n‚Ä¶Kind of?\nFocus nowadays: specialized hardware / compute architectures hyper-optimized for particular tasks\n\n[If you took DSAN 5500] Think of BLAS\n\nGraphic Processing Units (GPUs)\nField Programmable Gate Arrays (FPGAs)\nData Processing Units (DPUs)\nGoogle‚Äôs Tensor Processing Units (TPUs)",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#so-even-upgrading-our-hardware-didnt-help-now-what-do-we-do",
    "href": "w02/index.html#so-even-upgrading-our-hardware-didnt-help-now-what-do-we-do",
    "title": "Week 2: Cloud Computing",
    "section": "So, even upgrading our hardware didn‚Äôt help üò≠ now what do we do?",
    "text": "So, even upgrading our hardware didn‚Äôt help üò≠ now what do we do?\n\nWe distribute!\nMore CPUs, more memory, more storage!",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#my-favorite-example-ever",
    "href": "w02/index.html#my-favorite-example-ever",
    "title": "Week 2: Cloud Computing",
    "section": "My Favorite Example Ever",
    "text": "My Favorite Example Ever\n\nSpongebob is cooking Krabby Patties too slowly to satisfy ravenous customers‚Ä¶\nShould he upgrade his one spatula? How bout just many simple spatulas at once!",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#how-do-we-achieve-this-thats-exactly-what-the-cloud-is-for",
    "href": "w02/index.html#how-do-we-achieve-this-thats-exactly-what-the-cloud-is-for",
    "title": "Week 2: Cloud Computing",
    "section": "How Do We Achieve This? ‚Ä¶That‚Äôs Exactly What The Cloud is For!",
    "text": "How Do We Achieve This? ‚Ä¶That‚Äôs Exactly What The Cloud is For!",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#benefits",
    "href": "w02/index.html#benefits",
    "title": "Week 2: Cloud Computing",
    "section": "Benefits",
    "text": "Benefits\n\n\n\n\nProvides access to low-cost computing\nCosts are decreasing every year\nElastic\nPaaS works!\nMany other benefits‚Ä¶",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#q-what-is-the-cloud",
    "href": "w02/index.html#q-what-is-the-cloud",
    "title": "Week 2: Cloud Computing",
    "section": "Q: What is The Cloud?",
    "text": "Q: What is The Cloud?\n\nA: Using someone else‚Äôs computers",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#nist-definition",
    "href": "w02/index.html#nist-definition",
    "title": "Week 2: Cloud Computing",
    "section": "NIST Definition",
    "text": "NIST Definition\n\n\n\n\n\n\nBased on Mell and Grance (2011) (The ‚Äúofficial‚Äù NIST definition of ‚ÄúCloud Computing‚Äù)",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#service-models",
    "href": "w02/index.html#service-models",
    "title": "Week 2: Cloud Computing",
    "section": "Service Models",
    "text": "Service Models",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#hotel-analogy",
    "href": "w02/index.html#hotel-analogy",
    "title": "Week 2: Cloud Computing",
    "section": "Hotel Analogy",
    "text": "Hotel Analogy",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#infrastructure-as-a-service-iaas",
    "href": "w02/index.html#infrastructure-as-a-service-iaas",
    "title": "Week 2: Cloud Computing",
    "section": "Infrastructure as a Service (IaaS)",
    "text": "Infrastructure as a Service (IaaS)\n\nVirtualized computing resources delivered over the internet\nProvider manages: Physical hardware, virtualization, networking, storage\nYou manage: Operating systems, applications, runtime, data, middleware\n\nExamples and Use Cases\n\nAmazon EC2, Microsoft Azure VMs, Google Compute Engine\nPerfect for: Development environments, web hosting, backup & recovery\nBenefits: Rapid scaling, pay-as-you-go, global availability\n\n# Launch a virtual machine in seconds\naws ec2 run-instances --image-id ami-12345 --instance-type t3.xlarge",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#platform-as-a-service-paas",
    "href": "w02/index.html#platform-as-a-service-paas",
    "title": "Week 2: Cloud Computing",
    "section": "Platform as a Service (PaaS)",
    "text": "Platform as a Service (PaaS)\n\nWhat is PaaS?\n\nComplete development and deployment environment in the cloud\nProvider manages: Infrastructure, operating systems, runtime environments\nYou manage: Applications and data\n\nExamples and Use Cases\n\nAWS Elastic Beanstalk, Google App Engine, Microsoft Azure App Service\nPerfect for: Web applications, API development, microservices\nBenefits: Faster development, automatic scaling, integrated DevOps\n\n# Deploy your app with minimal configuration\ngit push heroku main  # App automatically deployed and scaled",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#software-as-a-service-saas",
    "href": "w02/index.html#software-as-a-service-saas",
    "title": "Week 2: Cloud Computing",
    "section": "Software as a Service (SaaS)",
    "text": "Software as a Service (SaaS)\n\nComplete applications delivered over the internet\nProvider manages: Everything (infrastructure, platform, software)\nYou manage: Your data and user access\n\nExamples and Use Cases\n\nGmail, Salesforce, Microsoft 365, Slack, Zoom\nPerfect for: Business applications, collaboration tools, CRM\nBenefits: No installation, automatic updates, accessible anywhere\n\nThe SaaS Revolution\n\n80% of companies use SaaS applications\n$195 billion market size (2023)",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#cloud-infrastructure-and-the-ai-revolution",
    "href": "w02/index.html#cloud-infrastructure-and-the-ai-revolution",
    "title": "Week 2: Cloud Computing",
    "section": "Cloud Infrastructure and the AI Revolution",
    "text": "Cloud Infrastructure and the AI Revolution\n\nMassive Computational Requirements\n\nTraining large language models requires thousands of GPUs\nCloud provides elastic access to specialized hardware\n\nData Storage and Processing\n\nAI models need petabytes of training data\nCloud offers unlimited, globally distributed storage\n\nCost Efficiency\n\nPay-per-use model for expensive AI hardware\nNo upfront investment in GPU clusters",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#gpu-infrastructure-the-ai-powerhouse",
    "href": "w02/index.html#gpu-infrastructure-the-ai-powerhouse",
    "title": "Week 2: Cloud Computing",
    "section": "GPU Infrastructure: The AI Powerhouse",
    "text": "GPU Infrastructure: The AI Powerhouse\nFrom Gaming to AI\n\nOriginally: Graphics processing for video games\nNow: Parallel processing powerhouse for AI/ML workloads\nArchitecture: Thousands of cores vs.¬†CPUs 8-64 cores\n\nCloud GPU Offerings\n# AWS GPU instances for different AI workloads\np4d.24xlarge   # 8x NVIDIA A100 (40GB) - $32/hour\np3.16xlarge    # 8x NVIDIA V100 (16GB) - $24/hour\ng4dn.xlarge    # 1x NVIDIA T4 (16GB)   - $0.50/hour\nThe Scale Challenge\n\nGPT-4 training: Estimated 25,000 A100 GPUs for 6 months\nMeta‚Äôs AI infrastructure: 350,000+ NVIDIA H100 chips\nCost: Single AI training run can cost $100M+",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#distributed-computing-and-large-clusters",
    "href": "w02/index.html#distributed-computing-and-large-clusters",
    "title": "Week 2: Cloud Computing",
    "section": "Distributed Computing and Large Clusters",
    "text": "Distributed Computing and Large Clusters\nModern AI Requires Massive Clusters\n\nModel parallelism: Split models across multiple GPUs\nData parallelism: Process different data batches simultaneously\n\nPipeline parallelism: Different layers on different GPUs\n\nTechnologies Enabling Scale\n\nKubernetes: Container orchestration for microservices\nApache Spark: Distributed data processing framework\nRay: Distributed AI/ML framework for Python\n\n# Distributed training with Ray\nimport ray\nfrom ray import tune\n# Scale across 1000+ machines automatically\ntune.run(train_model, resources_per_trial={\"gpu\": 8})",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#data-storage-evolution",
    "href": "w02/index.html#data-storage-evolution",
    "title": "Week 2: Cloud Computing",
    "section": "Data Storage Evolution",
    "text": "Data Storage Evolution\nThe Data Tsunami\n\n2025 projection: 181 zettabytes of data globally\nAI training datasets: CommonCrawl (800TB), ImageNet (150GB)\nReal-time requirements: 1-10ms latency for inference (some cases sub-millisecond)\n\nStorage Technologies\n\nObject Storage: Amazon S3, Google Cloud Storage (petabyte scale)\nHigh-performance: NVMe SSDs, persistent memory\nDistributed filesystems: HDFS, GlusterFS, Ceph\n\nThe Economics of Data\n\nStorage costs: Dropped 99.9% since 1980\nTransfer costs: Still significant for large datasets\nEdge computing: Bringing compute closer to data sources",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#evolution-of-the-cloud",
    "href": "w02/index.html#evolution-of-the-cloud",
    "title": "Week 2: Cloud Computing",
    "section": "Evolution of the Cloud",
    "text": "Evolution of the Cloud\n\n\n\n\n\n\n\n\nYesterday\nToday\nTomorrow\n\n\n\n\nLimited number of tools and vendors\nMany tools and vendors to work with\nIntegrated tools and vendors\n\n\nOne platform - few devices\nMultiple platforms - many devices\nConnected platforms and devices\n\n\nData is scarce but manageable\nOverabundance of data\nData is used for important business decisions\n\n\nIT has major influence and control\nIT has limited influence and control\nIT is strategic to the business\n\n\nPeople only work when they are at work\nPeople work wherever they want\nPeople have access to what they need wherever they are",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#what-does-the-cloud-look-like-microsoft-azure-data-center",
    "href": "w02/index.html#what-does-the-cloud-look-like-microsoft-azure-data-center",
    "title": "Week 2: Cloud Computing",
    "section": "What Does the Cloud Look Like? Microsoft Azure Data Center",
    "text": "What Does the Cloud Look Like? Microsoft Azure Data Center",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#loudon-county-va-cloudon",
    "href": "w02/index.html#loudon-county-va-cloudon",
    "title": "Week 2: Cloud Computing",
    "section": "Loudon County, VA: ‚ÄúCLoudon‚Äù",
    "text": "Loudon County, VA: ‚ÄúCLoudon‚Äù\n\n70% of the world‚Äôs internet traffic passes through Loudon\nHow data centers power VA‚Äôs Loudon County\nThe heart of ‚ÄúThe Cloud‚Äù is in Virginia\nCBS Sunday Morning Visits the Home of the Internet in Loudoun County",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#further-reading-saas-and-cloud-computing",
    "href": "w02/index.html#further-reading-saas-and-cloud-computing",
    "title": "Week 2: Cloud Computing",
    "section": "Further Reading: SaaS and Cloud Computing",
    "text": "Further Reading: SaaS and Cloud Computing\nSaaS and Cloud Computing Statistics:\n\n1. Zylo: 111 Unmissable SaaS Statistics for 2025\n2. Spendesk: 60+ eye-opening SaaS statistics (2025)\n3. Substly: The 28 most important SaaS statistics in 2023\n4. Harpa AI: SaaS Industry Key Statistics 2023\n5. Content Beta: 110+ SaaS Statistics and Trends\n6. Statista: Average number of SaaS apps used in the U.S. 2024\n7. Meetanshi: SaaS Statistics for 2025: Market Size, Growth, Trends & More",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#data-storage-and-ai-infrastructure",
    "href": "w02/index.html#data-storage-and-ai-infrastructure",
    "title": "Week 2: Cloud Computing",
    "section": "Data Storage and AI Infrastructure",
    "text": "Data Storage and AI Infrastructure\n\n2025 global data volume (181 zettabytes)\n\nrivery.io\nExplodingTopics.com\nPIT.edu\nForbes\n\nCommonCrawl and ImageNet dataset sizes\nAI inference latency and storage tech:\n\nIBM\nRCR Wireless",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#storage-economics-and-technology",
    "href": "w02/index.html#storage-economics-and-technology",
    "title": "Week 2: Cloud Computing",
    "section": "Storage Economics and Technology",
    "text": "Storage Economics and Technology\n\nStorage cost declines\n\nHumanProgress.org\nOur World In Data\n\nOngoing significance of transfer (egress) costs\nEdge computing trends",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#gpu-infrastructure-and-ai-training-costs",
    "href": "w02/index.html#gpu-infrastructure-and-ai-training-costs",
    "title": "Week 2: Cloud Computing",
    "section": "GPU Infrastructure and AI Training Costs",
    "text": "GPU Infrastructure and AI Training Costs\n\nGPT-4 GPU count and training duration\n\nGenspark.ai\nklu.ai\nacorn.io\n\nMeta‚Äôs NVIDIA H100 GPUs\nAI training cost estimates\n\nGenspark.ai\nacorn.io",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w02/index.html#references",
    "href": "w02/index.html#references",
    "title": "Week 2: Cloud Computing",
    "section": "References",
    "text": "References\n\n\nLoukides, Mike. 2010. ‚ÄúWhat Is Data Science?‚Äù O‚ÄôReilly Media. https://www.oreilly.com/radar/what-is-data-science/.\n\n\nMell, Peter, and Timothy Grance. 2011. ‚ÄúThe NIST Definition of Cloud Computing.‚Äù National Institute of Standards and Technology, Special Publication 800 (2011): 145. https://nvlpubs.nist.gov/nistpubs/legacy/sp/nistspecialpublication800-145.pdf.",
    "crumbs": [
      "Week 2: <span class='sec-w02-date'>Sep 2</span>"
    ]
  },
  {
    "objectID": "w04/slides.html#agenda-and-goals-for-today",
    "href": "w04/slides.html#agenda-and-goals-for-today",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Agenda and Goals for Today",
    "text": "Agenda and Goals for Today\n\n\nLecture\n\nDistributed file systems\nModern file types\nWorking with large tabular data on a single node\n\nDuckDB\nPolars\n\n\n\nLab\n\nRun a similar task with Pandas, polars and duckdb"
  },
  {
    "objectID": "w04/slides.html#logistics-and-review",
    "href": "w04/slides.html#logistics-and-review",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Logistics and Review",
    "text": "Logistics and Review\n\n\nDeadlines\n\nAssignment 1: Python Skills Due Sept 5 11:59pm\nLab 2: Cloud Tooling Due Sept 5 6pm\nAssignment 2: Shell & Linux Due Sept 11 11:59pm\nLab 3: Parallel Computing Due Sept 12 6pm\nAssignment 3: Parallelization Due Sept 18 11:59pm\nLab 4: Docker and Lambda Due Sept 19 6pm\nAssignment 4: Containers Due Sept 25 11:59pm\nLab 5: DuckDB & Polars Due Sept 26 6pm\n\n\nLook back and ahead\n\nContinue to use Slack for questions!\nDocker (containerization)\nLambda functions\nComing up: Spark and project"
  },
  {
    "objectID": "w04/slides.html#raw-ingredients-of-storage-systems",
    "href": "w04/slides.html#raw-ingredients-of-storage-systems",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Raw Ingredients of Storage Systems",
    "text": "Raw Ingredients of Storage Systems\n\n\n\nDisk drives (magnetic HDDs or SSDs)\nRAM\nNetworking and CPU\nSerialization\nCompression\nCaching\n\n\n\n\n\nFrom Reis and Housley (2022)"
  },
  {
    "objectID": "w04/slides.html#single-machine-vs.-distributed-storage",
    "href": "w04/slides.html#single-machine-vs.-distributed-storage",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Single-Machine vs.¬†Distributed Storage",
    "text": "Single-Machine vs.¬†Distributed Storage\n\nFrom Reis and Housley (2022)\n\n\nSingle-Machine\n\n\nThey are commonly used for storing operating system files, application files, and user data files.\nFilesystems are also used in databases to store data files, transaction logs, and backups.\n\n\n\nDistributed Storage\n\n\nA distributed filesystem is a type of filesystem that spans multiple computers.\nIt provides a unified view of files across all the computers in the system.\nHave existed before cloud"
  },
  {
    "objectID": "w04/slides.html#file-storage-types",
    "href": "w04/slides.html#file-storage-types",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "File Storage Types",
    "text": "File Storage Types\n\n\n\nLocal Disk\n\n\nOS-managed filesystems on local disk partition:\nNTFS (Windows)\nHFS+ (MacOS)\next4 (Linux)() on a local disk partition of SSD or magnetic disk\n\n\n\nNetwork-Attached (NAS)\n\n\nAccessed by clients over a network\nRedundancy and reliability, fine-grained control of resources, storage pooling across multiple disks for large virtual volumes, and file sharing across multiple machines\n\n\nCloud Filesystems\n\nNot object store (more on that later)\nNot the virtual hard drive attached to a virtual machine\nFully managed: Takes care of networking, managing disk clusters, failures, and configuration (Azure Files, Amazon Elastic Filesystem)\nBacked by Object Store\n\n\nBased on Reis and Housley (2022)"
  },
  {
    "objectID": "w04/slides.html#object-stores",
    "href": "w04/slides.html#object-stores",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Object Stores",
    "text": "Object Stores\n\nSomewhat confusing because object has several meanings in computer science.\nIn this context, we‚Äôre talking about a specialized file-like construct. It could be any type of file: TXT, CSV, JSON, images, videos, audio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContains objects of all shapes and sizes: each gets a unique identifier\nObjects are immutable: cannot be modified in place (unlike local FS)"
  },
  {
    "objectID": "w04/slides.html#distributed-fs-vs-object-store",
    "href": "w04/slides.html#distributed-fs-vs-object-store",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Distributed FS vs Object Store",
    "text": "Distributed FS vs Object Store\n\n\n\n\n\n\n\n\n\nDistributed File System\nObject Storage\n\n\n\n\nOrganization\nFiles in hierarchical directories\nFlat organization (though there can be overlays to provide hierarchical files structure)\n\n\nMethod\nPOSIX File Operations\nREST API\n\n\nImmutability\nNone: Random writes anywhere in file\nImmutable: need to replace/append entire object\n\n\nPerformance\nPerforms best for smaller files\nPerforms best for large files\n\n\nScalability\nMillions of files\nBillions of objects\n\n\n\nBoth provide:\n\nFault tolerance\nAvailability and consistency"
  },
  {
    "objectID": "w04/slides.html#before-data-locality-for-hadoop",
    "href": "w04/slides.html#before-data-locality-for-hadoop",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Before: Data locality (for Hadoop)",
    "text": "Before: Data locality (for Hadoop)\n\n\n\nFrom White (2015)"
  },
  {
    "objectID": "w04/slides.html#today-de-coupling-storage-from-compute",
    "href": "w04/slides.html#today-de-coupling-storage-from-compute",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Today: De-Coupling Storage from Compute",
    "text": "Today: De-Coupling Storage from Compute\n\nFrom Gopalan (2022)"
  },
  {
    "objectID": "w04/slides.html#data-on-disk-formats",
    "href": "w04/slides.html#data-on-disk-formats",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Data-on-Disk Formats",
    "text": "Data-on-Disk Formats\n\nPlain Text (CSV, TSV, FWF)\nJSON\nBinary Files"
  },
  {
    "objectID": "w04/slides.html#plain-text-csv-tsv-fwf",
    "href": "w04/slides.html#plain-text-csv-tsv-fwf",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Plain Text (CSV, TSV, FWF)",
    "text": "Plain Text (CSV, TSV, FWF)\n\n\n\n\n\n\nPay attention to encodings!\nLines end in linefeed, carriage-return, or both together depending on the OS that generated\nTypically, a single line of text contains a single record"
  },
  {
    "objectID": "w04/slides.html#json",
    "href": "w04/slides.html#json",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "JSON",
    "text": "JSON\n\n\n\n\n\n\nWarning\n\n\nJSON files have two flavors: JSON Lines vs.¬†JSON. Typically when we say data is in JSON format, we imply it‚Äôs JSON Lines which means that there is a single JSON object per line, and there are multiple lines.\n\n\n\n\n\nJSON Lines\n4 records, one per line, no end comma\n{\"id\":1, \"name\":\"marck\", \"last_name\":\"vaisman\"}\n{\"id\":2, \"name\":\"anderson\", \"last_name\":\"monken\"}\n{\"id\":3, \"name\":\"amit\", \"last_name\":\"arora\"}\n{\"id\":4, \"name\":\"abhijit\", \"last_name\":\"dasgupta\"}\n\nJSON\n4 records enclosed in 1 JSON Array\n[\n  {\"id\":1, \"name\":\"marck\", \"last_name\":\"vaisman\"},\n  {\"id\":2, \"name\":\"anderson\", \"last_name\":\"monken\"},\n  {\"id\":3, \"name\":\"amit\", \"last_name\":\"arora\"},\n  {\"id\":4, \"name\":\"abhijit\", \"last_name\":\"dasgupta\"},\n]"
  },
  {
    "objectID": "w04/slides.html#binary-files",
    "href": "w04/slides.html#binary-files",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Binary Files",
    "text": "Binary Files"
  },
  {
    "objectID": "w04/slides.html#issues-with-common-file-formats-especially-csv",
    "href": "w04/slides.html#issues-with-common-file-formats-especially-csv",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Issues with Common File Formats (Especially CSV)",
    "text": "Issues with Common File Formats (Especially CSV)\n\nUbiquitous but highly error-prone\nDefault delimiter: familiar character in English, the comma\nAmbiguities:\n\nDelimiter (comma, tab, semi-colon, custom)\nQuote characters (single or doble quote)\nEscaping to appropriately handle string data\n\nDoesn‚Äôt natively encode schema information\nNo direct support for nested structures\nEncoding+schema must be configured on target system to ensure ingestion\nAutodetection provided in many cloud environments but is inappropriate for production ingestion (can be painfully slow)\nData engineers often forced to work with CSV data and then build robust exception handling and error detection to ensure data quality (Pydantic!)"
  },
  {
    "objectID": "w04/slides.html#introducing-apache-parquet",
    "href": "w04/slides.html#introducing-apache-parquet",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Introducing Apache Parquet",
    "text": "Introducing Apache Parquet\n\nFree, open-source, column-oriented data storage format created by Twitter and Cloudera (v1.0 released July 2013)\nData stored in columnar format (as opposed to row format), designed for read and write performance\nBuilds in schema information and natively supports nested data\nSupported by R and Python through Apache Arrow (more coming up!)"
  },
  {
    "objectID": "w04/slides.html#traditional-row-store",
    "href": "w04/slides.html#traditional-row-store",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Traditional Row-Store",
    "text": "Traditional Row-Store\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuery: ‚ÄúHow many balls did we sell?\nThe engine must scan each and every row until the end!"
  },
  {
    "objectID": "w04/slides.html#column-store",
    "href": "w04/slides.html#column-store",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Column-Store",
    "text": "Column-Store\n\n\n\n\n\n\n\nParquet file format, everything you need to know"
  },
  {
    "objectID": "w04/slides.html#row-groups",
    "href": "w04/slides.html#row-groups",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Row Groups",
    "text": "Row Groups\n\n\n\nData is stored in row groups!\n\n\n\nOnly required fields"
  },
  {
    "objectID": "w04/slides.html#metadata-compression-and-dictionary-encoding",
    "href": "w04/slides.html#metadata-compression-and-dictionary-encoding",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Metadata, compression, and dictionary encoding",
    "text": "Metadata, compression, and dictionary encoding"
  },
  {
    "objectID": "w04/slides.html#apache-arrow-for-in-memory-analytics",
    "href": "w04/slides.html#apache-arrow-for-in-memory-analytics",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Apache Arrow for In-Memory Analytics",
    "text": "Apache Arrow for In-Memory Analytics\n\nApache Arrow is a development platform for in-memory analytics. It contains a set of technologies that enable big data systems to process and move data fast. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. (Topol and McKinney 2024)"
  },
  {
    "objectID": "w04/slides.html#before-arrow",
    "href": "w04/slides.html#before-arrow",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Before Arrow",
    "text": "Before Arrow\n\nTopol and McKinney (2024)"
  },
  {
    "objectID": "w04/slides.html#after-arrow",
    "href": "w04/slides.html#after-arrow",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "After Arrow",
    "text": "After Arrow\n\nTopol and McKinney (2024)"
  },
  {
    "objectID": "w04/slides.html#arrow-compatibility",
    "href": "w04/slides.html#arrow-compatibility",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Arrow Compatibility",
    "text": "Arrow Compatibility\n\nTopol and McKinney (2024)"
  },
  {
    "objectID": "w04/slides.html#arrow-performance",
    "href": "w04/slides.html#arrow-performance",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Arrow Performance",
    "text": "Arrow Performance\n\n\n\n\n\nTopol and McKinney (2024)\n\n\n\n\n\n\nTopol and McKinney (2024)"
  },
  {
    "objectID": "w04/slides.html#using-arrow-with-csv-and-parquet",
    "href": "w04/slides.html#using-arrow-with-csv-and-parquet",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Using Arrow with CSV and Parquet",
    "text": "Using Arrow with CSV and Parquet\n\n\n\nPython\n\npyarrow or from pandas\nimport pandas as pd\npd.read_csv(engine = 'pyarrow')\npd.read_parquet()\n\nimport pyarrow.csv\npyarrow.csv.read_csv()\n\nimport pyarrow.parquet\npyarrow.parquet.read_table()\n\n\nR\n\nUse the arrow package\nlibrary(arrow)\n\nread_csv_arrow()\nread_parquet()\nread_json_arrow()\n\nwrite_csv_arrow()\nwrite_parquet()\n\nRecommendation: save your intermediate and analytical datasets as Parquet!"
  },
  {
    "objectID": "w04/slides.html#before-we-begin",
    "href": "w04/slides.html#before-we-begin",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Before We Begin‚Ä¶",
    "text": "Before We Begin‚Ä¶\n\nPandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\n\nPandas is slow, but less slow if you use it the right way!\n\nApache Arrow and the ‚Äú10 Things I Hate About pandas‚Äù (A 2017 post from the creator of Pandas‚Ä¶))\n50x faster data loading in Pandas: no problem (an old 2019 article‚Ä¶)\nIs Pandas really that slow?\nPandas 2.0 and the arrow revolution"
  },
  {
    "objectID": "w04/slides.html#polars-1",
    "href": "w04/slides.html#polars-1",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Polars",
    "text": "Polars"
  },
  {
    "objectID": "w04/slides.html#why-is-polars-faster-than-pandas",
    "href": "w04/slides.html#why-is-polars-faster-than-pandas",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Why is Polars Faster than Pandas?",
    "text": "Why is Polars Faster than Pandas?\n\nPolars is written in Rust. Rust is compiled; Python is interpreted\n\nCompiled language: you generate the machine code only once then run it, subsequent runs do not need the compilation step.\nInterpreted language: code has to be parsed, interpreted and converted into machine code every single time.\n\nParallelization: Vectorized operations can be executed in parallel on multiple cores\nLazy evaluation: Polars supports two APIs lazy as well as eager evaluation (used by pandas). In lazy evaluation, a query is executed only when required. While in eager evaluation, a query is executed immediately.\nPolars uses Arrow for in-memory data representation. Similar to how pandas uses NumPy (Pandas 2 allows using Arrow as backend)\nPolars \\(\\approx\\) in-memory DataFrame library + query optimizer\n\n\n\n[Excerpt from this post from Ritchie Vink, author of Polars] Arrow provides the efficient data structures and some compute kernels, like a SUM, a FILTER, a MAX etc. Arrow is not a query engine. Polars is a DataFrame library on top of arrow that has implemented efficient algorithms for JOINS, GROUPBY, PIVOTs, MELTs, QUERY OPTIMIZATION, etc. (the things you expect from a DF lib)."
  },
  {
    "objectID": "w04/slides.html#ease-of-use",
    "href": "w04/slides.html#ease-of-use",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Ease of Use",
    "text": "Ease of Use\n\nFamiliar API for users of Pandas: differences in syntax but still a Dataframe API making it straightforward to perform common operations such as filtering, aggregating, and joining data\nSee Migrating from Pandas\nReading data\n# must install s3fs -&gt; \"pip install s3fs\"\n# Using Polars\nimport polars as pl\npolars_df = pl.read_parquet(\"s3://nyc-tlc/trip data/yellow_tripdata_2023-06.parquet\")\n\n# using Pandas\nimport pandas as pd\npandas_df = pd.read_parquet(\"s3://nyc-tlc/trip data/yellow_tripdata_2023-06.parquet\")\nSelecting columns (see Pushdown optimization)\n# Using Polars\nselected_columns_polars = polars_df[['column1', 'column2']]\n\n# Using Pandas\nselected_columns_pandas = pandas_df[['column1', 'column2']]"
  },
  {
    "objectID": "w04/slides.html#ease-of-use-contd.",
    "href": "w04/slides.html#ease-of-use-contd.",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Ease of Use (contd.)",
    "text": "Ease of Use (contd.)\n\nFiltering data\n# Using Polars\nfiltered_polars = polars_df[polars_df['column1'] &gt; 10]\n\n# Using Pandas\nfiltered_pandas = pandas_df[pandas_df['column1'] &gt; 10]\nThough you can write Polars code that looks like Pandas, better to write idiomatic Polars code that takes advantage of Polars‚Äô features\nMigrating from Apache Spark: Whereas Spark DataFrame is a collection of rows, Polars DataFrame is closer to a collection of columns"
  },
  {
    "objectID": "w04/slides.html#installation-data-loading-and-basic-operations",
    "href": "w04/slides.html#installation-data-loading-and-basic-operations",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Installation, Data Loading, and Basic Operations",
    "text": "Installation, Data Loading, and Basic Operations\nInstall via pip:\npip install polars\nImport polars in your Python code and read data as usual:\nimport polars as pl\ndf = pl.read_parquet(\"s3://nyc-tlc/trip data/yellow_tripdata_2023-06.parquet\")\ndf.head()\n\nshape: (5, 19)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ VendorID ‚îÜ tpep_picku ‚îÜ tpep_dropoff ‚îÜ passenger_co ‚îÜ ... ‚îÜ improvement_ ‚îÜ total_amount ‚îÜ congestion_s ‚îÜ Airport_fee ‚îÇ\n‚îÇ ---      ‚îÜ p_datetime ‚îÜ _datetime    ‚îÜ unt          ‚îÜ     ‚îÜ surcharge    ‚îÜ ---          ‚îÜ urcharge     ‚îÜ ---         ‚îÇ\n‚îÇ i32      ‚îÜ ---        ‚îÜ ---          ‚îÜ ---          ‚îÜ     ‚îÜ ---          ‚îÜ f64          ‚îÜ ---          ‚îÜ f64         ‚îÇ\n‚îÇ          ‚îÜ datetime[n ‚îÜ datetime[ns] ‚îÜ i64          ‚îÜ     ‚îÜ f64          ‚îÜ              ‚îÜ f64          ‚îÜ             ‚îÇ\n‚îÇ          ‚îÜ s]         ‚îÜ              ‚îÜ              ‚îÜ     ‚îÜ              ‚îÜ              ‚îÜ              ‚îÜ             ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ 1        ‚îÜ 2023-06-01 ‚îÜ 2023-06-01   ‚îÜ 1            ‚îÜ ... ‚îÜ 1.0          ‚îÜ 33.6         ‚îÜ 2.5          ‚îÜ 0.0         ‚îÇ\n‚îÇ          ‚îÜ 00:08:48   ‚îÜ 00:29:41     ‚îÜ              ‚îÜ     ‚îÜ              ‚îÜ              ‚îÜ              ‚îÜ             ‚îÇ\n‚îÇ 1        ‚îÜ 2023-06-01 ‚îÜ 2023-06-01   ‚îÜ 0            ‚îÜ ... ‚îÜ 1.0          ‚îÜ 23.6         ‚îÜ 2.5          ‚îÜ 0.0         ‚îÇ\n‚îÇ          ‚îÜ 00:15:04   ‚îÜ 00:25:18     ‚îÜ              ‚îÜ     ‚îÜ              ‚îÜ              ‚îÜ              ‚îÜ             ‚îÇ\n‚îÇ 1        ‚îÜ 2023-06-01 ‚îÜ 2023-06-01   ‚îÜ 1            ‚îÜ ... ‚îÜ 1.0          ‚îÜ 60.05        ‚îÜ 0.0          ‚îÜ 1.75        ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "w04/slides.html#polars-pipeline-example",
    "href": "w04/slides.html#polars-pipeline-example",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Polars Pipeline Example",
    "text": "Polars Pipeline Example\nWe‚Äôll run this as part of the lab in a little bit; think how you might code this in Pandas‚Ä¶\n\nPolars pipeline"
  },
  {
    "objectID": "w04/slides.html#further-reading",
    "href": "w04/slides.html#further-reading",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Further reading",
    "text": "Further reading\n\nPolars\nUser guide ‚Üê MUST READ\nPolars GitHub repo\nPandas Vs Polars: a syntax and speed comparison\nTips & tricks for working with strings in Polars"
  },
  {
    "objectID": "w04/slides.html#duckdb-1",
    "href": "w04/slides.html#duckdb-1",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "DuckDB",
    "text": "DuckDB\n\nDuckDB is an in-process SQL OLAP DB management system\nLike sqlite, but for analytics. What does this mean? It means that your database runs inside your process, there are no servers to manage, no remote system to connect to. Easy to experiment with SQL-like syntax.\nVectorized processing: Loads chunks of data into memory (tries to keep everything in the CPU‚Äôs L1 and L2 cache) and is thus able to handle datasets bigger than the amount of RAM available.\nSupports Python, R and a host of other languages\nImportant paper on DuckDB: Raasveldt and M√ºhleisen (2019)"
  },
  {
    "objectID": "w04/slides.html#key-features",
    "href": "w04/slides.html#key-features",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Key Features",
    "text": "Key Features\n\nColumnar Storage, Vectorized Query Processing: DuckDB contains a columnar-vectorized query execution engine, where queries are run on a large batch of values (a ‚Äúvector‚Äù) in one operation\n\nMost analytical queries (think group by and summarize) or even data retrieval for training ML models require retrieving a subset of columns and now the entire row, columnar storage make this faster\n\nIn-Memory Processing: All data needed for processing is brought within the process memory (recall that columnar storage format helps with this) making queries run faster (no DB call over the network)\nSQL Support: highly Postgres-compatible version of SQL1.\nACID Compliance: Transactional guarantees (ACID properties) through bulk-optimized Multi-Version Concurrency Control (MVCC).\n\nFriendlier SQL with DuckDB"
  },
  {
    "objectID": "w04/slides.html#use-cases-for-duckdb",
    "href": "w04/slides.html#use-cases-for-duckdb",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Use Cases for DuckDB",
    "text": "Use Cases for DuckDB\n\nData Warehousing\nBusiness Intelligence\nReal-Time Analytics\nIoT Data Processing\n\nDuckDB in the Wild\n\nHow We Silently Switched Mode‚Äôs In-Memory Data Engine to DuckDB To Boost Visual Data Exploration Speed\nWhy we built Rill with DuckDB\nLeveraging DuckDB for enhanced performance in dbt projects"
  },
  {
    "objectID": "w04/slides.html#duckdb-diy",
    "href": "w04/slides.html#duckdb-diy",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "DuckDB: DIY",
    "text": "DuckDB: DIY\n\nDatalake and DuckDBUsing DuckDB in AWS Lambda"
  },
  {
    "objectID": "w04/slides.html#duckdb-fully-managed",
    "href": "w04/slides.html#duckdb-fully-managed",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "DuckDB: Fully-Managed",
    "text": "DuckDB: Fully-Managed\n\nMotherDuck Architecture\nArchitecture and capabilities\nSeamlessly analyze data, whether it sits on your laptop, in the cloud or split between.\nHybrid execution automatically plans each part of your query and determines where it‚Äôs best computed\nDuckDB Vs MotherDuck"
  },
  {
    "objectID": "w04/slides.html#setting-up-duckdb",
    "href": "w04/slides.html#setting-up-duckdb",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Setting Up DuckDB",
    "text": "Setting Up DuckDB\nConfiguration and Initialization: DuckDB is integrated into Python and R for efficient interactive data analysis (APIs for Java, C, C++, Julia, Swift, and others)\npip install duckdb\nConnecting to DuckDB\nimport duckdb\n# directly query a Pandas DataFrame\nimport pandas as pd\ndata_url = \"https://raw.githubusercontent.com/anly503/datasets/main/EconomistData.csv\"\ndf = pd.read_csv(data_url)\nduckdb.sql('SELECT * FROM df')"
  },
  {
    "objectID": "w04/slides.html#setting-up-duckdb-contd.",
    "href": "w04/slides.html#setting-up-duckdb-contd.",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Setting Up DuckDB (contd.)",
    "text": "Setting Up DuckDB (contd.)\nSupported data formats: DuckDB can ingest data from a wide variety of formats ‚Äì both on-disk and in-memory. See the data ingestion page for more information.\nimport duckdb\nduckdb.read_csv('example.csv')                # read a CSV file into a Relation\nduckdb.read_parquet('example.parquet')        # read a Parquet file into a Relation\nduckdb.read_json('example.json')              # read a JSON file into a Relation\n\nduckdb.sql('SELECT * FROM \"example.csv\"')     # directly query a CSV file\nduckdb.sql('SELECT * FROM \"example.parquet\"') # directly query a Parquet file\nduckdb.sql('SELECT * FROM \"example.json\"')    # directly query a JSON file"
  },
  {
    "objectID": "w04/slides.html#querying-duckdb",
    "href": "w04/slides.html#querying-duckdb",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Querying DuckDB",
    "text": "Querying DuckDB\nEssential Reading\n\nFriendlier SQL with DuckDB\nSQL Introduction\n\nBasic SQL Queries\nimport duckdb\nimport pandas as pd\nbabynames = pd.read_parquet(\"https://github.com/anly503/datasets/raw/main/babynames.parquet.zstd\")\nduckdb.sql(\"select count(*)  from babynames where Name='John'\")\nAggregations and Grouping\nduckdb.sql(\"select State, Name, count(*) as count  from babynames group by State, Name order by State desc, count desc\")"
  },
  {
    "objectID": "w04/slides.html#querying-duckdb-contd.",
    "href": "w04/slides.html#querying-duckdb-contd.",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Querying DuckDB (contd.)",
    "text": "Querying DuckDB (contd.)\nEssential reading: FROM and JOIN clauses\nJoins and Subqueries\n# Join two tables together\nduckdb.sql(\"SELECT * FROM table_name JOIN other_table ON (table_name.key = other_table.key\")\nWindow Functions\npowerplants = pd.read_csv(\"https://raw.githubusercontent.com/anly503/datasets/main/powerplants.csv\", parse_dates=[\"date\"])\nq = \"\"\"\nSELECT \"plant\", \"date\",\n    AVG(\"MWh\") OVER (\n        PARTITION BY \"plant\"\n        ORDER BY \"date\" ASC\n        RANGE BETWEEN INTERVAL 3 DAYS PRECEDING\n                  AND INTERVAL 3 DAYS FOLLOWING)\n        AS \"MWh 7-day Moving Average\"\nFROM powerplants\nORDER BY 1, 2;\n\"\"\"\nduckdb.sql(q)"
  },
  {
    "objectID": "w04/slides.html#using-the-duckdb-cli-and-shell",
    "href": "w04/slides.html#using-the-duckdb-cli-and-shell",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Using the DuckDB CLI and Shell",
    "text": "Using the DuckDB CLI and Shell\n\nInstall DuckDB CLI or use in browser via shell.duckdb.org for data exploration via SQL; Once installed, import a local file into the shell and run queries\nYou can download powerplants.csv here\nC:\\Users\\&lt;username&gt;\\Downloads\\duckdb_cli-windows-amd64&gt; duckdb\nv0.8.1 6536a77232\nEnter \".help\" for usage hints.\nConnected to a transient in-memory database.\nUse \".open FILENAME\" to reopen on a persistent database.\nD CREATE TABLE powerplants AS SELECT * FROM read_csv_auto('powerplants.csv');\nD DESCRIBE powerplants;\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ column_name ‚îÇ column_type ‚îÇ  null   ‚îÇ   key   ‚îÇ default ‚îÇ extra ‚îÇ\n‚îÇ   varchar   ‚îÇ   varchar   ‚îÇ varchar ‚îÇ varchar ‚îÇ varchar ‚îÇ int32 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ plant       ‚îÇ VARCHAR     ‚îÇ YES     ‚îÇ         ‚îÇ         ‚îÇ       ‚îÇ\n‚îÇ date        ‚îÇ DATE        ‚îÇ YES     ‚îÇ         ‚îÇ         ‚îÇ       ‚îÇ\n‚îÇ MWh         ‚îÇ BIGINT      ‚îÇ YES     ‚îÇ         ‚îÇ         ‚îÇ       ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\nD  SELECT * from powerplants where plant='Boston' and date='2019-01-02';\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  plant  ‚îÇ    date    ‚îÇ  MWh   ‚îÇ\n‚îÇ varchar ‚îÇ    date    ‚îÇ int64  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ Boston  ‚îÇ 2019-01-02 ‚îÇ 564337 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "w04/slides.html#profiling-in-duckdb",
    "href": "w04/slides.html#profiling-in-duckdb",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Profiling in DuckDB",
    "text": "Profiling in DuckDB\nQuery Optimization: Use EXPLAIN and ANALYZE keywords to understand how your query is being executed and the time being spent in individual steps\nD EXPLAIN ANALYZE SELECT * from powerplants where plant='Boston' and date='2019-01-02';\nDuckDB will use all the cores available on the underlying compute, but you can adjust this (Full configuration details here)\nD select current_setting('threads');\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ current_setting('threads') ‚îÇ\n‚îÇ           int64            ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ                          8 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\nD SET threads=4;\nD select current_setting('threads');\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ current_setting('threads') ‚îÇ\n‚îÇ           int64            ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ                          4 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "w04/slides.html#benchmarks-and-comparisons",
    "href": "w04/slides.html#benchmarks-and-comparisons",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Benchmarks and Comparisons",
    "text": "Benchmarks and Comparisons\n\n\n\n\nTricky topic! Can make your chosen solution look better by focusing on metrics on which it provides better results\nIn general, TPC-H and TPC-DS are considered the standard benchmarks for data processing.\n\n\n\n\n\n\nTPC-DS Homepage"
  },
  {
    "objectID": "w04/slides.html#further-reading-on-duckdb",
    "href": "w04/slides.html#further-reading-on-duckdb",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Further Reading on DuckDB",
    "text": "Further Reading on DuckDB\n\nParallel Grouped Aggregation in DuckDB\nMeta queries\nProfiling queries in DuckDB\nDuckDB tutorial for beginners\nDuckDB CLI API\nUsing DuckDB in AWS Lambda\nRevisiting the Poor Man‚Äôs Data Lake with MotherDuck\nSupercharge your data processing with DuckDB\nFriendlier SQL with DuckDB\nBuilding and deploying data apps with DuckDB and Streamlit"
  },
  {
    "objectID": "w04/slides.html#references",
    "href": "w04/slides.html#references",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "References",
    "text": "References\n\n\nGopalan, Rukmani. 2022. The Cloud Data Lake: A Guide to Building Robust Cloud Data Architecture. O‚ÄôReilly Media, Inc. https://www.dropbox.com/scl/fi/7u469ccc8y169us5hydk0/The-cloud-data-lake-a-guide-to-building-robust-cloud-data-Rukmani-Gopalan.pdf?rlkey=ey06a6zt9g90d4zq8tfncndta&dl=1.\n\n\nRaasveldt, Mark, and Hannes M√ºhleisen. 2019. ‚ÄúDuckDB: An Embeddable Analytical Database.‚Äù In Proceedings of the 2019 International Conference on Management of Data, 1981‚Äì84. SIGMOD ‚Äô19. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3299869.3320212.\n\n\nReis, Joe, and Matt Housley. 2022. Fundamentals of Data Engineering: Plan and Build Robust Data Systems. O‚ÄôReilly Media, Inc.\n\n\nTopol, Matthew, and Wes McKinney. 2024. In-Memory Analytics with Apache Arrow. Packt Publishing Ltd.\n\n\nWhite, Tom E. 2015. Hadoop: The Definitive Guide. O‚ÄôReilly Media, Inc."
  },
  {
    "objectID": "w04/index.html",
    "href": "w04/index.html",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "",
    "text": "Open slides in new tab ‚Üí",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#agenda-and-goals-for-today",
    "href": "w04/index.html#agenda-and-goals-for-today",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Agenda and Goals for Today",
    "text": "Agenda and Goals for Today\n\n\n\nLecture\n\nDistributed file systems\nModern file types\nWorking with large tabular data on a single node\n\nDuckDB\nPolars\n\n\n\n\n\nLab\n\nRun a similar task with Pandas, polars and duckdb",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#logistics-and-review",
    "href": "w04/index.html#logistics-and-review",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Logistics and Review",
    "text": "Logistics and Review\n\n\nDeadlines\n\nAssignment 1: Python Skills Due Sept 5 11:59pm\nLab 2: Cloud Tooling Due Sept 5 6pm\nAssignment 2: Shell & Linux Due Sept 11 11:59pm\nLab 3: Parallel Computing Due Sept 12 6pm\nAssignment 3: Parallelization Due Sept 18 11:59pm\nLab 4: Docker and Lambda Due Sept 19 6pm\nAssignment 4: Containers Due Sept 25 11:59pm\nLab 5: DuckDB & Polars Due Sept 26 6pm\n\n\nLook back and ahead\n\nContinue to use Slack for questions!\nDocker (containerization)\nLambda functions\nComing up: Spark and project",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#raw-ingredients-of-storage-systems",
    "href": "w04/index.html#raw-ingredients-of-storage-systems",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Raw Ingredients of Storage Systems",
    "text": "Raw Ingredients of Storage Systems\n\n\n\nDisk drives (magnetic HDDs or SSDs)\nRAM\nNetworking and CPU\nSerialization\nCompression\nCaching\n\n\n\n\n\nFrom Reis and Housley (2022)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#single-machine-vs.-distributed-storage",
    "href": "w04/index.html#single-machine-vs.-distributed-storage",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Single-Machine vs.¬†Distributed Storage",
    "text": "Single-Machine vs.¬†Distributed Storage\n\n\n\nFrom Reis and Housley (2022)\n\n\n\n\n\nSingle-Machine\n\n\nThey are commonly used for storing operating system files, application files, and user data files.\nFilesystems are also used in databases to store data files, transaction logs, and backups.\n\n\n\nDistributed Storage\n\n\nA distributed filesystem is a type of filesystem that spans multiple computers.\nIt provides a unified view of files across all the computers in the system.\nHave existed before cloud",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#file-storage-types",
    "href": "w04/index.html#file-storage-types",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "File Storage Types",
    "text": "File Storage Types\n\n\n\nLocal Disk\n\n\nOS-managed filesystems on local disk partition:\nNTFS (Windows)\nHFS+ (MacOS)\next4 (Linux)() on a local disk partition of SSD or magnetic disk\n\n\n\nNetwork-Attached (NAS)\n\n\nAccessed by clients over a network\nRedundancy and reliability, fine-grained control of resources, storage pooling across multiple disks for large virtual volumes, and file sharing across multiple machines\n\n\nCloud Filesystems\n\nNot object store (more on that later)\nNot the virtual hard drive attached to a virtual machine\nFully managed: Takes care of networking, managing disk clusters, failures, and configuration (Azure Files, Amazon Elastic Filesystem)\nBacked by Object Store\n\n\n\nBased on Reis and Housley (2022)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#object-stores",
    "href": "w04/index.html#object-stores",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Object Stores",
    "text": "Object Stores\n\nSomewhat confusing because object has several meanings in computer science.\nIn this context, we‚Äôre talking about a specialized file-like construct. It could be any type of file: TXT, CSV, JSON, images, videos, audio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContains objects of all shapes and sizes: each gets a unique identifier\nObjects are immutable: cannot be modified in place (unlike local FS)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#distributed-fs-vs-object-store",
    "href": "w04/index.html#distributed-fs-vs-object-store",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Distributed FS vs Object Store",
    "text": "Distributed FS vs Object Store\n\n\n\n\n\n\n\n\n\nDistributed File System\nObject Storage\n\n\n\n\nOrganization\nFiles in hierarchical directories\nFlat organization (though there can be overlays to provide hierarchical files structure)\n\n\nMethod\nPOSIX File Operations\nREST API\n\n\nImmutability\nNone: Random writes anywhere in file\nImmutable: need to replace/append entire object\n\n\nPerformance\nPerforms best for smaller files\nPerforms best for large files\n\n\nScalability\nMillions of files\nBillions of objects\n\n\n\nBoth provide:\n\nFault tolerance\nAvailability and consistency",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#before-data-locality-for-hadoop",
    "href": "w04/index.html#before-data-locality-for-hadoop",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Before: Data locality (for Hadoop)",
    "text": "Before: Data locality (for Hadoop)\n\n\n\nFrom White (2015)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#today-de-coupling-storage-from-compute",
    "href": "w04/index.html#today-de-coupling-storage-from-compute",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Today: De-Coupling Storage from Compute",
    "text": "Today: De-Coupling Storage from Compute\n\n\n\nFrom Gopalan (2022)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#data-on-disk-formats",
    "href": "w04/index.html#data-on-disk-formats",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Data-on-Disk Formats",
    "text": "Data-on-Disk Formats\n\nPlain Text (CSV, TSV, FWF)\nJSON\nBinary Files",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#plain-text-csv-tsv-fwf",
    "href": "w04/index.html#plain-text-csv-tsv-fwf",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Plain Text (CSV, TSV, FWF)",
    "text": "Plain Text (CSV, TSV, FWF)\n\n\n\n\n\n\nPay attention to encodings!\nLines end in linefeed, carriage-return, or both together depending on the OS that generated\nTypically, a single line of text contains a single record",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#json",
    "href": "w04/index.html#json",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "JSON",
    "text": "JSON\n\n\n\n\n\n\nWarning\n\n\n\nJSON files have two flavors: JSON Lines vs.¬†JSON. Typically when we say data is in JSON format, we imply it‚Äôs JSON Lines which means that there is a single JSON object per line, and there are multiple lines.\n\n\n\n\nJSON Lines\n4 records, one per line, no end comma\n{\"id\":1, \"name\":\"marck\", \"last_name\":\"vaisman\"}\n{\"id\":2, \"name\":\"anderson\", \"last_name\":\"monken\"}\n{\"id\":3, \"name\":\"amit\", \"last_name\":\"arora\"}\n{\"id\":4, \"name\":\"abhijit\", \"last_name\":\"dasgupta\"}\n\nJSON\n4 records enclosed in 1 JSON Array\n[\n  {\"id\":1, \"name\":\"marck\", \"last_name\":\"vaisman\"},\n  {\"id\":2, \"name\":\"anderson\", \"last_name\":\"monken\"},\n  {\"id\":3, \"name\":\"amit\", \"last_name\":\"arora\"},\n  {\"id\":4, \"name\":\"abhijit\", \"last_name\":\"dasgupta\"},\n]",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#binary-files",
    "href": "w04/index.html#binary-files",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Binary Files",
    "text": "Binary Files",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#issues-with-common-file-formats-especially-csv",
    "href": "w04/index.html#issues-with-common-file-formats-especially-csv",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Issues with Common File Formats (Especially CSV)",
    "text": "Issues with Common File Formats (Especially CSV)\n\nUbiquitous but highly error-prone\nDefault delimiter: familiar character in English, the comma\nAmbiguities:\n\nDelimiter (comma, tab, semi-colon, custom)\nQuote characters (single or doble quote)\nEscaping to appropriately handle string data\n\nDoesn‚Äôt natively encode schema information\nNo direct support for nested structures\nEncoding+schema must be configured on target system to ensure ingestion\nAutodetection provided in many cloud environments but is inappropriate for production ingestion (can be painfully slow)\nData engineers often forced to work with CSV data and then build robust exception handling and error detection to ensure data quality (Pydantic!)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#introducing-apache-parquet",
    "href": "w04/index.html#introducing-apache-parquet",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Introducing Apache Parquet",
    "text": "Introducing Apache Parquet\n\nFree, open-source, column-oriented data storage format created by Twitter and Cloudera (v1.0 released July 2013)\nData stored in columnar format (as opposed to row format), designed for read and write performance\nBuilds in schema information and natively supports nested data\nSupported by R and Python through Apache Arrow (more coming up!)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#traditional-row-store",
    "href": "w04/index.html#traditional-row-store",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Traditional Row-Store",
    "text": "Traditional Row-Store\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuery: ‚ÄúHow many balls did we sell?\nThe engine must scan each and every row until the end!",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#column-store",
    "href": "w04/index.html#column-store",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Column-Store",
    "text": "Column-Store\n\n\n\n\n\n\n\nParquet file format, everything you need to know",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#row-groups",
    "href": "w04/index.html#row-groups",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Row Groups",
    "text": "Row Groups\n\n\n\nData is stored in row groups!\n\n\n\nOnly required fields",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#metadata-compression-and-dictionary-encoding",
    "href": "w04/index.html#metadata-compression-and-dictionary-encoding",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Metadata, compression, and dictionary encoding",
    "text": "Metadata, compression, and dictionary encoding",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#apache-arrow-for-in-memory-analytics",
    "href": "w04/index.html#apache-arrow-for-in-memory-analytics",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Apache Arrow for In-Memory Analytics",
    "text": "Apache Arrow for In-Memory Analytics\n\nApache Arrow is a development platform for in-memory analytics. It contains a set of technologies that enable big data systems to process and move data fast. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. (Topol and McKinney 2024)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#before-arrow",
    "href": "w04/index.html#before-arrow",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Before Arrow",
    "text": "Before Arrow\n\n\n\nTopol and McKinney (2024)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#after-arrow",
    "href": "w04/index.html#after-arrow",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "After Arrow",
    "text": "After Arrow\n\n\n\nTopol and McKinney (2024)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#arrow-compatibility",
    "href": "w04/index.html#arrow-compatibility",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Arrow Compatibility",
    "text": "Arrow Compatibility\n\n\n\nTopol and McKinney (2024)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#arrow-performance",
    "href": "w04/index.html#arrow-performance",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Arrow Performance",
    "text": "Arrow Performance\n\n\n\n\n\nTopol and McKinney (2024)\n\n\n\n\n\n\nTopol and McKinney (2024)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#using-arrow-with-csv-and-parquet",
    "href": "w04/index.html#using-arrow-with-csv-and-parquet",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Using Arrow with CSV and Parquet",
    "text": "Using Arrow with CSV and Parquet\n\n\n\nPython\n\npyarrow or from pandas\nimport pandas as pd\npd.read_csv(engine = 'pyarrow')\npd.read_parquet()\n\nimport pyarrow.csv\npyarrow.csv.read_csv()\n\nimport pyarrow.parquet\npyarrow.parquet.read_table()\n\n\nR\n\nUse the arrow package\nlibrary(arrow)\n\nread_csv_arrow()\nread_parquet()\nread_json_arrow()\n\nwrite_csv_arrow()\nwrite_parquet()\n\n\nRecommendation: save your intermediate and analytical datasets as Parquet!",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#before-we-begin",
    "href": "w04/index.html#before-we-begin",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Before We Begin‚Ä¶",
    "text": "Before We Begin‚Ä¶\n\nPandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\n\nPandas is slow, but less slow if you use it the right way!\n\nApache Arrow and the ‚Äú10 Things I Hate About pandas‚Äù (A 2017 post from the creator of Pandas‚Ä¶))\n50x faster data loading in Pandas: no problem (an old 2019 article‚Ä¶)\nIs Pandas really that slow?\nPandas 2.0 and the arrow revolution",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#polars-1",
    "href": "w04/index.html#polars-1",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Polars",
    "text": "Polars",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#why-is-polars-faster-than-pandas",
    "href": "w04/index.html#why-is-polars-faster-than-pandas",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Why is Polars Faster than Pandas?",
    "text": "Why is Polars Faster than Pandas?\n\nPolars is written in Rust. Rust is compiled; Python is interpreted\n\nCompiled language: you generate the machine code only once then run it, subsequent runs do not need the compilation step.\nInterpreted language: code has to be parsed, interpreted and converted into machine code every single time.\n\nParallelization: Vectorized operations can be executed in parallel on multiple cores\nLazy evaluation: Polars supports two APIs lazy as well as eager evaluation (used by pandas). In lazy evaluation, a query is executed only when required. While in eager evaluation, a query is executed immediately.\nPolars uses Arrow for in-memory data representation. Similar to how pandas uses NumPy (Pandas 2 allows using Arrow as backend)\nPolars \\(\\approx\\) in-memory DataFrame library + query optimizer\n\n\n\n[Excerpt from this post from Ritchie Vink, author of Polars] Arrow provides the efficient data structures and some compute kernels, like a SUM, a FILTER, a MAX etc. Arrow is not a query engine. Polars is a DataFrame library on top of arrow that has implemented efficient algorithms for JOINS, GROUPBY, PIVOTs, MELTs, QUERY OPTIMIZATION, etc. (the things you expect from a DF lib).",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#ease-of-use",
    "href": "w04/index.html#ease-of-use",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Ease of Use",
    "text": "Ease of Use\n\nFamiliar API for users of Pandas: differences in syntax but still a Dataframe API making it straightforward to perform common operations such as filtering, aggregating, and joining data\nSee Migrating from Pandas\nReading data\n# must install s3fs -&gt; \"pip install s3fs\"\n# Using Polars\nimport polars as pl\npolars_df = pl.read_parquet(\"s3://nyc-tlc/trip data/yellow_tripdata_2023-06.parquet\")\n\n# using Pandas\nimport pandas as pd\npandas_df = pd.read_parquet(\"s3://nyc-tlc/trip data/yellow_tripdata_2023-06.parquet\")\nSelecting columns (see Pushdown optimization)\n# Using Polars\nselected_columns_polars = polars_df[['column1', 'column2']]\n\n# Using Pandas\nselected_columns_pandas = pandas_df[['column1', 'column2']]",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#ease-of-use-contd.",
    "href": "w04/index.html#ease-of-use-contd.",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Ease of Use (contd.)",
    "text": "Ease of Use (contd.)\n\nFiltering data\n# Using Polars\nfiltered_polars = polars_df[polars_df['column1'] &gt; 10]\n\n# Using Pandas\nfiltered_pandas = pandas_df[pandas_df['column1'] &gt; 10]\nThough you can write Polars code that looks like Pandas, better to write idiomatic Polars code that takes advantage of Polars‚Äô features\nMigrating from Apache Spark: Whereas Spark DataFrame is a collection of rows, Polars DataFrame is closer to a collection of columns",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#installation-data-loading-and-basic-operations",
    "href": "w04/index.html#installation-data-loading-and-basic-operations",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Installation, Data Loading, and Basic Operations",
    "text": "Installation, Data Loading, and Basic Operations\nInstall via pip:\npip install polars\nImport polars in your Python code and read data as usual:\nimport polars as pl\ndf = pl.read_parquet(\"s3://nyc-tlc/trip data/yellow_tripdata_2023-06.parquet\")\ndf.head()\n\nshape: (5, 19)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ VendorID ‚îÜ tpep_picku ‚îÜ tpep_dropoff ‚îÜ passenger_co ‚îÜ ... ‚îÜ improvement_ ‚îÜ total_amount ‚îÜ congestion_s ‚îÜ Airport_fee ‚îÇ\n‚îÇ ---      ‚îÜ p_datetime ‚îÜ _datetime    ‚îÜ unt          ‚îÜ     ‚îÜ surcharge    ‚îÜ ---          ‚îÜ urcharge     ‚îÜ ---         ‚îÇ\n‚îÇ i32      ‚îÜ ---        ‚îÜ ---          ‚îÜ ---          ‚îÜ     ‚îÜ ---          ‚îÜ f64          ‚îÜ ---          ‚îÜ f64         ‚îÇ\n‚îÇ          ‚îÜ datetime[n ‚îÜ datetime[ns] ‚îÜ i64          ‚îÜ     ‚îÜ f64          ‚îÜ              ‚îÜ f64          ‚îÜ             ‚îÇ\n‚îÇ          ‚îÜ s]         ‚îÜ              ‚îÜ              ‚îÜ     ‚îÜ              ‚îÜ              ‚îÜ              ‚îÜ             ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ 1        ‚îÜ 2023-06-01 ‚îÜ 2023-06-01   ‚îÜ 1            ‚îÜ ... ‚îÜ 1.0          ‚îÜ 33.6         ‚îÜ 2.5          ‚îÜ 0.0         ‚îÇ\n‚îÇ          ‚îÜ 00:08:48   ‚îÜ 00:29:41     ‚îÜ              ‚îÜ     ‚îÜ              ‚îÜ              ‚îÜ              ‚îÜ             ‚îÇ\n‚îÇ 1        ‚îÜ 2023-06-01 ‚îÜ 2023-06-01   ‚îÜ 0            ‚îÜ ... ‚îÜ 1.0          ‚îÜ 23.6         ‚îÜ 2.5          ‚îÜ 0.0         ‚îÇ\n‚îÇ          ‚îÜ 00:15:04   ‚îÜ 00:25:18     ‚îÜ              ‚îÜ     ‚îÜ              ‚îÜ              ‚îÜ              ‚îÜ             ‚îÇ\n‚îÇ 1        ‚îÜ 2023-06-01 ‚îÜ 2023-06-01   ‚îÜ 1            ‚îÜ ... ‚îÜ 1.0          ‚îÜ 60.05        ‚îÜ 0.0          ‚îÜ 1.75        ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#polars-pipeline-example",
    "href": "w04/index.html#polars-pipeline-example",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Polars Pipeline Example",
    "text": "Polars Pipeline Example\nWe‚Äôll run this as part of the lab in a little bit; think how you might code this in Pandas‚Ä¶\n\n\n\nPolars pipeline",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#further-reading",
    "href": "w04/index.html#further-reading",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Further reading",
    "text": "Further reading\n\nPolars\nUser guide ‚Üê MUST READ\nPolars GitHub repo\nPandas Vs Polars: a syntax and speed comparison\nTips & tricks for working with strings in Polars",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#duckdb-1",
    "href": "w04/index.html#duckdb-1",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "DuckDB",
    "text": "DuckDB\n\nDuckDB is an in-process SQL OLAP DB management system\nLike sqlite, but for analytics. What does this mean? It means that your database runs inside your process, there are no servers to manage, no remote system to connect to. Easy to experiment with SQL-like syntax.\nVectorized processing: Loads chunks of data into memory (tries to keep everything in the CPU‚Äôs L1 and L2 cache) and is thus able to handle datasets bigger than the amount of RAM available.\nSupports Python, R and a host of other languages\nImportant paper on DuckDB: Raasveldt and M√ºhleisen (2019)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#key-features",
    "href": "w04/index.html#key-features",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Key Features",
    "text": "Key Features\n\nColumnar Storage, Vectorized Query Processing: DuckDB contains a columnar-vectorized query execution engine, where queries are run on a large batch of values (a ‚Äúvector‚Äù) in one operation\n\nMost analytical queries (think group by and summarize) or even data retrieval for training ML models require retrieving a subset of columns and now the entire row, columnar storage make this faster\n\nIn-Memory Processing: All data needed for processing is brought within the process memory (recall that columnar storage format helps with this) making queries run faster (no DB call over the network)\nSQL Support: highly Postgres-compatible version of SQL1.\nACID Compliance: Transactional guarantees (ACID properties) through bulk-optimized Multi-Version Concurrency Control (MVCC).",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#use-cases-for-duckdb",
    "href": "w04/index.html#use-cases-for-duckdb",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Use Cases for DuckDB",
    "text": "Use Cases for DuckDB\n\nData Warehousing\nBusiness Intelligence\nReal-Time Analytics\nIoT Data Processing\n\nDuckDB in the Wild\n\nHow We Silently Switched Mode‚Äôs In-Memory Data Engine to DuckDB To Boost Visual Data Exploration Speed\nWhy we built Rill with DuckDB\nLeveraging DuckDB for enhanced performance in dbt projects",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#duckdb-diy",
    "href": "w04/index.html#duckdb-diy",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "DuckDB: DIY",
    "text": "DuckDB: DIY\n\n\n\nDatalake and DuckDB\n\n\nUsing DuckDB in AWS Lambda",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#duckdb-fully-managed",
    "href": "w04/index.html#duckdb-fully-managed",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "DuckDB: Fully-Managed",
    "text": "DuckDB: Fully-Managed\n\n\n\nMotherDuck Architecture\n\n\n\nArchitecture and capabilities\nSeamlessly analyze data, whether it sits on your laptop, in the cloud or split between.\nHybrid execution automatically plans each part of your query and determines where it‚Äôs best computed\nDuckDB Vs MotherDuck",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#setting-up-duckdb",
    "href": "w04/index.html#setting-up-duckdb",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Setting Up DuckDB",
    "text": "Setting Up DuckDB\nConfiguration and Initialization: DuckDB is integrated into Python and R for efficient interactive data analysis (APIs for Java, C, C++, Julia, Swift, and others)\npip install duckdb\nConnecting to DuckDB\nimport duckdb\n# directly query a Pandas DataFrame\nimport pandas as pd\ndata_url = \"https://raw.githubusercontent.com/anly503/datasets/main/EconomistData.csv\"\ndf = pd.read_csv(data_url)\nduckdb.sql('SELECT * FROM df')",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#setting-up-duckdb-contd.",
    "href": "w04/index.html#setting-up-duckdb-contd.",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Setting Up DuckDB (contd.)",
    "text": "Setting Up DuckDB (contd.)\nSupported data formats: DuckDB can ingest data from a wide variety of formats ‚Äì both on-disk and in-memory. See the data ingestion page for more information.\nimport duckdb\nduckdb.read_csv('example.csv')                # read a CSV file into a Relation\nduckdb.read_parquet('example.parquet')        # read a Parquet file into a Relation\nduckdb.read_json('example.json')              # read a JSON file into a Relation\n\nduckdb.sql('SELECT * FROM \"example.csv\"')     # directly query a CSV file\nduckdb.sql('SELECT * FROM \"example.parquet\"') # directly query a Parquet file\nduckdb.sql('SELECT * FROM \"example.json\"')    # directly query a JSON file",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#querying-duckdb",
    "href": "w04/index.html#querying-duckdb",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Querying DuckDB",
    "text": "Querying DuckDB\nEssential Reading\n\nFriendlier SQL with DuckDB\nSQL Introduction\n\nBasic SQL Queries\nimport duckdb\nimport pandas as pd\nbabynames = pd.read_parquet(\"https://github.com/anly503/datasets/raw/main/babynames.parquet.zstd\")\nduckdb.sql(\"select count(*)  from babynames where Name='John'\")\nAggregations and Grouping\nduckdb.sql(\"select State, Name, count(*) as count  from babynames group by State, Name order by State desc, count desc\")",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#querying-duckdb-contd.",
    "href": "w04/index.html#querying-duckdb-contd.",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Querying DuckDB (contd.)",
    "text": "Querying DuckDB (contd.)\nEssential reading: FROM and JOIN clauses\nJoins and Subqueries\n# Join two tables together\nduckdb.sql(\"SELECT * FROM table_name JOIN other_table ON (table_name.key = other_table.key\")\nWindow Functions\npowerplants = pd.read_csv(\"https://raw.githubusercontent.com/anly503/datasets/main/powerplants.csv\", parse_dates=[\"date\"])\nq = \"\"\"\nSELECT \"plant\", \"date\",\n    AVG(\"MWh\") OVER (\n        PARTITION BY \"plant\"\n        ORDER BY \"date\" ASC\n        RANGE BETWEEN INTERVAL 3 DAYS PRECEDING\n                  AND INTERVAL 3 DAYS FOLLOWING)\n        AS \"MWh 7-day Moving Average\"\nFROM powerplants\nORDER BY 1, 2;\n\"\"\"\nduckdb.sql(q)",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#using-the-duckdb-cli-and-shell",
    "href": "w04/index.html#using-the-duckdb-cli-and-shell",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Using the DuckDB CLI and Shell",
    "text": "Using the DuckDB CLI and Shell\n\nInstall DuckDB CLI or use in browser via shell.duckdb.org for data exploration via SQL; Once installed, import a local file into the shell and run queries\nYou can download powerplants.csv here\nC:\\Users\\&lt;username&gt;\\Downloads\\duckdb_cli-windows-amd64&gt; duckdb\nv0.8.1 6536a77232\nEnter \".help\" for usage hints.\nConnected to a transient in-memory database.\nUse \".open FILENAME\" to reopen on a persistent database.\nD CREATE TABLE powerplants AS SELECT * FROM read_csv_auto('powerplants.csv');\nD DESCRIBE powerplants;\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ column_name ‚îÇ column_type ‚îÇ  null   ‚îÇ   key   ‚îÇ default ‚îÇ extra ‚îÇ\n‚îÇ   varchar   ‚îÇ   varchar   ‚îÇ varchar ‚îÇ varchar ‚îÇ varchar ‚îÇ int32 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ plant       ‚îÇ VARCHAR     ‚îÇ YES     ‚îÇ         ‚îÇ         ‚îÇ       ‚îÇ\n‚îÇ date        ‚îÇ DATE        ‚îÇ YES     ‚îÇ         ‚îÇ         ‚îÇ       ‚îÇ\n‚îÇ MWh         ‚îÇ BIGINT      ‚îÇ YES     ‚îÇ         ‚îÇ         ‚îÇ       ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\nD  SELECT * from powerplants where plant='Boston' and date='2019-01-02';\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  plant  ‚îÇ    date    ‚îÇ  MWh   ‚îÇ\n‚îÇ varchar ‚îÇ    date    ‚îÇ int64  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ Boston  ‚îÇ 2019-01-02 ‚îÇ 564337 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#profiling-in-duckdb",
    "href": "w04/index.html#profiling-in-duckdb",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Profiling in DuckDB",
    "text": "Profiling in DuckDB\nQuery Optimization: Use EXPLAIN and ANALYZE keywords to understand how your query is being executed and the time being spent in individual steps\nD EXPLAIN ANALYZE SELECT * from powerplants where plant='Boston' and date='2019-01-02';\nDuckDB will use all the cores available on the underlying compute, but you can adjust this (Full configuration details here)\nD select current_setting('threads');\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ current_setting('threads') ‚îÇ\n‚îÇ           int64            ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ                          8 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\nD SET threads=4;\nD select current_setting('threads');\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ current_setting('threads') ‚îÇ\n‚îÇ           int64            ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ                          4 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#benchmarks-and-comparisons",
    "href": "w04/index.html#benchmarks-and-comparisons",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Benchmarks and Comparisons",
    "text": "Benchmarks and Comparisons\n\n\n\n\nTricky topic! Can make your chosen solution look better by focusing on metrics on which it provides better results\nIn general, TPC-H and TPC-DS are considered the standard benchmarks for data processing.\n\n\n\n\n\n\nTPC-DS Homepage",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#further-reading-on-duckdb",
    "href": "w04/index.html#further-reading-on-duckdb",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Further Reading on DuckDB",
    "text": "Further Reading on DuckDB\n\nParallel Grouped Aggregation in DuckDB\nMeta queries\nProfiling queries in DuckDB\nDuckDB tutorial for beginners\nDuckDB CLI API\nUsing DuckDB in AWS Lambda\nRevisiting the Poor Man‚Äôs Data Lake with MotherDuck\nSupercharge your data processing with DuckDB\nFriendlier SQL with DuckDB\nBuilding and deploying data apps with DuckDB and Streamlit",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#references",
    "href": "w04/index.html#references",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "References",
    "text": "References\n\n\nGopalan, Rukmani. 2022. The Cloud Data Lake: A Guide to Building Robust Cloud Data Architecture. O‚ÄôReilly Media, Inc. https://www.dropbox.com/scl/fi/7u469ccc8y169us5hydk0/The-cloud-data-lake-a-guide-to-building-robust-cloud-data-Rukmani-Gopalan.pdf?rlkey=ey06a6zt9g90d4zq8tfncndta&dl=1.\n\n\nRaasveldt, Mark, and Hannes M√ºhleisen. 2019. ‚ÄúDuckDB: An Embeddable Analytical Database.‚Äù In Proceedings of the 2019 International Conference on Management of Data, 1981‚Äì84. SIGMOD ‚Äô19. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3299869.3320212.\n\n\nReis, Joe, and Matt Housley. 2022. Fundamentals of Data Engineering: Plan and Build Robust Data Systems. O‚ÄôReilly Media, Inc.\n\n\nTopol, Matthew, and Wes McKinney. 2024. In-Memory Analytics with Apache Arrow. Packt Publishing Ltd.\n\n\nWhite, Tom E. 2015. Hadoop: The Definitive Guide. O‚ÄôReilly Media, Inc.",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w04/index.html#footnotes",
    "href": "w04/index.html#footnotes",
    "title": "Week 4: DuckDB, Polars, File Formats",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFriendlier SQL with DuckDB‚Ü©Ô∏é",
    "crumbs": [
      "Week 4: <span class='sec-w04-date'>Sep 15</span>"
    ]
  },
  {
    "objectID": "w03/index.html",
    "href": "w03/index.html",
    "title": "Week 3: Parallelization Concepts",
    "section": "",
    "text": "Open slides in new tab ‚Üí",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#looking-back",
    "href": "w03/index.html#looking-back",
    "title": "Week 3: Parallelization Concepts",
    "section": "Looking back",
    "text": "Looking back\n\nContinued great use of Slack!\n\nNice interactions\n\nDue date reminders:\n\nAssignment 2: September 17, 2025\nLab 3: September 17, 2025\nAssignment 3: September 24, 2025",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#glossary",
    "href": "w03/index.html#glossary",
    "title": "Week 3: Parallelization Concepts",
    "section": "Glossary",
    "text": "Glossary\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nLocal\nYour current workstation (laptop, desktop, etc.), wherever you start the terminal/console application.\n\n\nRemote\nAny machine you connect to via ssh or other means.\n\n\nEC2\nSingle virtual machine in the cloud where you can run computation\n\n\nSageMaker\nIntegrated Developer Environment where you can conduct data science on single machines or distributed training\n\n\nGPU\nGraphics Processing Unit - specialized hardware for parallel computation, essential for AI/ML\n\n\nTPU\nTensor Processing Unit - Google‚Äôs custom AI accelerator chips\n\n\nEphemeral\nLasting for a short time - any machine that will get turned off or place you will lose data\n\n\nPersistent\nLasting for a long time - any environment where your work is NOT lost when the timer goes off",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#quick-survey-question-for-intuition-building",
    "href": "w03/index.html#quick-survey-question-for-intuition-building",
    "title": "Week 3: Parallelization Concepts",
    "section": "Quick Survey Question, for Intuition-Building",
    "text": "Quick Survey Question, for Intuition-Building\n\nAre humans capable of ‚Äútrue‚Äù multi-tasking?\n\nAs in, doing two things at the exact same time?\n\n(Or, do we instead rapidly switch back and forth between tasks?)",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#the-answer",
    "href": "w03/index.html#the-answer",
    "title": "Week 3: Parallelization Concepts",
    "section": "The Answer",
    "text": "The Answer\n\n(From what we understand, at the moment, by way of studies in neuroscience/cognitive science/etc‚Ä¶)\nHumans are not capable of true multitasking! In CS terms, this would be called multiprocessing (more on this later)\nWe are capable, however, of various modes of concurrency!\n\n\n\n\n\n\n\n\n\n\nMultithreading\nAsynchronous Execution\n\n\n\n\nUnconsciously(you do it already, ‚Äúnaturally‚Äù)\nFocus on one speaker within a loud room, with tons of other conversations entering your ears\nPut something in oven, set alarm, go do something else, take out of oven once alarm goes off\n\n\nConsciously(you can do it with effort/practice)\nPat head (up and down) and rub stomach (circular motion) ‚Äúsimultaneously‚Äù\nThrow a ball in the air, clap 3 times, catch ball",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#helpful-specifically-for-programming",
    "href": "w03/index.html#helpful-specifically-for-programming",
    "title": "Week 3: Parallelization Concepts",
    "section": "Helpful Specifically for Programming",
    "text": "Helpful Specifically for Programming\n\nCourse notes from MIT‚Äôs class on parallel computing phrases it like: if implemented thoughtfully, concurrency is a power multiplier for your code (do 10 things in 1 second instead of 10 seconds‚Ä¶)",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#helpful-in-general-as-a-way-of-thinking",
    "href": "w03/index.html#helpful-in-general-as-a-way-of-thinking",
    "title": "Week 3: Parallelization Concepts",
    "section": "Helpful In General as a Way of Thinking!",
    "text": "Helpful In General as a Way of Thinking!\n\nSay you get hired as a Project Manager‚Ä¶\nPart of your job will fundamentally involve pipelines!\n\nNeed to know when Task \\(B\\) does/does not require Task \\(A\\) as a prerequisite\nNeed to know whether Task \\(A\\) and Task \\(B\\) can share one resource or need their own individual resources\nOnce Task \\(A\\) and \\(B\\) both complete, how do we merge their results together?",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#avoiding-the-rabbithole",
    "href": "w03/index.html#avoiding-the-rabbithole",
    "title": "Week 3: Parallelization Concepts",
    "section": "Avoiding the Rabbithole",
    "text": "Avoiding the Rabbithole\n\nParallel computing is a rabbithole, but one you can safely avoid via simple heuristics (‚Äúrules of thumb‚Äù)!\n\n\nCheck for optimizations to serial code first,\nCheck for embarrassingly parallel code blocks\nUse map-reduce approach for more complicated cases",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#typical-real-world-scenarios",
    "href": "w03/index.html#typical-real-world-scenarios",
    "title": "Week 3: Parallelization Concepts",
    "section": "Typical Real-World Scenarios",
    "text": "Typical Real-World Scenarios\n\nYou need to prepare training data for LLMs by cleaning and deduplicating 100TB of web-scraped text\nYou are building a RAG system that requires embedding and indexing millions of documents in parallel\nYou need to extract structured data from millions of PDFs using vision models for document AI\nYou are preprocessing multimodal datasets with billions of image-text pairs for foundation model training\nYou need to run quality filtering on petabytes of Common Crawl data for training dataset\nYou are generating synthetic training data using LLMs to augment limited real-world datasets\nYou need to transform and tokenize text across 100+ languages for multilingual AI\nYou are building real-time data pipelines that process streaming data for online learning",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#embarrassingly-parallel-pipelines",
    "href": "w03/index.html#embarrassingly-parallel-pipelines",
    "title": "Week 3: Parallelization Concepts",
    "section": "‚ÄúEmbarrassingly Parallel‚Äù Pipelines",
    "text": "‚ÄúEmbarrassingly Parallel‚Äù Pipelines\n\nTechnical definition: tasks within pipeline can easily be parallelized bc no dependence and no need for communication (triple spatula!)",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#parallelizing-non-embarrassingly-parallel-pipelines",
    "href": "w03/index.html#parallelizing-non-embarrassingly-parallel-pipelines",
    "title": "Week 3: Parallelization Concepts",
    "section": "Parallelizing Non-Embarrassingly-Parallel Pipelines",
    "text": "Parallelizing Non-Embarrassingly-Parallel Pipelines",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#buzzkill-complications-to-come",
    "href": "w03/index.html#buzzkill-complications-to-come",
    "title": "Week 3: Parallelization Concepts",
    "section": "Buzzkill: Complications to Come üò∞",
    "text": "Buzzkill: Complications to Come üò∞\n\nIf it‚Äôs such a magical powerup, shouldn‚Äôt we just parallelize everything? Answer: No üòû bc overhead.\nOverhead source 1: Sending tasks to workers (processors), collecting results\nOverhead source 2: Even after setting up new stacks and heaps, threads may need to communicate with each other (e.g.¬†if they need to synchronize at some point(s))\nIn fact, probably the earliest super-popular parallelization library was created to handle Source 2, not Source 1: Message Passing Interface (C, C++, and Fortran)",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#rules-of-thumb-for-parallelization",
    "href": "w03/index.html#rules-of-thumb-for-parallelization",
    "title": "Week 3: Parallelization Concepts",
    "section": "Rules of Thumb for Parallelization",
    "text": "Rules of Thumb for Parallelization\n\n\nYes - Parallelize These\n\nData Preparation:\n\nText extraction from documents\nTokenization of text corpora\nImage preprocessing\nEmbedding generation for documents\nData quality filtering and validation\nFormat conversions (audio features)\nWeb scraping and data collection\nSynthetic data generation\n\nData Processing:\n\nBatch inference on datasets\nFeature extraction at scale\nData deduplication (local)\n\n\n\n\nNo - Keep Sequential\n\nOrder-Dependent:\n\nConversation threading\nTime-series preprocessing\nSequential data validation\nCumulative statistics\n\nGlobal Operations:\n\nGlobal deduplication\nCross-dataset joins\nComputing exact quantiles\n\n\n\n\n\n\n\n\nFor data operations in the ‚ÄúNo‚Äù column, they often require global coordination or maintain strict ordering. However, many can be approximated with parallel algorithms (like approximate deduplication with locality-sensitive hashing)",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#in-action",
    "href": "w03/index.html#in-action",
    "title": "Week 3: Parallelization Concepts",
    "section": "In Action",
    "text": "In Action\n\nimport time\nfrom sympy.ntheory import factorint\nfrom joblib import Parallel, delayed\nparallel_runner = Parallel(n_jobs=4)\nstart, end = 500, 580\ndef find_prime_factors(num):\n  time.sleep(.01)\n  return factorint(num, multiple=True)\ndisp_time = lambda start, end: print('{:.4f} s'.format(end - start))\n\n\n\n\nserial_start = time.time()\nresult = [\n  (i,find_prime_factors(i))\n  for i in range(start, end+1)\n]\nserial_end = time.time()\ndisp_time(serial_start, serial_end)\n\n0.9834 s\n\n\n\n\npar_start = time.time()\nresult = parallel_runner(\n  delayed(find_prime_factors)(i)\n  for i in range(start, end+1)\n)\npar_end = time.time()\ndisp_time(par_start, par_end)\n\n2.7042 s",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#what-happens-when-not-embarrassingly-parallel",
    "href": "w03/index.html#what-happens-when-not-embarrassingly-parallel",
    "title": "Week 3: Parallelization Concepts",
    "section": "What Happens When Not Embarrassingly Parallel?",
    "text": "What Happens When Not Embarrassingly Parallel?\n\nThink of the difference between linear and quadratic equations in algebra:\n\\(3x - 1 = 0\\) is ‚Äúembarrassingly‚Äù solvable, on its own: you can solve it directly, by adding 3 to both sides \\(\\implies x = \\frac{1}{3}\\). Same for \\(2x + 3 = 0 \\implies x = -\\frac{3}{2}\\)\nNow consider \\(6x^2 + 7x - 3 = 0\\): Harder to solve ‚Äúdirectly‚Äù, so your instinct might be to turn to the laborious quadratic equation:\n\n\\[\n\\begin{align*}\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} = \\frac{-7 \\pm \\sqrt{49 - 4(6)(-3)}}{2(6)} = \\frac{-7 \\pm 11}{12} = \\left\\{\\frac{1}{3},-\\frac{3}{2}\\right\\}\n\\end{align*}\n\\]\n\nAnd yet, \\(6x^2 + 7x - 3 = (3x - 1)(2x + 3)\\), meaning that we could have split the problem into two ‚Äúembarrassingly‚Äù solvable pieces, then multiplied to get result!",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#the-analogy-to-map-reduce",
    "href": "w03/index.html#the-analogy-to-map-reduce",
    "title": "Week 3: Parallelization Concepts",
    "section": "The Analogy to Map-Reduce",
    "text": "The Analogy to Map-Reduce\n\n\n\n\n\n\n\n\\(\\leadsto\\) If code is not embarrassingly parallel (instinctually requiring laborious serial execution),\n\\(\\underbrace{6x^2 + 7x - 3 = 0}_{\\text{Solve using Quadratic Eqn}}\\)\n\n\nBut can be split into‚Ä¶\n\\((3x - 1)(2x + 3) = 0\\)\n\n\nEmbarrassingly parallel pieces which combine to same result,\n\\(\\underbrace{3x - 1 = 0}_{\\text{Solve directly}}, \\underbrace{2x + 3 = 0}_{\\text{Solve directly}}\\)\n\n\nWe can use map-reduce to achieve ultra speedup (running ‚Äúpieces‚Äù on GPU!)\n\\(\\underbrace{(3x-1)(2x+3) = 0}_{\\text{Solutions satisfy this product}}\\)",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#the-direct-analogy-map-reduce",
    "href": "w03/index.html#the-direct-analogy-map-reduce",
    "title": "Week 3: Parallelization Concepts",
    "section": "The Direct Analogy: Map-Reduce!",
    "text": "The Direct Analogy: Map-Reduce!\n\nProblem from DSAN 5000/5100: Computing SSR (Sum of Squared Residuals)\n\\(y = (1,3,2), \\widehat{y} = (2, 5, 0) \\implies \\text{SSR} = (1-2)^2 + (3-5)^2 + (2-0)^2 = 9\\)\nComputing pieces separately:\nmap(do_something_with_piece, list_of_pieces)\n\nmy_map = map(lambda input: input**2, [(1-2), (3-5), (2-0)])\nmap_result = list(my_map)\nmap_result\n\n[1, 4, 4]\n\n\nCombining solved pieces\nreduce(how_to_combine_pair_of_pieces, pieces_to_combine)\n\nfrom functools import reduce\nmy_reduce = reduce(lambda piece1, piece2: piece1 + piece2, map_result)\nmy_reduce\n\n9",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#functions-vs.-functionals",
    "href": "w03/index.html#functions-vs.-functionals",
    "title": "Week 3: Parallelization Concepts",
    "section": "Functions vs.¬†Functionals",
    "text": "Functions vs.¬†Functionals\nYou may have noticed: map() and reduce() are ‚Äúmeta-functions‚Äù: functions that take other functions as inputs\n\n\n\ndef add_5(num):\n  return num + 5\nadd_5(10)\n\n15\n\n\n\n\ndef apply_twice(fn, arg):\n  return fn(fn(arg))\napply_twice(add_5, 10)\n\n20\n\n\n\n\nIn Python, functions can be used as vars (Hence lambda):\n\nadd_5 = lambda num: num + 5\napply_twice(add_5, 10)\n\n20\n\n\nThis relates to a whole paradigm, ‚Äúfunctional programming‚Äù: mostly outside scope of course, but lots of important+useful takeaways/rules-of-thumb!",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#train-your-brain-for-functional-approach-implies-master-debugging",
    "href": "w03/index.html#train-your-brain-for-functional-approach-implies-master-debugging",
    "title": "Week 3: Parallelization Concepts",
    "section": "Train Your Brain for Functional Approach \\(\\implies\\) Master Debugging!",
    "text": "Train Your Brain for Functional Approach \\(\\implies\\) Master Debugging!\n\nIn CS Theory: enables formal proofs of correctness\nIn CS practice:\n\nWhen a program doesn‚Äôt work, each function is an interface point where you can check that the data are correct. You can look at the intermediate inputs and outputs to quickly isolate the function that‚Äôs responsible for a bug.(from Python‚Äôs ‚ÄúFunctional Programming HowTo‚Äù)",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#code-rightarrow-pipelines-rightarrow-debuggable-pipelines",
    "href": "w03/index.html#code-rightarrow-pipelines-rightarrow-debuggable-pipelines",
    "title": "Week 3: Parallelization Concepts",
    "section": "Code \\(\\rightarrow\\) Pipelines \\(\\rightarrow\\) Debuggable Pipelines",
    "text": "Code \\(\\rightarrow\\) Pipelines \\(\\rightarrow\\) Debuggable Pipelines\n\n\nScenario: Run code, check the output, and‚Ä¶ it‚Äôs wrong üòµ what do you do?\nUsual approach: Read lines one-by-one, figuring out what they do, seeing if something pops out that seems wrong; adding comments like # Convert to lowercase\n\n\n\n\nEasy case: found typo in punctuation removal code. Fix the error, add comment like # Remove punctuation\n Rule 1 of FP: transform these comments into function names\n\n\n\nHard case: Something in load_text() modifies a variable that later on breaks remove_punct() (Called a side-effect)\n Rule 2 of FP: NO SIDE-EFFECTS!\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\ninput\nin.txt\n\n\n\nload_text\nload_text\n(Verb)\n\n\n\n\ninput-&gt;load_text\n\n\n\n\n\nlowercase\nlowercase\n(Verb)\n\n\n\n\nload_text-&gt;lowercase\n\n\nüßê ‚úÖ\n\n\n\nremove_punct\nremove_punct\n(Verb)\n\n\n\n\nlowercase-&gt;remove_punct\n\n\nüßê ‚úÖ\n\n\n\nremove_stopwords\nremove_stopwords\n(Verb)\n\n\n\n\nremove_punct-&gt;remove_stopwords\n\n\nüßê ‚ùå‚ùóÔ∏è\n\n\n\noutput\nout.txt\n\n\n\nremove_stopwords-&gt;output\n\n\n\n\n\n\n (Does this way of diagramming a program look familiar?) \n\n\n\n\nWith side effects: ‚ùå \\(\\implies\\) issue is somewhere earlier in the chain üò©üèÉ‚Äç‚ôÇÔ∏è\nNo side effects: ‚ùå \\(\\implies\\) issue must be in remove_punct()!!! üòé ‚è±Ô∏è = üí∞",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#if-its-so-useful-why-doesnt-everyone-do-it",
    "href": "w03/index.html#if-its-so-useful-why-doesnt-everyone-do-it",
    "title": "Week 3: Parallelization Concepts",
    "section": "If It‚Äôs So Useful, Why Doesn‚Äôt Everyone Do It?",
    "text": "If It‚Äôs So Useful, Why Doesn‚Äôt Everyone Do It?\n\nTrapped in imperative (sequential) coding mode: Path dependency / QWERTY\nWe need to start thinking like this bc, 1000x harder to debug parallel code! So we need to be less ad hoc in how we write+debug, from here on out! üôá‚Äç‚ôÇÔ∏èüôè\n\n\n\n\nFrom Leskovec, Rajaraman, and Ullman (2014)\n\n\n\nThe title relates to a classic Economics joke (the best kind of joke): ‚ÄúAn economist and a CEO are walking down the street, when the CEO points at the ground and tells the economist, ‚Äòlook! A $20 bill on the ground!‚Äô The economist keeps on walking, scoffing at the CEO: ‚Äòdon‚Äôt be silly, if there was a $20 bill on the ground, somebody would have picked it up already‚Äô.‚Äù",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#the-killer-application-matrix-multiplication",
    "href": "w03/index.html#the-killer-application-matrix-multiplication",
    "title": "Week 3: Parallelization Concepts",
    "section": "The ‚ÄúKiller Application‚Äù: Matrix Multiplication",
    "text": "The ‚ÄúKiller Application‚Äù: Matrix Multiplication\n\n(I learned from Jeff Ullman, who did the obnoxious Stanford thing of mentioning in passing how ‚Äútwo previous students in the class did this for a cool final project on web crawling and, well, it escalated quickly‚Äù, aka became Google)\n\n\n\n\nFrom Leskovec, Rajaraman, and Ullman (2014), which is (legally) free online!",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#the-killer-way-to-learn-text-counts",
    "href": "w03/index.html#the-killer-way-to-learn-text-counts",
    "title": "Week 3: Parallelization Concepts",
    "section": "The Killer Way-To-Learn: Text Counts!",
    "text": "The Killer Way-To-Learn: Text Counts!\n\n(2014): Text counts (2.2) \\(\\rightarrow\\) Matrix multiplication (2.3) \\(\\rightarrow \\cdots \\rightarrow\\) PageRank (5.1)\nThe goal: User searches ‚ÄúDenzel Curry‚Äù‚Ä¶ How relevant is a given webpage?\nScenario 1: Entire internet fits on CPU \\(\\implies\\) We can just make a big dict:\n\n\n\n\n\n\n\n\nG\n\n\n\ninternet\n\nScan in O(n):\nToday Denzel Washington\nate a big bowl of Yum's\ncurry. Denzel allegedly\nrubbed his tum and said\n\"yum yum yum\" when he\ntasted today's curry.\n\"Yum! It is me Denzel,\ncurry is my fav!\", he\nexclaimed. According to\nhis friend Steph, curry\nis indeed Denzel's fav.\nWe are live with Del\nCurry in Washington for\na Denzel curry update.\n\n\n\n\nccounts\n\nOverall Counts\n\n('according',1)\n('allegedly',1)\n('ate',1)\n('big',1)\n('bowl',1)\n('curry',6)\n('del',1)\n('denzel',5)\n('exclaimed',1)\n('fav',2)\n('friend',1)\n('indeed',1)\n('live',1)\n('rubbed',1)\n('said',1)\n('steph',1)\n('tasted',1)\n('today',2)\n('tum',1)\n('update',1)\n('washington',2)\n('yum',4)\n\n\n\n\ninternet-&gt;ccounts\n\n\nLoop Over Words",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#if-everything-doesnt-fit-on-cpu",
    "href": "w03/index.html#if-everything-doesnt-fit-on-cpu",
    "title": "Week 3: Parallelization Concepts",
    "section": "If Everything Doesn‚Äôt Fit on CPU‚Ä¶",
    "text": "If Everything Doesn‚Äôt Fit on CPU‚Ä¶\n\n\n\nFrom Cornell Virtual Workshop, ‚ÄúUnderstanding GPU Architecture‚Äù",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#break-problem-into-chunks-for-the-green-bois",
    "href": "w03/index.html#break-problem-into-chunks-for-the-green-bois",
    "title": "Week 3: Parallelization Concepts",
    "section": "Break Problem into Chunks for the Green Bois!",
    "text": "Break Problem into Chunks for the Green Bois!\n\n\n\n\n\n\n\nG\n\n\n\nchunked\n\nChunked Document\n\nToday Denzel Washington\nate a big bowl of Yum's\ncurry. Denzel allegedly\nrubbed his tum and said\n\n\"yum yum yum\" when he\ntasted today's curry.\n\"Yum! It is me Denzel,\ncurry is my fav!\", he\n\nexclaimed. According to\nhis friend Steph, curry\nis indeed Denzel's fav.\nWe are live with Del\nCurry in Washington for\na Denzel curry update.\n\n\n\n\nchcounts\n\nChunked Counts\n\n('today',1)\n('denzel',1)\n...\n('tum',1)\n('said',1)\n\n('yum',1)\n('yum',1)\n('yum',1)\n...\n('fav',1)\n\n('exclaimed',1)\n...\n('del',1)\n('curry',1)\n('washington',1)\n('denzel',1)\n('curry',1)\n('update',1)\n\n\n\n\nchunked:p1-&gt;chcounts:p1\n\n\nO(n/4)\n\n\n\nchunked:p2-&gt;chcounts:p2\n\n\nO(n/4)\n\n\n\nchunked:p3-&gt;chcounts:p3\n\n\nO(n/4)\n\n\n\nchunked:p4-&gt;chcounts:p4\n\n\nO(n/4)\n\n\n\nscounts\n\nHashed Counts\n\n('allegedly',1)\n...\n('curry',1)\n('denzel',2)\n...\n('yum',1)\n\n('curry',2)\n('denzel',1)\n...\n('yum',4)\n\n('according',1)\n('curry',1)\n('del',1)\n('denzel',1)\n...\n('curry',2)\n('denzel',1)\n('update',1)\n('washington',1)\n\n\n\n\nchcounts:p1-&gt;scounts:p1\n\n\nO(n/4)\n\n\n\nchcounts:p2-&gt;scounts:p2\n\n\nO(n/4)\n\n\n\nchcounts:p3-&gt;scounts:p3\n\n\nO(n/4)\n\n\n\nchcounts:p4-&gt;scounts:p4\n\n\nO(n/4)\n\n\n\nccounts\n\nOverall Counts\n('according',1)\n('allegedly',1)\n('ate',1)\n('big',1)\n('bowl',1)\n('curry',6)\n('del',1)\n('denzel',5)\n('exclaimed',1)\n('fav',2)\n('friend',1)\n('indeed',1)\n('live',1)\n('rubbed',1)\n('said',1)\n('steph',1)\n('tasted',1)\n('today',2)\n('tum',1)\n('update',1)\n('washington',2)\n('yum',4)\n\n\n\n\nscounts:p1-&gt;ccounts:p1\n\n\n\n\n\nscounts:p2-&gt;ccounts:p1\n\n\n\n\n\nscounts:p3-&gt;ccounts:p1\n\n\n\n\n\nscounts:p4-&gt;ccounts:p1\n\n\n\n\n\nscounts:p2-&gt;ccounts\n\n\n \n\nMerge in\nO(n)\n\n\n\n\n\n\n\n\n\n\\(\\implies\\) Total = \\(O(3n) = O(n)\\)\nBut also optimized in terms of constants, because of sequential memory reads",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/index.html#references",
    "href": "w03/index.html#references",
    "title": "Week 3: Parallelization Concepts",
    "section": "References",
    "text": "References\n\n\nLeskovec, Jure, Anand Rajaraman, and Jeffrey David Ullman. 2014. Mining of Massive Datasets. Cambridge University Press.",
    "crumbs": [
      "Week 3: <span class='sec-w03-date'>Sep 8</span>"
    ]
  },
  {
    "objectID": "w03/slides.html#looking-back",
    "href": "w03/slides.html#looking-back",
    "title": "Week 3: Parallelization Concepts",
    "section": "Looking back",
    "text": "Looking back\n\nContinued great use of Slack!\n\nNice interactions\n\nDue date reminders:\n\nAssignment 2: September 17, 2025\nLab 3: September 17, 2025\nAssignment 3: September 24, 2025"
  },
  {
    "objectID": "w03/slides.html#glossary",
    "href": "w03/slides.html#glossary",
    "title": "Week 3: Parallelization Concepts",
    "section": "Glossary",
    "text": "Glossary\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nLocal\nYour current workstation (laptop, desktop, etc.), wherever you start the terminal/console application.\n\n\nRemote\nAny machine you connect to via ssh or other means.\n\n\nEC2\nSingle virtual machine in the cloud where you can run computation\n\n\nSageMaker\nIntegrated Developer Environment where you can conduct data science on single machines or distributed training\n\n\nGPU\nGraphics Processing Unit - specialized hardware for parallel computation, essential for AI/ML\n\n\nTPU\nTensor Processing Unit - Google‚Äôs custom AI accelerator chips\n\n\nEphemeral\nLasting for a short time - any machine that will get turned off or place you will lose data\n\n\nPersistent\nLasting for a long time - any environment where your work is NOT lost when the timer goes off"
  },
  {
    "objectID": "w03/slides.html#quick-survey-question-for-intuition-building",
    "href": "w03/slides.html#quick-survey-question-for-intuition-building",
    "title": "Week 3: Parallelization Concepts",
    "section": "Quick Survey Question, for Intuition-Building",
    "text": "Quick Survey Question, for Intuition-Building\n\nAre humans capable of ‚Äútrue‚Äù multi-tasking?\n\nAs in, doing two things at the exact same time?\n\n(Or, do we instead rapidly switch back and forth between tasks?)"
  },
  {
    "objectID": "w03/slides.html#the-answer",
    "href": "w03/slides.html#the-answer",
    "title": "Week 3: Parallelization Concepts",
    "section": "The Answer",
    "text": "The Answer\n\n(From what we understand, at the moment, by way of studies in neuroscience/cognitive science/etc‚Ä¶)\nHumans are not capable of true multitasking! In CS terms, this would be called multiprocessing (more on this later)\nWe are capable, however, of various modes of concurrency!\n\n\n\n\n\n\n\n\n\n\nMultithreading\nAsynchronous Execution\n\n\n\n\nUnconsciously(you do it already, ‚Äúnaturally‚Äù)\nFocus on one speaker within a loud room, with tons of other conversations entering your ears\nPut something in oven, set alarm, go do something else, take out of oven once alarm goes off\n\n\nConsciously(you can do it with effort/practice)\nPat head (up and down) and rub stomach (circular motion) ‚Äúsimultaneously‚Äù\nThrow a ball in the air, clap 3 times, catch ball"
  },
  {
    "objectID": "w03/slides.html#helpful-specifically-for-programming",
    "href": "w03/slides.html#helpful-specifically-for-programming",
    "title": "Week 3: Parallelization Concepts",
    "section": "Helpful Specifically for Programming",
    "text": "Helpful Specifically for Programming\n\nCourse notes from MIT‚Äôs class on parallel computing phrases it like: if implemented thoughtfully, concurrency is a power multiplier for your code (do 10 things in 1 second instead of 10 seconds‚Ä¶)"
  },
  {
    "objectID": "w03/slides.html#helpful-in-general-as-a-way-of-thinking",
    "href": "w03/slides.html#helpful-in-general-as-a-way-of-thinking",
    "title": "Week 3: Parallelization Concepts",
    "section": "Helpful In General as a Way of Thinking!",
    "text": "Helpful In General as a Way of Thinking!\n\nSay you get hired as a Project Manager‚Ä¶\nPart of your job will fundamentally involve pipelines!\n\nNeed to know when Task \\(B\\) does/does not require Task \\(A\\) as a prerequisite\nNeed to know whether Task \\(A\\) and Task \\(B\\) can share one resource or need their own individual resources\nOnce Task \\(A\\) and \\(B\\) both complete, how do we merge their results together?"
  },
  {
    "objectID": "w03/slides.html#avoiding-the-rabbithole",
    "href": "w03/slides.html#avoiding-the-rabbithole",
    "title": "Week 3: Parallelization Concepts",
    "section": "Avoiding the Rabbithole",
    "text": "Avoiding the Rabbithole\n\nParallel computing is a rabbithole, but one you can safely avoid via simple heuristics (‚Äúrules of thumb‚Äù)!\n\n\nCheck for optimizations to serial code first,\nCheck for embarrassingly parallel code blocks\nUse map-reduce approach for more complicated cases"
  },
  {
    "objectID": "w03/slides.html#typical-real-world-scenarios",
    "href": "w03/slides.html#typical-real-world-scenarios",
    "title": "Week 3: Parallelization Concepts",
    "section": "Typical Real-World Scenarios",
    "text": "Typical Real-World Scenarios\n\nYou need to prepare training data for LLMs by cleaning and deduplicating 100TB of web-scraped text\nYou are building a RAG system that requires embedding and indexing millions of documents in parallel\nYou need to extract structured data from millions of PDFs using vision models for document AI\nYou are preprocessing multimodal datasets with billions of image-text pairs for foundation model training\nYou need to run quality filtering on petabytes of Common Crawl data for training dataset\nYou are generating synthetic training data using LLMs to augment limited real-world datasets\nYou need to transform and tokenize text across 100+ languages for multilingual AI\nYou are building real-time data pipelines that process streaming data for online learning"
  },
  {
    "objectID": "w03/slides.html#embarrassingly-parallel-pipelines",
    "href": "w03/slides.html#embarrassingly-parallel-pipelines",
    "title": "Week 3: Parallelization Concepts",
    "section": "‚ÄúEmbarrassingly Parallel‚Äù Pipelines",
    "text": "‚ÄúEmbarrassingly Parallel‚Äù Pipelines\n\nTechnical definition: tasks within pipeline can easily be parallelized bc no dependence and no need for communication (triple spatula!)"
  },
  {
    "objectID": "w03/slides.html#parallelizing-non-embarrassingly-parallel-pipelines",
    "href": "w03/slides.html#parallelizing-non-embarrassingly-parallel-pipelines",
    "title": "Week 3: Parallelization Concepts",
    "section": "Parallelizing Non-Embarrassingly-Parallel Pipelines",
    "text": "Parallelizing Non-Embarrassingly-Parallel Pipelines"
  },
  {
    "objectID": "w03/slides.html#buzzkill-complications-to-come",
    "href": "w03/slides.html#buzzkill-complications-to-come",
    "title": "Week 3: Parallelization Concepts",
    "section": "Buzzkill: Complications to Come üò∞",
    "text": "Buzzkill: Complications to Come üò∞\n\nIf it‚Äôs such a magical powerup, shouldn‚Äôt we just parallelize everything? Answer: No üòû bc overhead.\nOverhead source 1: Sending tasks to workers (processors), collecting results\nOverhead source 2: Even after setting up new stacks and heaps, threads may need to communicate with each other (e.g.¬†if they need to synchronize at some point(s))\nIn fact, probably the earliest super-popular parallelization library was created to handle Source 2, not Source 1: Message Passing Interface (C, C++, and Fortran)"
  },
  {
    "objectID": "w03/slides.html#rules-of-thumb-for-parallelization",
    "href": "w03/slides.html#rules-of-thumb-for-parallelization",
    "title": "Week 3: Parallelization Concepts",
    "section": "Rules of Thumb for Parallelization",
    "text": "Rules of Thumb for Parallelization\n\n\nYes - Parallelize These\n\nData Preparation:\n\nText extraction from documents\nTokenization of text corpora\nImage preprocessing\nEmbedding generation for documents\nData quality filtering and validation\nFormat conversions (audio features)\nWeb scraping and data collection\nSynthetic data generation\n\nData Processing:\n\nBatch inference on datasets\nFeature extraction at scale\nData deduplication (local)\n\n\n\nNo - Keep Sequential\n\nOrder-Dependent:\n\nConversation threading\nTime-series preprocessing\nSequential data validation\nCumulative statistics\n\nGlobal Operations:\n\nGlobal deduplication\nCross-dataset joins\nComputing exact quantiles\n\n\n\n\n\n\n\nFor data operations in the ‚ÄúNo‚Äù column, they often require global coordination or maintain strict ordering. However, many can be approximated with parallel algorithms (like approximate deduplication with locality-sensitive hashing)"
  },
  {
    "objectID": "w03/slides.html#in-action",
    "href": "w03/slides.html#in-action",
    "title": "Week 3: Parallelization Concepts",
    "section": "In Action",
    "text": "In Action\n\nimport time\nfrom sympy.ntheory import factorint\nfrom joblib import Parallel, delayed\nparallel_runner = Parallel(n_jobs=4)\nstart, end = 500, 580\ndef find_prime_factors(num):\n  time.sleep(.01)\n  return factorint(num, multiple=True)\ndisp_time = lambda start, end: print('{:.4f} s'.format(end - start))\n\n\n\n\nserial_start = time.time()\nresult = [\n  (i,find_prime_factors(i))\n  for i in range(start, end+1)\n]\nserial_end = time.time()\ndisp_time(serial_start, serial_end)\n\n0.9835 s\n\n\n\n\npar_start = time.time()\nresult = parallel_runner(\n  delayed(find_prime_factors)(i)\n  for i in range(start, end+1)\n)\npar_end = time.time()\ndisp_time(par_start, par_end)\n\n2.7223 s"
  },
  {
    "objectID": "w03/slides.html#what-happens-when-not-embarrassingly-parallel",
    "href": "w03/slides.html#what-happens-when-not-embarrassingly-parallel",
    "title": "Week 3: Parallelization Concepts",
    "section": "What Happens When Not Embarrassingly Parallel?",
    "text": "What Happens When Not Embarrassingly Parallel?\n\nThink of the difference between linear and quadratic equations in algebra:\n\\(3x - 1 = 0\\) is ‚Äúembarrassingly‚Äù solvable, on its own: you can solve it directly, by adding 3 to both sides \\(\\implies x = \\frac{1}{3}\\). Same for \\(2x + 3 = 0 \\implies x = -\\frac{3}{2}\\)\nNow consider \\(6x^2 + 7x - 3 = 0\\): Harder to solve ‚Äúdirectly‚Äù, so your instinct might be to turn to the laborious quadratic equation:\n\n\\[\n\\begin{align*}\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} = \\frac{-7 \\pm \\sqrt{49 - 4(6)(-3)}}{2(6)} = \\frac{-7 \\pm 11}{12} = \\left\\{\\frac{1}{3},-\\frac{3}{2}\\right\\}\n\\end{align*}\n\\]\n\nAnd yet, \\(6x^2 + 7x - 3 = (3x - 1)(2x + 3)\\), meaning that we could have split the problem into two ‚Äúembarrassingly‚Äù solvable pieces, then multiplied to get result!"
  },
  {
    "objectID": "w03/slides.html#the-analogy-to-map-reduce",
    "href": "w03/slides.html#the-analogy-to-map-reduce",
    "title": "Week 3: Parallelization Concepts",
    "section": "The Analogy to Map-Reduce",
    "text": "The Analogy to Map-Reduce\n\n\n\n\n\n\n\n\\(\\leadsto\\) If code is not embarrassingly parallel (instinctually requiring laborious serial execution),\n\\(\\underbrace{6x^2 + 7x - 3 = 0}_{\\text{Solve using Quadratic Eqn}}\\)\n\n\nBut can be split into‚Ä¶\n\\((3x - 1)(2x + 3) = 0\\)\n\n\nEmbarrassingly parallel pieces which combine to same result,\n\\(\\underbrace{3x - 1 = 0}_{\\text{Solve directly}}, \\underbrace{2x + 3 = 0}_{\\text{Solve directly}}\\)\n\n\nWe can use map-reduce to achieve ultra speedup (running ‚Äúpieces‚Äù on GPU!)\n\\(\\underbrace{(3x-1)(2x+3) = 0}_{\\text{Solutions satisfy this product}}\\)"
  },
  {
    "objectID": "w03/slides.html#the-direct-analogy-map-reduce",
    "href": "w03/slides.html#the-direct-analogy-map-reduce",
    "title": "Week 3: Parallelization Concepts",
    "section": "The Direct Analogy: Map-Reduce!",
    "text": "The Direct Analogy: Map-Reduce!\n\nProblem from DSAN 5000/5100: Computing SSR (Sum of Squared Residuals)\n\\(y = (1,3,2), \\widehat{y} = (2, 5, 0) \\implies \\text{SSR} = (1-2)^2 + (3-5)^2 + (2-0)^2 = 9\\)\nComputing pieces separately:\nmap(do_something_with_piece, list_of_pieces)\n\nmy_map = map(lambda input: input**2, [(1-2), (3-5), (2-0)])\nmap_result = list(my_map)\nmap_result\n\n[1, 4, 4]\n\n\nCombining solved pieces\nreduce(how_to_combine_pair_of_pieces, pieces_to_combine)\n\nfrom functools import reduce\nmy_reduce = reduce(lambda piece1, piece2: piece1 + piece2, map_result)\nmy_reduce\n\n9"
  },
  {
    "objectID": "w03/slides.html#functions-vs.-functionals",
    "href": "w03/slides.html#functions-vs.-functionals",
    "title": "Week 3: Parallelization Concepts",
    "section": "Functions vs.¬†Functionals",
    "text": "Functions vs.¬†Functionals\nYou may have noticed: map() and reduce() are ‚Äúmeta-functions‚Äù: functions that take other functions as inputs\n\n\n\ndef add_5(num):\n  return num + 5\nadd_5(10)\n\n15\n\n\n\n\ndef apply_twice(fn, arg):\n  return fn(fn(arg))\napply_twice(add_5, 10)\n\n20\n\n\n\nIn Python, functions can be used as vars (Hence lambda):\n\nadd_5 = lambda num: num + 5\napply_twice(add_5, 10)\n\n20\n\n\nThis relates to a whole paradigm, ‚Äúfunctional programming‚Äù: mostly outside scope of course, but lots of important+useful takeaways/rules-of-thumb!"
  },
  {
    "objectID": "w03/slides.html#train-your-brain-for-functional-approach-implies-master-debugging",
    "href": "w03/slides.html#train-your-brain-for-functional-approach-implies-master-debugging",
    "title": "Week 3: Parallelization Concepts",
    "section": "Train Your Brain for Functional Approach \\(\\implies\\) Master Debugging!",
    "text": "Train Your Brain for Functional Approach \\(\\implies\\) Master Debugging!\n\nIn CS Theory: enables formal proofs of correctness\nIn CS practice:\n\nWhen a program doesn‚Äôt work, each function is an interface point where you can check that the data are correct. You can look at the intermediate inputs and outputs to quickly isolate the function that‚Äôs responsible for a bug.(from Python‚Äôs ‚ÄúFunctional Programming HowTo‚Äù)"
  },
  {
    "objectID": "w03/slides.html#code-rightarrow-pipelines-rightarrow-debuggable-pipelines",
    "href": "w03/slides.html#code-rightarrow-pipelines-rightarrow-debuggable-pipelines",
    "title": "Week 3: Parallelization Concepts",
    "section": "Code \\(\\rightarrow\\) Pipelines \\(\\rightarrow\\) Debuggable Pipelines",
    "text": "Code \\(\\rightarrow\\) Pipelines \\(\\rightarrow\\) Debuggable Pipelines\n\n\nScenario: Run code, check the output, and‚Ä¶ it‚Äôs wrong üòµ what do you do?\nUsual approach: Read lines one-by-one, figuring out what they do, seeing if something pops out that seems wrong; adding comments like # Convert to lowercase\n\n\n\n\nEasy case: found typo in punctuation removal code. Fix the error, add comment like # Remove punctuation\n Rule 1 of FP: transform these comments into function names\n\n\n\nHard case: Something in load_text() modifies a variable that later on breaks remove_punct() (Called a side-effect)\n Rule 2 of FP: NO SIDE-EFFECTS!\n\n\n\n\n\n\n\n\n\nG\n\n\n\ninput\nin.txt\n\n\n\nload_text\nload_text\n(Verb)\n\n\n\n\ninput-&gt;load_text\n\n\n\n\n\nlowercase\nlowercase\n(Verb)\n\n\n\n\nload_text-&gt;lowercase\n\n\nüßê ‚úÖ\n\n\n\nremove_punct\nremove_punct\n(Verb)\n\n\n\n\nlowercase-&gt;remove_punct\n\n\nüßê ‚úÖ\n\n\n\nremove_stopwords\nremove_stopwords\n(Verb)\n\n\n\n\nremove_punct-&gt;remove_stopwords\n\n\nüßê ‚ùå‚ùóÔ∏è\n\n\n\noutput\nout.txt\n\n\n\nremove_stopwords-&gt;output\n\n\n\n\n\n\n (Does this way of diagramming a program look familiar?) \n\n\n\n\nWith side effects: ‚ùå \\(\\implies\\) issue is somewhere earlier in the chain üò©üèÉ‚Äç‚ôÇÔ∏è\nNo side effects: ‚ùå \\(\\implies\\) issue must be in remove_punct()!!! üòé ‚è±Ô∏è = üí∞"
  },
  {
    "objectID": "w03/slides.html#if-its-so-useful-why-doesnt-everyone-do-it",
    "href": "w03/slides.html#if-its-so-useful-why-doesnt-everyone-do-it",
    "title": "Week 3: Parallelization Concepts",
    "section": "If It‚Äôs So Useful, Why Doesn‚Äôt Everyone Do It?",
    "text": "If It‚Äôs So Useful, Why Doesn‚Äôt Everyone Do It?\n\nTrapped in imperative (sequential) coding mode: Path dependency / QWERTY\nWe need to start thinking like this bc, 1000x harder to debug parallel code! So we need to be less ad hoc in how we write+debug, from here on out! üôá‚Äç‚ôÇÔ∏èüôè\n\n\nFrom Leskovec, Rajaraman, and Ullman (2014)\nThe title relates to a classic Economics joke (the best kind of joke): ‚ÄúAn economist and a CEO are walking down the street, when the CEO points at the ground and tells the economist, ‚Äòlook! A $20 bill on the ground!‚Äô The economist keeps on walking, scoffing at the CEO: ‚Äòdon‚Äôt be silly, if there was a $20 bill on the ground, somebody would have picked it up already‚Äô.‚Äù"
  },
  {
    "objectID": "w03/slides.html#the-killer-application-matrix-multiplication",
    "href": "w03/slides.html#the-killer-application-matrix-multiplication",
    "title": "Week 3: Parallelization Concepts",
    "section": "The ‚ÄúKiller Application‚Äù: Matrix Multiplication",
    "text": "The ‚ÄúKiller Application‚Äù: Matrix Multiplication\n\n(I learned from Jeff Ullman, who did the obnoxious Stanford thing of mentioning in passing how ‚Äútwo previous students in the class did this for a cool final project on web crawling and, well, it escalated quickly‚Äù, aka became Google)\n\n\nFrom Leskovec, Rajaraman, and Ullman (2014), which is (legally) free online!"
  },
  {
    "objectID": "w03/slides.html#the-killer-way-to-learn-text-counts",
    "href": "w03/slides.html#the-killer-way-to-learn-text-counts",
    "title": "Week 3: Parallelization Concepts",
    "section": "The Killer Way-To-Learn: Text Counts!",
    "text": "The Killer Way-To-Learn: Text Counts!\n\n(2014): Text counts (2.2) \\(\\rightarrow\\) Matrix multiplication (2.3) \\(\\rightarrow \\cdots \\rightarrow\\) PageRank (5.1)\nThe goal: User searches ‚ÄúDenzel Curry‚Äù‚Ä¶ How relevant is a given webpage?\nScenario 1: Entire internet fits on CPU \\(\\implies\\) We can just make a big dict:\n\n\n\n\n\n\n\n\nG\n\n\n\ninternet\n\nScan in O(n):\nToday Denzel Washington\nate a big bowl of Yum's\ncurry. Denzel allegedly\nrubbed his tum and said\n\"yum yum yum\" when he\ntasted today's curry.\n\"Yum! It is me Denzel,\ncurry is my fav!\", he\nexclaimed. According to\nhis friend Steph, curry\nis indeed Denzel's fav.\nWe are live with Del\nCurry in Washington for\na Denzel curry update.\n\n\n\n\nccounts\n\nOverall Counts\n\n('according',1)\n('allegedly',1)\n('ate',1)\n('big',1)\n('bowl',1)\n('curry',6)\n('del',1)\n('denzel',5)\n('exclaimed',1)\n('fav',2)\n('friend',1)\n('indeed',1)\n('live',1)\n('rubbed',1)\n('said',1)\n('steph',1)\n('tasted',1)\n('today',2)\n('tum',1)\n('update',1)\n('washington',2)\n('yum',4)\n\n\n\n\ninternet-&gt;ccounts\n\n\nLoop Over Words"
  },
  {
    "objectID": "w03/slides.html#if-everything-doesnt-fit-on-cpu",
    "href": "w03/slides.html#if-everything-doesnt-fit-on-cpu",
    "title": "Week 3: Parallelization Concepts",
    "section": "If Everything Doesn‚Äôt Fit on CPU‚Ä¶",
    "text": "If Everything Doesn‚Äôt Fit on CPU‚Ä¶\n\nFrom Cornell Virtual Workshop, ‚ÄúUnderstanding GPU Architecture‚Äù"
  },
  {
    "objectID": "w03/slides.html#break-problem-into-chunks-for-the-green-bois",
    "href": "w03/slides.html#break-problem-into-chunks-for-the-green-bois",
    "title": "Week 3: Parallelization Concepts",
    "section": "Break Problem into Chunks for the Green Bois!",
    "text": "Break Problem into Chunks for the Green Bois!\n\n\n\n\n\n\n\nG\n\n\n\nchunked\n\nChunked Document\n\nToday Denzel Washington\nate a big bowl of Yum's\ncurry. Denzel allegedly\nrubbed his tum and said\n\n\"yum yum yum\" when he\ntasted today's curry.\n\"Yum! It is me Denzel,\ncurry is my fav!\", he\n\nexclaimed. According to\nhis friend Steph, curry\nis indeed Denzel's fav.\nWe are live with Del\nCurry in Washington for\na Denzel curry update.\n\n\n\n\nchcounts\n\nChunked Counts\n\n('today',1)\n('denzel',1)\n...\n('tum',1)\n('said',1)\n\n('yum',1)\n('yum',1)\n('yum',1)\n...\n('fav',1)\n\n('exclaimed',1)\n...\n('del',1)\n('curry',1)\n('washington',1)\n('denzel',1)\n('curry',1)\n('update',1)\n\n\n\n\nchunked:p1-&gt;chcounts:p1\n\n\nO(n/4)\n\n\n\nchunked:p2-&gt;chcounts:p2\n\n\nO(n/4)\n\n\n\nchunked:p3-&gt;chcounts:p3\n\n\nO(n/4)\n\n\n\nchunked:p4-&gt;chcounts:p4\n\n\nO(n/4)\n\n\n\nscounts\n\nHashed Counts\n\n('allegedly',1)\n...\n('curry',1)\n('denzel',2)\n...\n('yum',1)\n\n('curry',2)\n('denzel',1)\n...\n('yum',4)\n\n('according',1)\n('curry',1)\n('del',1)\n('denzel',1)\n...\n('curry',2)\n('denzel',1)\n('update',1)\n('washington',1)\n\n\n\n\nchcounts:p1-&gt;scounts:p1\n\n\nO(n/4)\n\n\n\nchcounts:p2-&gt;scounts:p2\n\n\nO(n/4)\n\n\n\nchcounts:p3-&gt;scounts:p3\n\n\nO(n/4)\n\n\n\nchcounts:p4-&gt;scounts:p4\n\n\nO(n/4)\n\n\n\nccounts\n\nOverall Counts\n('according',1)\n('allegedly',1)\n('ate',1)\n('big',1)\n('bowl',1)\n('curry',6)\n('del',1)\n('denzel',5)\n('exclaimed',1)\n('fav',2)\n('friend',1)\n('indeed',1)\n('live',1)\n('rubbed',1)\n('said',1)\n('steph',1)\n('tasted',1)\n('today',2)\n('tum',1)\n('update',1)\n('washington',2)\n('yum',4)\n\n\n\n\nscounts:p1-&gt;ccounts:p1\n\n\n\n\n\nscounts:p2-&gt;ccounts:p1\n\n\n\n\n\nscounts:p3-&gt;ccounts:p1\n\n\n\n\n\nscounts:p4-&gt;ccounts:p1\n\n\n\n\n\nscounts:p2-&gt;ccounts\n\n\n \n\nMerge in\nO(n)\n\n\n\n\n\n\n\n\n\n\\(\\implies\\) Total = \\(O(3n) = O(n)\\)\nBut also optimized in terms of constants, because of sequential memory reads"
  },
  {
    "objectID": "w03/slides.html#references",
    "href": "w03/slides.html#references",
    "title": "Week 3: Parallelization Concepts",
    "section": "References",
    "text": "References\n\n\nLeskovec, Jure, Anand Rajaraman, and Jeffrey David Ullman. 2014. Mining of Massive Datasets. Cambridge University Press."
  },
  {
    "objectID": "w05/index.html",
    "href": "w05/index.html",
    "title": "Week 5: Data Engineering",
    "section": "",
    "text": "Open slides in new tab ‚Üí",
    "crumbs": [
      "Week 5: <span class='sec-w05-date'>Sep 22</span>"
    ]
  },
  {
    "objectID": "w05/index.html#agenda",
    "href": "w05/index.html#agenda",
    "title": "Week 5: Data Engineering",
    "section": "Agenda",
    "text": "Agenda\n\n\nLooking Back\n\nIntroduction to cloud\nPython Multiprocessing\nDuckDB, Polars\n\n\n\nFuture\n\nProject introduction\nSpark dataframe, ML, NLP\n\n\n\n\n\nToday\n\nData Engineering\n\nData Lakes, Warehouses\nNoSQL Databases\n\nLab:\n\nAthena\nEC2",
    "crumbs": [
      "Week 5: <span class='sec-w05-date'>Sep 22</span>"
    ]
  },
  {
    "objectID": "w05/index.html#what-is-data-engineering",
    "href": "w05/index.html#what-is-data-engineering",
    "title": "Week 5: Data Engineering",
    "section": "What is Data Engineering?",
    "text": "What is Data Engineering?\nAs the scale of the data grew, the existing ETL processes alone were not sufficient, a separate discipline was needed for:\n\nCollecting data\nManaging storage\nCataloging\nMaking it available for applications such as analytics & machine learning)\nSecurity\nLifecycle management\nAnd more‚Ä¶\n\n\nFrom Wikipedia: Data engineering refers to the building of systems to enable the collection and usage of data. This data is usually used to enable subsequent analysis and data science; which often involves machine learning. Making the data usable usually involves substantial compute and storage, as well as data processing and cleaning.",
    "crumbs": [
      "Week 5: <span class='sec-w05-date'>Sep 22</span>"
    ]
  },
  {
    "objectID": "w05/index.html#what-do-data-engineers-do",
    "href": "w05/index.html#what-do-data-engineers-do",
    "title": "Week 5: Data Engineering",
    "section": "What do Data Engineers do?",
    "text": "What do Data Engineers do?\nData engineers build systems that collect data from different sources and make this data available for analytics and ML application. This usually involves the following:\n\nAcquisition: Finding all the different datasets around the business.\n\nThese could be availble in databases, shared drives, ingested directly from IoT devices, external datasets, and more.\n\nCleaning: The raw data usually cannot be used as is, it needs to be cleaned.\nConversion: Since the data is coming from different sources, it would probably in different formats (database tables, CSV, JSON, custom). Needs to be converted into a common format (such as parquet) before it becomes usable.\n\nMultiple datasets need to be joined together to answer a business question.",
    "crumbs": [
      "Week 5: <span class='sec-w05-date'>Sep 22</span>"
    ]
  },
  {
    "objectID": "w05/index.html#what-do-data-engineers-do-contd.",
    "href": "w05/index.html#what-do-data-engineers-do-contd.",
    "title": "Week 5: Data Engineering",
    "section": "What do Data Engineers do (contd.)?",
    "text": "What do Data Engineers do (contd.)?\n\nDisambiguation: How to interpret what the data means? Use a data catalog and then with the help of subject matter experts (often called Data Stewards) add meaningful description to the datasets.\nDeduplication: Having a single source of truth!\nData Governance: for how long to store the data, how to enforce access controls (Principle of least privilege) etc.\n\nOnce this is done, data may be stored in a central repository such as a data lake or data lakehouse. Data engineers may also copy and move subsets of data into a data warehouse.",
    "crumbs": [
      "Week 5: <span class='sec-w05-date'>Sep 22</span>"
    ]
  },
  {
    "objectID": "w05/index.html#data-engineering-tools-and-technologies",
    "href": "w05/index.html#data-engineering-tools-and-technologies",
    "title": "Week 5: Data Engineering",
    "section": "Data Engineering Tools and Technologies",
    "text": "Data Engineering Tools and Technologies\nData engineers work with a variety of tools and technologies, including:\n\nETL Tools: ETL (extract, transform, load) tools move data between systems. They access data, then apply rules to ‚Äútransform‚Äù the data through steps that make it more suitable for analysis.\nSQL: Structured Query Language (SQL) is the standard language for querying relational databases.\nPython: Python is a general programming language. Data engineers may choose to use Python for ETL tasks. Spark (pyspark) for Big Data, Apache Flink for streaming data.\nCloud Data Storage: Including Amazon S3, Azure Data Lake Storage (ADLS), Google Cloud Storage, etc.\nCloud Data Warehouses: Data ready for use by data scientists and analysts is stored in data warehouses, such as Amazon Redshift, Google BigQuery, Azure Data Warehouse, Snowflake etc.",
    "crumbs": [
      "Week 5: <span class='sec-w05-date'>Sep 22</span>"
    ]
  },
  {
    "objectID": "w05/index.html#popular-data-engineering-tools",
    "href": "w05/index.html#popular-data-engineering-tools",
    "title": "Week 5: Data Engineering",
    "section": "Popular Data engineering tools",
    "text": "Popular Data engineering tools\n\n\n\nSource",
    "crumbs": [
      "Week 5: <span class='sec-w05-date'>Sep 22</span>"
    ]
  },
  {
    "objectID": "w05/index.html#wikipedia-definition",
    "href": "w05/index.html#wikipedia-definition",
    "title": "Week 5: Data Engineering",
    "section": "Wikipedia Definition",
    "text": "Wikipedia Definition\n\nFrom Wikipedia: A data lake is a system or repository of data stored in its natural/raw format, usually object blobs or files. A data lake is usually a single store of data including raw copies of source system data, sensor data, social data etc., and transformed data used for tasks such as reporting, visualization, advanced analytics and machine learning. A data lake can include structured data from relational databases (rows and columns), semi-structured data (CSV, logs, XML, JSON), unstructured data (emails, documents, PDFs) and binary data (images, audio, video).",
    "crumbs": [
      "Week 5: <span class='sec-w05-date'>Sep 22</span>"
    ]
  },
  {
    "objectID": "w05/index.html#cloud-provider-definitions",
    "href": "w05/index.html#cloud-provider-definitions",
    "title": "Week 5: Data Engineering",
    "section": "Cloud Provider Definitions",
    "text": "Cloud Provider Definitions\nDefinitions in the wild (emphasis mine).\n\n\nAWS\nA data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. You can store your data as-is, without having to first structure the data, and run different types of analytics‚Äîfrom dashboards and visualizations to big data processing, real-time analytics, and machine learning to guide better decisions.\n\nGCP\nA data lake provides a scalable and secure platform that allows enterprises to: ingest any data from any system at any speed‚Äîeven if the data comes from on-premises, cloud, or edge-computing systems; store any type or volume of data in full fidelity; process data in real time or batch mode; and analyze data using SQL, Python, R, or any other language, third-party data, or analytics application.\n\nAzure\nAzure Data Lake includes all the capabilities required to make it easy for developers, data scientists, and analysts to store data of any size, shape, and speed, and do all types of processing and analytics across platforms and languages.\n\nDataBricks\nA data lake is a central location that holds a large amount of data in its native, raw format. Compared to a hierarchical data warehouse, which stores data in files or folders, a data lake uses a flat architecture and object storage to store the data.‚Äç",
    "crumbs": [
      "Week 5: <span class='sec-w05-date'>Sep 22</span>"
    ]
  },
  {
    "objectID": "w05/index.html#working-with-a-cloud-data-lake",
    "href": "w05/index.html#working-with-a-cloud-data-lake",
    "title": "Week 5: Data Engineering",
    "section": "Working with a Cloud Data Lake",
    "text": "Working with a Cloud Data Lake\nA cloud data lake is setup using the cloud provider‚Äôs object store (S3, GCS, Azure Blob Storage).\n\nThe object stores are extremely scalable, for context, the maximum size of an object in S3 is 5 TB and there is no limit to number of objects in an S3 bucket.\nThey are extremely duarable, 99.999999999%. Provide strong consistency (read-after-write, listing buckets and objects, granting permissions etc.).\nCost effective, with multiple storage classes.\nIntegration with data processing tools such as Spark, machine learning tools such as SageMaker, data warehouses such as RedShift and data cataloging tools.\nThey provide Fine Grained Access Control to the data.",
    "crumbs": [
      "Week 5: <span class='sec-w05-date'>Sep 22</span>"
    ]
  },
  {
    "objectID": "w05/index.html#data-warehouses",
    "href": "w05/index.html#data-warehouses",
    "title": "Week 5: Data Engineering",
    "section": "Data Warehouses",
    "text": "Data Warehouses\n\n\nFrom Wikipedia: In computing, a data warehouse (DW or DWH), also known as an enterprise data warehouse (EDW), is a system used for reporting and data analysis and is considered a core component of business intelligence. DWs are central repositories of integrated data from one or more disparate sources. They store current and historical data in one single place that are used for creating analytical reports for workers throughout the enterprise.",
    "crumbs": [
      "Week 5: <span class='sec-w05-date'>Sep 22</span>"
    ]
  },
  {
    "objectID": "w05/index.html#data-warehouse-cloud-provider-definitions",
    "href": "w05/index.html#data-warehouse-cloud-provider-definitions",
    "title": "Week 5: Data Engineering",
    "section": "Data Warehouse (Cloud Provider Definitions)",
    "text": "Data Warehouse (Cloud Provider Definitions)\nDefinitions in the wild (emphasis mine).\n\n\nAWS\nA data warehouse is a central repository of information that can be analyzed to make more informed decisions. Data flows into a data warehouse from transactional systems, relational databases, and other sources, typically on a regular cadence. Business analysts, data engineers, data scientists, and decision makers access the data through business intelligence (BI) tools, SQL clients, and other analytics applications.\nAzure\nA data warehouse is a centralized repository of integrated data from one or more disparate sources. Data warehouses store current and historical data and are used for reporting and analysis of the data.\n\nGCP\nA data warehouse is an enterprise system used for the analysis and reporting of structured and semi-structured data from multiple sources, such as point-of-sale transactions, marketing automation, customer relationship management, and more.\nSnowflake\nA data warehouse (DW) is a relational database that is designed for analytical rather than transactional work. It collects and aggregates data from one or many sources so it can be analyzed to produce business insights.",
    "crumbs": [
      "Week 5: <span class='sec-w05-date'>Sep 22</span>"
    ]
  },
  {
    "objectID": "w05/index.html#working-with-a-cloud-data-warehouse",
    "href": "w05/index.html#working-with-a-cloud-data-warehouse",
    "title": "Week 5: Data Engineering",
    "section": "Working with a Cloud Data Warehouse",
    "text": "Working with a Cloud Data Warehouse\nAll cloud providers provide a data warehouse solution that works in conjunction with their data lake solution.\n\nAWS has Redshift, Azure has Synapse Analytics. GCP has BigQuery and then there is Snowflake.\nIn a data warehouse, Online analytical processing (OLAP) allows for fast querying and analysis of data from different perspectives. It also helps in pre-aggregating and pre-calculating the information available in the archive.\nData warehouses are Peta Byte scale (Amazon RedShift, Google BigQuery, Azure Synapse Analytics).\nData warehouses can have dedicated compute provisioned or be serverless (BigQuery is serverless, Redshift allows both options now).\nData warehouses now offer integrated ML capabilities, you can build models with SQL and use them in queries (Amazon RedShift ML, Google BigQuery ML, Azure Synapse Analytics ML).\nIntegration with reporting and dashboarding tools such as Tableau, Grafana, Looker etc. and data analytics tools such as Spark and data cataloging tools.\nThey provide Fine Grained Access Control to the data.",
    "crumbs": [
      "Week 5: <span class='sec-w05-date'>Sep 22</span>"
    ]
  },
  {
    "objectID": "w05/index.html#combining-data-lakes-and-data-warehouses",
    "href": "w05/index.html#combining-data-lakes-and-data-warehouses",
    "title": "Week 5: Data Engineering",
    "section": "Combining Data Lakes and Data Warehouses",
    "text": "Combining Data Lakes and Data Warehouses\nCombine the flexibility, cost-efficiency, and scale of data lakes with the data management and ACID transactions of data warehouses to provide a single architecture that can enable business intelligence and machine learning on all data.\n\nBuild a lakehouse architecture on AWS\nThe Databricks lakehouse platform\nOpen data lakehouse on Google Cloud\nThe data lakehouse, the data warehouse and a modern data platform on Azure",
    "crumbs": [
      "Week 5: <span class='sec-w05-date'>Sep 22</span>"
    ]
  },
  {
    "objectID": "w05/index.html#nosql-databases",
    "href": "w05/index.html#nosql-databases",
    "title": "Week 5: Data Engineering",
    "section": "NoSQL Databases",
    "text": "NoSQL Databases\nAt some point we needed to think beyond relation databases, because:\n\nData became more and more complex (not all data is tabular, thinkin JSON data emitted by an IoT device).\nCost of storage decreased (everything did not need to be stored in the 3rd normal form)\nMore data on the cloud meant data needed to be placed across different servers (scale-out)\nData needed to be placed intelligently in geo locations of interest\nAnd more‚Ä¶\n\n\nFrom Wikipedia: A NoSQL (originally referring to ‚Äúnon-SQL‚Äù or ‚Äúnon-relational‚Äù) database provides a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases. Such databases have existed since the late 1960s, but the name ‚ÄúNoSQL‚Äù was only coined in the early 21st century, triggered by the needs of Web 2.0 companies. NoSQL databases are increasingly used in big data and real-time web applications. NoSQL systems are also sometimes called Not only SQL to emphasize that they may support SQL-like query languages or sit alongside SQL databases in polyglot-persistent architectures.",
    "crumbs": [
      "Week 5: <span class='sec-w05-date'>Sep 22</span>"
    ]
  },
  {
    "objectID": "w05/index.html#types-of-nosql-databases",
    "href": "w05/index.html#types-of-nosql-databases",
    "title": "Week 5: Data Engineering",
    "section": "Types of NoSQL Databases",
    "text": "Types of NoSQL Databases\nOver time, four major types of NoSQL databases emerged: document databases, key-value databases, wide-column stores, and graph databases.\n\nDocument databases store data in documents similar to JSON (JavaScript Object Notation) objects. Each document contains pairs of fields and values. The values can typically be a variety of types including things like strings, numbers, booleans, arrays, or objects.\nKey-value databases are a simpler type of database where each item contains keys and values.\nWide-column stores store data in tables, rows, and dynamic columns.\nGraph databases store data in nodes and edges. Nodes typically store information about people, places, and things, while edges store information about the relationships between the nodes.",
    "crumbs": [
      "Week 5: <span class='sec-w05-date'>Sep 22</span>"
    ]
  },
  {
    "objectID": "w05/index.html#examples-of-nosql-databases",
    "href": "w05/index.html#examples-of-nosql-databases",
    "title": "Week 5: Data Engineering",
    "section": "Examples of NoSQL Databases",
    "text": "Examples of NoSQL Databases\n\n\n\n\n\n\n\nNOSQL Database Type\nExamples\n\n\n\n\nDocument Database\nAmazon DocumentDB, MongoDB, Cosmos DB, ArangoDB, Couchbase Server, CouchDB\n\n\nKey-value Database\nAmazon DynamoDB, Couchbase, Memcached, Redis\n\n\nWide-column datastores\nAmazon DynamoDB, Apache Cassandra, Google Bigtable, Azure Tables\n\n\nGraph databases\nAmazon Neptune, Neo4j\n\n\n\nAs a data scientist you would work with a NoSQL database through an SDK/API. Several programming languages are supported including Python, Java, Go, C++ etc.",
    "crumbs": [
      "Week 5: <span class='sec-w05-date'>Sep 22</span>"
    ]
  },
  {
    "objectID": "w05/index.html#example-of-documents-in-a-document-database",
    "href": "w05/index.html#example-of-documents-in-a-document-database",
    "title": "Week 5: Data Engineering",
    "section": "Example of documents in a document database",
    "text": "Example of documents in a document database\nHere is an example of a document inserted in a key-value/document database such as MongoDB.\n{\n   \"name\" : \"IS 350\",\n   \"year\" : \"2015\",\n   \"make\" : \"Lexus\",\n   \"colors\" : [\"black\",\"white\",\"grey\"],\n   \"spec\" : {\n      \"engine\" : \"V6\",\n      \"wheelbase\" : \"110.2 in\",\n      \"length\" : \"183.7 in\"\n   }\n}\nThe same example can be inserted in an Amazon DynamoDB table called (say) Cars.\ndatabase = boto3.resource('dynamodb')\ntable = database.Table('cars')\nitem = {\n   \"name\" : \"IS 350\",\n   \"year\" : \"2015\",\n   \"make\" : \"Lexus\",\n   \"colors\" : [\"black\",\"white\",\"grey\"],\n   \"spec\" : {\n      \"engine\" : \"V6\",\n      \"wheelbase\" : \"110.2 in\",\n      \"length\" : \"183.7 in\"\n   }\n}\ntable.put_item(Item = item)",
    "crumbs": [
      "Week 5: <span class='sec-w05-date'>Sep 22</span>"
    ]
  },
  {
    "objectID": "w05/index.html#other-data-stores-to-know-about",
    "href": "w05/index.html#other-data-stores-to-know-about",
    "title": "Week 5: Data Engineering",
    "section": "Other Data Stores to Know About",
    "text": "Other Data Stores to Know About\nBesides the general concepts about data lkakes, warehouses, different types of databases, there are some purpose built databases that are good to know about.\n\nSplunk\nElasticsearch\nDuckDB\nMany many more‚Ä¶",
    "crumbs": [
      "Week 5: <span class='sec-w05-date'>Sep 22</span>"
    ]
  },
  {
    "objectID": "w05/index.html#further-reading",
    "href": "w05/index.html#further-reading",
    "title": "Week 5: Data Engineering",
    "section": "Further Reading",
    "text": "Further Reading\nPlease lookup these topics on Google for further reading. Not providing specific links here because they all point to vendor specific products.\n\nData Catalog\nData Governance\nData Mesh\nData Fabric",
    "crumbs": [
      "Week 5: <span class='sec-w05-date'>Sep 22</span>"
    ]
  },
  {
    "objectID": "w05/index.html#boar-book-time",
    "href": "w05/index.html#boar-book-time",
    "title": "Week 5: Data Engineering",
    "section": "Boar Book Time!",
    "text": "Boar Book Time!\n\n\nKleppmann (2017), pages 69-95",
    "crumbs": [
      "Week 5: <span class='sec-w05-date'>Sep 22</span>"
    ]
  },
  {
    "objectID": "w05/index.html#telemetry-analytics",
    "href": "w05/index.html#telemetry-analytics",
    "title": "Week 5: Data Engineering",
    "section": "Telemetry ‚Üí Analytics",
    "text": "Telemetry ‚Üí Analytics\nRecall how many tiny bits of information are streaming into a given app‚Äôs servers at any given time!\n\nDatabase administrators therefore closely guard their OLTP databases. They are usually reluctant to let business analysts run ad hoc analytic queries on an OLTP database, since those queries are often expensive, scanning large parts of the dataset, which can harm the performance of concurrently executing transactions.",
    "crumbs": [
      "Week 5: <span class='sec-w05-date'>Sep 22</span>"
    ]
  },
  {
    "objectID": "w05/index.html#what-is-the-quickest-possible-way-to-explore-data-in-s3",
    "href": "w05/index.html#what-is-the-quickest-possible-way-to-explore-data-in-s3",
    "title": "Week 5: Data Engineering",
    "section": "What is the Quickest Possible Way to Explore Data in S3?",
    "text": "What is the Quickest Possible Way to Explore Data in S3?\n\n\n\nFrom this video",
    "crumbs": [
      "Week 5: <span class='sec-w05-date'>Sep 22</span>"
    ]
  },
  {
    "objectID": "w05/index.html#lab-analyzing-nyc-taxi-dataset-with-athena",
    "href": "w05/index.html#lab-analyzing-nyc-taxi-dataset-with-athena",
    "title": "Week 5: Data Engineering",
    "section": "Lab: Analyzing NYC Taxi dataset with Athena",
    "text": "Lab: Analyzing NYC Taxi dataset with Athena\n\nA simple example of using DuckDB and Apache Arrow using NYC Taxi dataset\nThis notebook reads the NYC taxi dataset files for the first 6 months of the year 2023 (about ~20 million rows) and runs some analytics operation on this dataset. This dataset is too big to fit into memory.\n\nGitHub Classroom Link for Lab 5\nGitHub Classroom Link for Assignment 5",
    "crumbs": [
      "Week 5: <span class='sec-w05-date'>Sep 22</span>"
    ]
  },
  {
    "objectID": "w05/slides.html#agenda",
    "href": "w05/slides.html#agenda",
    "title": "Week 5: Data Engineering",
    "section": "Agenda",
    "text": "Agenda\n\nLooking Back\n\nIntroduction to cloud\nPython Multiprocessing\nDuckDB, Polars\n\nFuture\n\nProject introduction\nSpark dataframe, ML, NLP\n\n\nToday\n\nData Engineering\n\nData Lakes, Warehouses\nNoSQL Databases\n\nLab:\n\nAthena\nEC2"
  },
  {
    "objectID": "w05/slides.html#what-is-data-engineering",
    "href": "w05/slides.html#what-is-data-engineering",
    "title": "Week 5: Data Engineering",
    "section": "What is Data Engineering?",
    "text": "What is Data Engineering?\nAs the scale of the data grew, the existing ETL processes alone were not sufficient, a separate discipline was needed for:\n\nCollecting data\nManaging storage\nCataloging\nMaking it available for applications such as analytics & machine learning)\nSecurity\nLifecycle management\nAnd more‚Ä¶\n\n\nFrom Wikipedia: Data engineering refers to the building of systems to enable the collection and usage of data. This data is usually used to enable subsequent analysis and data science; which often involves machine learning. Making the data usable usually involves substantial compute and storage, as well as data processing and cleaning."
  },
  {
    "objectID": "w05/slides.html#what-do-data-engineers-do",
    "href": "w05/slides.html#what-do-data-engineers-do",
    "title": "Week 5: Data Engineering",
    "section": "What do Data Engineers do?",
    "text": "What do Data Engineers do?\nData engineers build systems that collect data from different sources and make this data available for analytics and ML application. This usually involves the following:\n\nAcquisition: Finding all the different datasets around the business.\n\nThese could be availble in databases, shared drives, ingested directly from IoT devices, external datasets, and more.\n\nCleaning: The raw data usually cannot be used as is, it needs to be cleaned.\nConversion: Since the data is coming from different sources, it would probably in different formats (database tables, CSV, JSON, custom). Needs to be converted into a common format (such as parquet) before it becomes usable.\n\nMultiple datasets need to be joined together to answer a business question."
  },
  {
    "objectID": "w05/slides.html#what-do-data-engineers-do-contd.",
    "href": "w05/slides.html#what-do-data-engineers-do-contd.",
    "title": "Week 5: Data Engineering",
    "section": "What do Data Engineers do (contd.)?",
    "text": "What do Data Engineers do (contd.)?\n\nDisambiguation: How to interpret what the data means? Use a data catalog and then with the help of subject matter experts (often called Data Stewards) add meaningful description to the datasets.\nDeduplication: Having a single source of truth!\nData Governance: for how long to store the data, how to enforce access controls (Principle of least privilege) etc.\n\nOnce this is done, data may be stored in a central repository such as a data lake or data lakehouse. Data engineers may also copy and move subsets of data into a data warehouse."
  },
  {
    "objectID": "w05/slides.html#data-engineering-tools-and-technologies",
    "href": "w05/slides.html#data-engineering-tools-and-technologies",
    "title": "Week 5: Data Engineering",
    "section": "Data Engineering Tools and Technologies",
    "text": "Data Engineering Tools and Technologies\nData engineers work with a variety of tools and technologies, including:\n\nETL Tools: ETL (extract, transform, load) tools move data between systems. They access data, then apply rules to ‚Äútransform‚Äù the data through steps that make it more suitable for analysis.\nSQL: Structured Query Language (SQL) is the standard language for querying relational databases.\nPython: Python is a general programming language. Data engineers may choose to use Python for ETL tasks. Spark (pyspark) for Big Data, Apache Flink for streaming data.\nCloud Data Storage: Including Amazon S3, Azure Data Lake Storage (ADLS), Google Cloud Storage, etc.\nCloud Data Warehouses: Data ready for use by data scientists and analysts is stored in data warehouses, such as Amazon Redshift, Google BigQuery, Azure Data Warehouse, Snowflake etc."
  },
  {
    "objectID": "w05/slides.html#popular-data-engineering-tools",
    "href": "w05/slides.html#popular-data-engineering-tools",
    "title": "Week 5: Data Engineering",
    "section": "Popular Data engineering tools",
    "text": "Popular Data engineering tools\n\nSource"
  },
  {
    "objectID": "w05/slides.html#wikipedia-definition",
    "href": "w05/slides.html#wikipedia-definition",
    "title": "Week 5: Data Engineering",
    "section": "Wikipedia Definition",
    "text": "Wikipedia Definition\n\nFrom Wikipedia: A data lake is a system or repository of data stored in its natural/raw format, usually object blobs or files. A data lake is usually a single store of data including raw copies of source system data, sensor data, social data etc., and transformed data used for tasks such as reporting, visualization, advanced analytics and machine learning. A data lake can include structured data from relational databases (rows and columns), semi-structured data (CSV, logs, XML, JSON), unstructured data (emails, documents, PDFs) and binary data (images, audio, video)."
  },
  {
    "objectID": "w05/slides.html#cloud-provider-definitions",
    "href": "w05/slides.html#cloud-provider-definitions",
    "title": "Week 5: Data Engineering",
    "section": "Cloud Provider Definitions",
    "text": "Cloud Provider Definitions\nDefinitions in the wild (emphasis mine).\n\n\nAWS\nA data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. You can store your data as-is, without having to first structure the data, and run different types of analytics‚Äîfrom dashboards and visualizations to big data processing, real-time analytics, and machine learning to guide better decisions.\n\nGCP\nA data lake provides a scalable and secure platform that allows enterprises to: ingest any data from any system at any speed‚Äîeven if the data comes from on-premises, cloud, or edge-computing systems; store any type or volume of data in full fidelity; process data in real time or batch mode; and analyze data using SQL, Python, R, or any other language, third-party data, or analytics application.\n\nAzure\nAzure Data Lake includes all the capabilities required to make it easy for developers, data scientists, and analysts to store data of any size, shape, and speed, and do all types of processing and analytics across platforms and languages.\n\nDataBricks\nA data lake is a central location that holds a large amount of data in its native, raw format. Compared to a hierarchical data warehouse, which stores data in files or folders, a data lake uses a flat architecture and object storage to store the data.‚Äç"
  },
  {
    "objectID": "w05/slides.html#working-with-a-cloud-data-lake",
    "href": "w05/slides.html#working-with-a-cloud-data-lake",
    "title": "Week 5: Data Engineering",
    "section": "Working with a Cloud Data Lake",
    "text": "Working with a Cloud Data Lake\nA cloud data lake is setup using the cloud provider‚Äôs object store (S3, GCS, Azure Blob Storage).\n\nThe object stores are extremely scalable, for context, the maximum size of an object in S3 is 5 TB and there is no limit to number of objects in an S3 bucket.\nThey are extremely duarable, 99.999999999%. Provide strong consistency (read-after-write, listing buckets and objects, granting permissions etc.).\nCost effective, with multiple storage classes.\nIntegration with data processing tools such as Spark, machine learning tools such as SageMaker, data warehouses such as RedShift and data cataloging tools.\nThey provide Fine Grained Access Control to the data."
  },
  {
    "objectID": "w05/slides.html#data-warehouses",
    "href": "w05/slides.html#data-warehouses",
    "title": "Week 5: Data Engineering",
    "section": "Data Warehouses",
    "text": "Data Warehouses\n\n\nFrom Wikipedia: In computing, a data warehouse (DW or DWH), also known as an enterprise data warehouse (EDW), is a system used for reporting and data analysis and is considered a core component of business intelligence. DWs are central repositories of integrated data from one or more disparate sources. They store current and historical data in one single place that are used for creating analytical reports for workers throughout the enterprise."
  },
  {
    "objectID": "w05/slides.html#data-warehouse-cloud-provider-definitions",
    "href": "w05/slides.html#data-warehouse-cloud-provider-definitions",
    "title": "Week 5: Data Engineering",
    "section": "Data Warehouse (Cloud Provider Definitions)",
    "text": "Data Warehouse (Cloud Provider Definitions)\nDefinitions in the wild (emphasis mine).\n\n\nAWS\nA data warehouse is a central repository of information that can be analyzed to make more informed decisions. Data flows into a data warehouse from transactional systems, relational databases, and other sources, typically on a regular cadence. Business analysts, data engineers, data scientists, and decision makers access the data through business intelligence (BI) tools, SQL clients, and other analytics applications.\nAzure\nA data warehouse is a centralized repository of integrated data from one or more disparate sources. Data warehouses store current and historical data and are used for reporting and analysis of the data.\n\nGCP\nA data warehouse is an enterprise system used for the analysis and reporting of structured and semi-structured data from multiple sources, such as point-of-sale transactions, marketing automation, customer relationship management, and more.\nSnowflake\nA data warehouse (DW) is a relational database that is designed for analytical rather than transactional work. It collects and aggregates data from one or many sources so it can be analyzed to produce business insights."
  },
  {
    "objectID": "w05/slides.html#working-with-a-cloud-data-warehouse",
    "href": "w05/slides.html#working-with-a-cloud-data-warehouse",
    "title": "Week 5: Data Engineering",
    "section": "Working with a Cloud Data Warehouse",
    "text": "Working with a Cloud Data Warehouse\nAll cloud providers provide a data warehouse solution that works in conjunction with their data lake solution.\n\nAWS has Redshift, Azure has Synapse Analytics. GCP has BigQuery and then there is Snowflake.\nIn a data warehouse, Online analytical processing (OLAP) allows for fast querying and analysis of data from different perspectives. It also helps in pre-aggregating and pre-calculating the information available in the archive.\nData warehouses are Peta Byte scale (Amazon RedShift, Google BigQuery, Azure Synapse Analytics).\nData warehouses can have dedicated compute provisioned or be serverless (BigQuery is serverless, Redshift allows both options now).\nData warehouses now offer integrated ML capabilities, you can build models with SQL and use them in queries (Amazon RedShift ML, Google BigQuery ML, Azure Synapse Analytics ML).\nIntegration with reporting and dashboarding tools such as Tableau, Grafana, Looker etc. and data analytics tools such as Spark and data cataloging tools.\nThey provide Fine Grained Access Control to the data."
  },
  {
    "objectID": "w05/slides.html#combining-data-lakes-and-data-warehouses",
    "href": "w05/slides.html#combining-data-lakes-and-data-warehouses",
    "title": "Week 5: Data Engineering",
    "section": "Combining Data Lakes and Data Warehouses",
    "text": "Combining Data Lakes and Data Warehouses\nCombine the flexibility, cost-efficiency, and scale of data lakes with the data management and ACID transactions of data warehouses to provide a single architecture that can enable business intelligence and machine learning on all data.\n\nBuild a lakehouse architecture on AWS\nThe Databricks lakehouse platform\nOpen data lakehouse on Google Cloud\nThe data lakehouse, the data warehouse and a modern data platform on Azure"
  },
  {
    "objectID": "w05/slides.html#nosql-databases",
    "href": "w05/slides.html#nosql-databases",
    "title": "Week 5: Data Engineering",
    "section": "NoSQL Databases",
    "text": "NoSQL Databases\nAt some point we needed to think beyond relation databases, because:\n\nData became more and more complex (not all data is tabular, thinkin JSON data emitted by an IoT device).\nCost of storage decreased (everything did not need to be stored in the 3rd normal form)\nMore data on the cloud meant data needed to be placed across different servers (scale-out)\nData needed to be placed intelligently in geo locations of interest\nAnd more‚Ä¶\n\n\nFrom Wikipedia: A NoSQL (originally referring to ‚Äúnon-SQL‚Äù or ‚Äúnon-relational‚Äù) database provides a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases. Such databases have existed since the late 1960s, but the name ‚ÄúNoSQL‚Äù was only coined in the early 21st century, triggered by the needs of Web 2.0 companies. NoSQL databases are increasingly used in big data and real-time web applications. NoSQL systems are also sometimes called Not only SQL to emphasize that they may support SQL-like query languages or sit alongside SQL databases in polyglot-persistent architectures."
  },
  {
    "objectID": "w05/slides.html#types-of-nosql-databases",
    "href": "w05/slides.html#types-of-nosql-databases",
    "title": "Week 5: Data Engineering",
    "section": "Types of NoSQL Databases",
    "text": "Types of NoSQL Databases\nOver time, four major types of NoSQL databases emerged: document databases, key-value databases, wide-column stores, and graph databases.\n\nDocument databases store data in documents similar to JSON (JavaScript Object Notation) objects. Each document contains pairs of fields and values. The values can typically be a variety of types including things like strings, numbers, booleans, arrays, or objects.\nKey-value databases are a simpler type of database where each item contains keys and values.\nWide-column stores store data in tables, rows, and dynamic columns.\nGraph databases store data in nodes and edges. Nodes typically store information about people, places, and things, while edges store information about the relationships between the nodes."
  },
  {
    "objectID": "w05/slides.html#examples-of-nosql-databases",
    "href": "w05/slides.html#examples-of-nosql-databases",
    "title": "Week 5: Data Engineering",
    "section": "Examples of NoSQL Databases",
    "text": "Examples of NoSQL Databases\n\n\n\n\n\n\n\nNOSQL Database Type\nExamples\n\n\n\n\nDocument Database\nAmazon DocumentDB, MongoDB, Cosmos DB, ArangoDB, Couchbase Server, CouchDB\n\n\nKey-value Database\nAmazon DynamoDB, Couchbase, Memcached, Redis\n\n\nWide-column datastores\nAmazon DynamoDB, Apache Cassandra, Google Bigtable, Azure Tables\n\n\nGraph databases\nAmazon Neptune, Neo4j\n\n\n\nAs a data scientist you would work with a NoSQL database through an SDK/API. Several programming languages are supported including Python, Java, Go, C++ etc."
  },
  {
    "objectID": "w05/slides.html#example-of-documents-in-a-document-database",
    "href": "w05/slides.html#example-of-documents-in-a-document-database",
    "title": "Week 5: Data Engineering",
    "section": "Example of documents in a document database",
    "text": "Example of documents in a document database\nHere is an example of a document inserted in a key-value/document database such as MongoDB.\n{\n   \"name\" : \"IS 350\",\n   \"year\" : \"2015\",\n   \"make\" : \"Lexus\",\n   \"colors\" : [\"black\",\"white\",\"grey\"],\n   \"spec\" : {\n      \"engine\" : \"V6\",\n      \"wheelbase\" : \"110.2 in\",\n      \"length\" : \"183.7 in\"\n   }\n}\nThe same example can be inserted in an Amazon DynamoDB table called (say) Cars.\ndatabase = boto3.resource('dynamodb')\ntable = database.Table('cars')\nitem = {\n   \"name\" : \"IS 350\",\n   \"year\" : \"2015\",\n   \"make\" : \"Lexus\",\n   \"colors\" : [\"black\",\"white\",\"grey\"],\n   \"spec\" : {\n      \"engine\" : \"V6\",\n      \"wheelbase\" : \"110.2 in\",\n      \"length\" : \"183.7 in\"\n   }\n}\ntable.put_item(Item = item)"
  },
  {
    "objectID": "w05/slides.html#other-data-stores-to-know-about",
    "href": "w05/slides.html#other-data-stores-to-know-about",
    "title": "Week 5: Data Engineering",
    "section": "Other Data Stores to Know About",
    "text": "Other Data Stores to Know About\nBesides the general concepts about data lkakes, warehouses, different types of databases, there are some purpose built databases that are good to know about.\n\nSplunk\nElasticsearch\nDuckDB\nMany many more‚Ä¶"
  },
  {
    "objectID": "w05/slides.html#further-reading",
    "href": "w05/slides.html#further-reading",
    "title": "Week 5: Data Engineering",
    "section": "Further Reading",
    "text": "Further Reading\nPlease lookup these topics on Google for further reading. Not providing specific links here because they all point to vendor specific products.\n\nData Catalog\nData Governance\nData Mesh\nData Fabric"
  },
  {
    "objectID": "w05/slides.html#boar-book-time",
    "href": "w05/slides.html#boar-book-time",
    "title": "Week 5: Data Engineering",
    "section": "Boar Book Time!",
    "text": "Boar Book Time!\n\n\nKleppmann (2017), pages 69-95"
  },
  {
    "objectID": "w05/slides.html#telemetry-analytics",
    "href": "w05/slides.html#telemetry-analytics",
    "title": "Week 5: Data Engineering",
    "section": "Telemetry ‚Üí Analytics",
    "text": "Telemetry ‚Üí Analytics\nRecall how many tiny bits of information are streaming into a given app‚Äôs servers at any given time!\n\nDatabase administrators therefore closely guard their OLTP databases. They are usually reluctant to let business analysts run ad hoc analytic queries on an OLTP database, since those queries are often expensive, scanning large parts of the dataset, which can harm the performance of concurrently executing transactions."
  },
  {
    "objectID": "w05/slides.html#what-is-the-quickest-possible-way-to-explore-data-in-s3",
    "href": "w05/slides.html#what-is-the-quickest-possible-way-to-explore-data-in-s3",
    "title": "Week 5: Data Engineering",
    "section": "What is the Quickest Possible Way to Explore Data in S3?",
    "text": "What is the Quickest Possible Way to Explore Data in S3?\n\nFrom this video"
  },
  {
    "objectID": "w05/slides.html#lab-analyzing-nyc-taxi-dataset-with-athena",
    "href": "w05/slides.html#lab-analyzing-nyc-taxi-dataset-with-athena",
    "title": "Week 5: Data Engineering",
    "section": "Lab: Analyzing NYC Taxi dataset with Athena",
    "text": "Lab: Analyzing NYC Taxi dataset with Athena\n\nA simple example of using DuckDB and Apache Arrow using NYC Taxi dataset\nThis notebook reads the NYC taxi dataset files for the first 6 months of the year 2023 (about ~20 million rows) and runs some analytics operation on this dataset. This dataset is too big to fit into memory.\n\nGitHub Classroom Link for Lab 5\nGitHub Classroom Link for Assignment 5\n\n\n\n\n\nKleppmann, Martin. 2017. Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems. O‚ÄôReilly Media, Inc. https://www.dropbox.com/scl/fi/42hm32h6qz8mrj5bm5eet/DesigningData-IntensiveApplications_TheBigIdeasBehind-Kleppmann-Martin.pdf?rlkey=nournir9s213rsmegi3eo0zn3&dl=1."
  },
  {
    "objectID": "writeups/sql-guide/index.html",
    "href": "writeups/sql-guide/index.html",
    "title": "From Pandas to SQL",
    "section": "",
    "text": "At some point during Week 4, you may have noticed a somewhat-jarring shift in our approach to data analytics‚Ä¶\nSpecifically, you may have noticed that:\nSo, given that your work in DSAN courses like DSAN 5000 and DSAN 5300 has been primarily oriented around Pandas syntax and coding patterns, the goal of this writeup is to get you up-to-speed with how you can ‚Äútranslate‚Äù the types of things you‚Äôre used to doing with Pandas code into equivalent SQL queries."
  },
  {
    "objectID": "writeups/sql-guide/index.html#eager-vs.-lazy-evaluation",
    "href": "writeups/sql-guide/index.html#eager-vs.-lazy-evaluation",
    "title": "From Pandas to SQL",
    "section": "Eager vs.¬†Lazy Evaluation",
    "text": "Eager vs.¬†Lazy Evaluation\nAs a final note before we ‚Äúzoom in‚Äù and focus on SQL, though: keep in mind that this move from code to queries is part of our endeavor in this class to start decoupling data from computation! In an eager-evaluation setting like Pandas code, when we write a data-processing pipeline, the steps are immediately evaluated, line-by-line (like any other Python code). Consider the following example, where the goal of the pipeline is to figure out our average ratio of sleep hours to work hours on weekdays:\n\n\nCode\nimport pandas as pd\nimport numpy as np\nrng = np.random.default_rng(seed=6000)\nimport calendar\nlist(calendar.day_name)\nsleep_df = pd.DataFrame({\n  'day': list(calendar.day_name),\n  'sleep_hrs': rng.uniform(3, 8, size=7),\n  'work_hrs': rng.uniform(6, 10, size=7),\n})\ndisplay(sleep_df)\n\n\n\n\n\n\n\n\n\nday\nsleep_hrs\nwork_hrs\n\n\n\n\n0\nMonday\n6.089095\n7.418997\n\n\n1\nTuesday\n7.912447\n8.747633\n\n\n2\nWednesday\n5.393727\n6.011502\n\n\n3\nThursday\n4.347202\n7.318025\n\n\n4\nFriday\n4.556042\n6.341836\n\n\n5\nSaturday\n4.552443\n7.279092\n\n\n6\nSunday\n6.606959\n6.652768\n\n\n\n\n\n\n\n\n\nCode\n# Step 1: Compute ratios\nratio_df = sleep_df.copy()\nratio_df['ratio'] = ratio_df['work_hrs'] / ratio_df['sleep_hrs']\ndisplay(ratio_df)\n\n\n\n\n\n\n\n\n\nday\nsleep_hrs\nwork_hrs\nratio\n\n\n\n\n0\nMonday\n6.089095\n7.418997\n1.218407\n\n\n1\nTuesday\n7.912447\n8.747633\n1.105553\n\n\n2\nWednesday\n5.393727\n6.011502\n1.114536\n\n\n3\nThursday\n4.347202\n7.318025\n1.683387\n\n\n4\nFriday\n4.556042\n6.341836\n1.391962\n\n\n5\nSaturday\n4.552443\n7.279092\n1.598942\n\n\n6\nSunday\n6.606959\n6.652768\n1.006934\n\n\n\n\n\n\n\n\n\nCode\n# Step 2: Filter to just weekdays\nweekend_days = ['Saturday', 'Sunday']\nweekday_df = ratio_df[~ratio_df['day'].isin(weekend_days)]\ndisplay(weekday_df)\n\n\n\n\n\n\n\n\n\nday\nsleep_hrs\nwork_hrs\nratio\n\n\n\n\n0\nMonday\n6.089095\n7.418997\n1.218407\n\n\n1\nTuesday\n7.912447\n8.747633\n1.105553\n\n\n2\nWednesday\n5.393727\n6.011502\n1.114536\n\n\n3\nThursday\n4.347202\n7.318025\n1.683387\n\n\n4\nFriday\n4.556042\n6.341836\n1.391962\n\n\n\n\n\n\n\n\n\nCode\n# Step 3: Compute mean ratio over the weekdays\nweekday_df['ratio'].mean()\n\n\n1.3027691063719424\n\n\nNotice here, however, that we are carrying out unnecessary computations in Step 1! Since we‚Äôre going to drop the weekend rows in Step 2 anyways, there is no need to compute the ratio for these two days. Since Python (and thus Pandas) is evaluated line-by-line, however, Pandas has no way of knowing that these two ratios don‚Äôt need to be computed. It simply does what it is told, dutifully computing all 7 ratios.\nThere is an alternative approach here: we could instead write out our ‚Äúplan‚Äù for this pipeline in advance, in the form of a query, before actually running any code! We can then utilize a query optimization engine like Polars, which can scan over our query and figure out that we don‚Äôt need to compute ratios for weekend days, since they would be dropped in Step 2! Here is the same pipeline re-written using Polars‚Äô LazyFrame in place of a Pandas DataFrame:\n\n\nCode\nimport polars as pl\nsleep_pl = pl.LazyFrame(sleep_df)\ncompute_ratio_expr = pl.col('work_hrs') / pl.col('sleep_hrs')\ncompute_ratios_expr = (\n  sleep_pl.with_columns(\n    compute_ratio_expr.alias('ratio')\n  ).filter(~pl.col('day').is_in(weekend_days))\n)\n\n\nNote that no computation has actually taken place yet! We have just written out our plan for the computation, in the form of Polars expressions (which don‚Äôt actually run until they‚Äôre paired with a context).\nBecause of this, we can see Polars‚Äô query optimization engine in action. First, we can use the optimized=False argument to show_graph() to see what our pipeline looks like without any optimizations (that is, running line-by-line as written, as our Pandas code did above):\n\n\nCode\ncompute_ratios_expr.show_graph(optimized=False)\n\n\n\n\n\n\n\n\n\nRemember that these query plans should be read from bottom to top. Now, compare that with the following graph, which shows our query after Polars scans over our instructions, detecting points where it can re-arrange our pipeline for greater efficiency:\n\n\nCode\ncompute_ratios_expr.show_graph()\n\n\n\n\n\n\n\n\n\nNotice the difference in the ordering of steps: Polars has successfully figured out that it should filter out the weekend days before carrying out the calculations!"
  },
  {
    "objectID": "writeups/sql-guide/index.html#onto-duckdb",
    "href": "writeups/sql-guide/index.html#onto-duckdb",
    "title": "From Pandas to SQL",
    "section": "Onto DuckDB!",
    "text": "Onto DuckDB!\nOnce you get the hang of the material in Weeks 4-6, it may slowly dawn on you that Polars is kind of a‚Ä¶ ‚Äúmiddle ground‚Äù compromise, allowing:\n\nPandas enthusiasts who are most comfortable writing Python code but want to write it in a way that it can be optimized before execution to ‚Äúmeet halfway‚Äù with\nSQL enthusiasts who are already used to writing all of their queries out before executing them\n\nI choose the ‚Äúmeet halfway‚Äù metaphor very specifically because, when you write Polars expressions like the compute_ratios_expr expression in the above code block, you are essentially writing SQL queries programmatically! Meaning, as you become more and more comfortable with Polars, you are also implicitly becoming more and more comfortable with writing queries, you just don‚Äôt know it yet!\nTo see what I mean, let‚Äôs do one final ‚Äútranslation‚Äù and re-write our pipeline once more using DuckDB this time:\n\n\nCode\nimport duckdb\ncon = duckdb.connect()\nratio_query = \"\"\"\nSELECT\n  day,\n  sleep_hrs,\n  work_hrs,\n  work_hrs / sleep_hrs AS ratio\nFROM sleep_df\nWHERE day NOT IN ('Saturday','Sunday')\n\"\"\"\nresult_df = con.execute(ratio_query).df()\nresult_df\n\n\n\n\n\n\n\n\n\nday\nsleep_hrs\nwork_hrs\nratio\n\n\n\n\n0\nMonday\n6.089095\n7.418997\n1.218407\n\n\n1\nTuesday\n7.912447\n8.747633\n1.105553\n\n\n2\nWednesday\n5.393727\n6.011502\n1.114536\n\n\n3\nThursday\n4.347202\n7.318025\n1.683387\n\n\n4\nFriday\n4.556042\n6.341836\n1.391962\n\n\n\n\n\n\n\nNow, just like with Polars, we can ask DuckDB to explain how it will carry out this query: the syntax is a bit weird, but you can just add the commands EXPLAIN ANALYZE to the beginning of any query to obtain DuckDB‚Äôs query plan:\n\n\nCode\nratio_query_explain = f\"EXPLAIN ANALYZE {ratio_query}\"\nprint(ratio_query_explain)\nexplanation = con.execute(ratio_query_explain).fetchone()\nprint(explanation[1])\n\n\nEXPLAIN ANALYZE \nSELECT\n  day,\n  sleep_hrs,\n  work_hrs,\n  work_hrs / sleep_hrs AS ratio\nFROM sleep_df\nWHERE day NOT IN ('Saturday','Sunday')\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ\n‚îÇ‚îÇ    Query Profiling Information    ‚îÇ‚îÇ\n‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\nEXPLAIN ANALYZE  SELECT   day,   sleep_hrs,   work_hrs,   work_hrs / sleep_hrs AS ratio FROM sleep_df WHERE day NOT IN ('Saturday','Sunday') \n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ\n‚îÇ‚îÇ        Total Time: 0.0004s        ‚îÇ‚îÇ\n‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ      RESULT_COLLECTOR     ‚îÇ\n‚îÇ   ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ   ‚îÇ\n‚îÇ             0             ‚îÇ\n‚îÇ          (0.00s)          ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                             \n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ      EXPLAIN_ANALYZE      ‚îÇ\n‚îÇ   ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ   ‚îÇ\n‚îÇ             0             ‚îÇ\n‚îÇ          (0.00s)          ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                             \n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ         PROJECTION        ‚îÇ\n‚îÇ   ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ   ‚îÇ\n‚îÇ            day            ‚îÇ\n‚îÇ         sleep_hrs         ‚îÇ\n‚îÇ          work_hrs         ‚îÇ\n‚îÇ           ratio           ‚îÇ\n‚îÇ   ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ   ‚îÇ\n‚îÇ             5             ‚îÇ\n‚îÇ          (0.00s)          ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                             \n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ           FILTER          ‚îÇ\n‚îÇ   ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ   ‚îÇ\n‚îÇ ((day != 'Saturday') AND  ‚îÇ\n‚îÇ     (day != 'Sunday'))    ‚îÇ\n‚îÇ   ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ   ‚îÇ\n‚îÇ           EC: 1           ‚îÇ\n‚îÇ   ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ   ‚îÇ\n‚îÇ             5             ‚îÇ\n‚îÇ          (0.00s)          ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                             \n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ        PANDAS_SCAN        ‚îÇ\n‚îÇ   ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ   ‚îÇ\n‚îÇ            day            ‚îÇ\n‚îÇ         sleep_hrs         ‚îÇ\n‚îÇ          work_hrs         ‚îÇ\n‚îÇ   ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ   ‚îÇ\n‚îÇ           EC: 7           ‚îÇ\n‚îÇ   ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ   ‚îÇ\n‚îÇ             7             ‚îÇ\n‚îÇ          (0.00s)          ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                             \n\n\n\nAnd, like before (again reading from bottom to top of the query plan), we can see that despite us writing the division work_hrs / sleep_hrs earlier in our query than the filtering step (the WHERE statement), DuckDB has also successfully figured out that executing these in reverse order is more efficient! (The PROJECTION step, occurring after the FILTER step, is the first step where the ratio result appears)"
  },
  {
    "objectID": "writeups/sql-guide/index.html#basic-pandas-operations-in-sql",
    "href": "writeups/sql-guide/index.html#basic-pandas-operations-in-sql",
    "title": "From Pandas to SQL",
    "section": "Basic Pandas Operations in SQL",
    "text": "Basic Pandas Operations in SQL\nWith all that in mind, you can now move towards thinking through how you can carry out the above sequence of ‚Äútranslations‚Äù ‚Äì from Pandas to Polars to DuckDB ‚Äì for other common Pandas tasks you‚Äôre used to from e.g.¬†DSAN 5000!\nTo nudge you in that direction, in this section we‚Äôll conclude with a few examples of common Pandas pipelines and how they translate to SQL code.\n\nFiltering Rows\nThis is probably the most straightforward operation in SQL world, since SQL provides us with the WHERE operator which we can pair with a boolean condition to filter the rows of our DataFrame.\nSo, for example, if we wanted to select one specific row in our DataFrame on the basis of some value, we can do this using the general syntax SELECT * FROM df WHERE condition. The following code uses this syntax to select just the row containing data for Thursday:\n\n\nCode\ncon.execute(\"\"\"\nSELECT * FROM sleep_df\nWHERE day = 'Friday'\n\"\"\").df()\n\n\n\n\n\n\n\n\n\nday\nsleep_hrs\nwork_hrs\n\n\n\n\n0\nFriday\n4.556042\n6.341836\n\n\n\n\n\n\n\nNotice how, since SQL is a ‚Äúdeclarative‚Äù language, it avoids the confusion which comes up in Python between the single-equals-sign assignment operator = and the double-equals-sign equality operator ==. In SQL, you can actually use either of these in a boolean operator to indicate that you‚Äôd like to check for equality, though the single-equals-sign = used above is more common:\n\n\nCode\ncon.execute(\"\"\"\nSELECT * FROM sleep_df\nWHERE day == 'Friday'\n\"\"\").df()\n\n\n\n\n\n\n\n\n\nday\nsleep_hrs\nwork_hrs\n\n\n\n\n0\nFriday\n4.556042\n6.341836\n\n\n\n\n\n\n\nYou may have noticed, however, that we didn‚Äôt use this = operator when filtering out weekend days above. Instead, we used the IN operator, along with the NOT operator for negation. The IN operator on its own works like:\n\n\nCode\ncon.execute(\"\"\"\nSELECT * FROM sleep_df\nWHERE day IN ('Monday','Wednesday','Friday')\n\"\"\").df()\n\n\n\n\n\n\n\n\n\nday\nsleep_hrs\nwork_hrs\n\n\n\n\n0\nMonday\n6.089095\n7.418997\n\n\n1\nWednesday\n5.393727\n6.011502\n\n\n2\nFriday\n4.556042\n6.341836\n\n\n\n\n\n\n\nAnd then we can use the NOT operator to obtain the exact opposite (the days which are not among the three listed days in the query):\n\n\nCode\ncon.execute(\"\"\"\nSELECT * FROM sleep_df\nWHERE day NOT IN ('Monday','Wednesday','Friday')\n\"\"\").df()\n\n\n\n\n\n\n\n\n\nday\nsleep_hrs\nwork_hrs\n\n\n\n\n0\nTuesday\n7.912447\n8.747633\n\n\n1\nThursday\n4.347202\n7.318025\n\n\n2\nSaturday\n4.552443\n7.279092\n\n\n3\nSunday\n6.606959\n6.652768\n\n\n\n\n\n\n\n\n\nSelecting Columns\nNotice how, up until now, we‚Äôve been using SELECT * at the beginning of our queries. That‚Äôs because our goal was to view the entire contents of the DataFrame, just with certain rows selected and others filtered out.\nIf we instead want to select certain columns, we can just replace the wildcard symbol * that we placed after SELECT with a specific list of columns:\n\n\nCode\ncon.execute(\"\"\"\nSELECT day, work_hrs FROM sleep_df\n\"\"\").df()\n\n\n\n\n\n\n\n\n\nday\nwork_hrs\n\n\n\n\n0\nMonday\n7.418997\n\n\n1\nTuesday\n8.747633\n\n\n2\nWednesday\n6.011502\n\n\n3\nThursday\n7.318025\n\n\n4\nFriday\n6.341836\n\n\n5\nSaturday\n7.279092\n\n\n6\nSunday\n6.652768\n\n\n\n\n\n\n\nImportantly, we can also match regular expression patterns in the names of columns, by using SELECT COLUMNS() rather than just SELECT. Here, for example, we use it to select all of the columns whose names end with hrs:\n\n\nCode\ncon.execute(\"\"\"\nSELECT COLUMNS('.+_hrs') FROM sleep_df\n\"\"\").df()\n\n\n\n\n\n\n\n\n\nsleep_hrs\nwork_hrs\n\n\n\n\n0\n6.089095\n7.418997\n\n\n1\n7.912447\n8.747633\n\n\n2\n5.393727\n6.011502\n\n\n3\n4.347202\n7.318025\n\n\n4\n4.556042\n6.341836\n\n\n5\n4.552443\n7.279092\n\n\n6\n6.606959\n6.652768\n\n\n\n\n\n\n\nNote that this COLUMNS() syntax is specific to DuckDB‚Äôs ‚Äúflavor‚Äù of SQL, and not standard in other implementations of SQL.\n\n\nDefining New Columns\nFinally, you may have noticed that the SELECT operator is basically doing double duty: not only did we use it in the previous section to request the three original columns in sleep_df (day, sleep_hrs, and work_hrs), but also to define a new column named ratio, by dividing work_hrs by sleep_hrs.\nThis is a general feature of SQL, and DuckDB (and the select() function in Polars)! We can SELECT the columns ‚Äúas-is‚Äù by just including their names, or we can define new columns within the select statement. For example, if we wanted the squared values of our two numeric columns for some reason, we could do the following:\n\n\nCode\ncon.execute(\"\"\"\nSELECT\n  day,\n  sleep_hrs,\n  sleep_hrs^2 AS sleep_squared,\n  work_hrs,\n  work_hrs^2 AS work_squared\nFROM sleep_df\n\"\"\").df()\n\n\n\n\n\n\n\n\n\nday\nsleep_hrs\nsleep_squared\nwork_hrs\nwork_squared\n\n\n\n\n0\nMonday\n6.089095\n37.077083\n7.418997\n55.041509\n\n\n1\nTuesday\n7.912447\n62.606823\n8.747633\n76.521076\n\n\n2\nWednesday\n5.393727\n29.092288\n6.011502\n36.138152\n\n\n3\nThursday\n4.347202\n18.898161\n7.318025\n53.553486\n\n\n4\nFriday\n4.556042\n20.757516\n6.341836\n40.218888\n\n\n5\nSaturday\n4.552443\n20.724741\n7.279092\n52.985182\n\n\n6\nSunday\n6.606959\n43.651905\n6.652768\n44.259327\n\n\n\n\n\n\n\n\n\nCombining Operations\nTo bring everything together, let‚Äôs see how we can combine the above three tasks into a single query!\n\n\nCode\ncon.execute(\"\"\"\nSELECT\n  day,\n  work_hrs,\n  work_hrs^2 AS work_squared\nFROM sleep_df\nWHERE day = 'Tuesday'\n\"\"\").df()\n\n\n\n\n\n\n\n\n\nday\nwork_hrs\nwork_squared\n\n\n\n\n0\nTuesday\n8.747633\n76.521076"
  },
  {
    "objectID": "w06/index.html",
    "href": "w06/index.html",
    "title": "Week 6: Introduction to Spark",
    "section": "",
    "text": "The content for today is split between the following two resources:",
    "crumbs": [
      "Week 6: <span class='sec-w06-date'>Sep 29</span>"
    ]
  },
  {
    "objectID": "w06/index.html#what-happens-when-not-embarrassingly-parallel",
    "href": "w06/index.html#what-happens-when-not-embarrassingly-parallel",
    "title": "Week 6: Introduction to Spark",
    "section": "What Happens When Not Embarrassingly Parallel?",
    "text": "What Happens When Not Embarrassingly Parallel?\n\nThink of the difference between linear and quadratic equations in algebra:\n\\(3x - 1 = 0\\) is ‚Äúembarrassingly‚Äù solvable, on its own: you can solve it directly, by adding 3 to both sides \\(\\implies x = \\frac{1}{3}\\). Same for \\(2x + 3 = 0 \\implies x = -\\frac{3}{2}\\)\nNow consider \\(6x^2 + 7x - 3 = 0\\): Harder to solve ‚Äúdirectly‚Äù, so your instinct might be to turn to the laborious quadratic equation:\n\n\\[\n\\begin{align*}\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} = \\frac{-7 \\pm \\sqrt{49 - 4(6)(-3)}}{2(6)} = \\frac{-7 \\pm 11}{12} = \\left\\{\\frac{1}{3},-\\frac{3}{2}\\right\\}\n\\end{align*}\n\\]\n\nAnd yet, \\(6x^2 + 7x - 3 = (3x - 1)(2x + 3)\\), meaning that we could have split the problem into two ‚Äúembarrassingly‚Äù solvable pieces, then multiplied to get result!",
    "crumbs": [
      "Week 6: <span class='sec-w06-date'>Sep 29</span>"
    ]
  },
  {
    "objectID": "w06/index.html#the-analogy-to-map-reduce",
    "href": "w06/index.html#the-analogy-to-map-reduce",
    "title": "Week 6: Introduction to Spark",
    "section": "The Analogy to Map-Reduce",
    "text": "The Analogy to Map-Reduce\n\n\n\n\n\n\n\n\\(\\leadsto\\) If code is not embarrassingly parallel (instinctually requiring laborious serial execution),\n\\(\\underbrace{6x^2 + 7x - 3 = 0}_{\\text{Solve using Quadratic Eqn}}\\)\n\n\nBut can be split into‚Ä¶\n\\((3x - 1)(2x + 3) = 0\\)\n\n\nEmbarrassingly parallel pieces which combine to same result,\n\\(\\underbrace{3x - 1 = 0}_{\\text{Solve directly}}, \\underbrace{2x + 3 = 0}_{\\text{Solve directly}}\\)\n\n\nWe can use map-reduce to achieve ultra speedup (running ‚Äúpieces‚Äù on GPU!)\n\\(\\underbrace{(3x-1)(2x+3) = 0}_{\\text{Solutions satisfy this product}}\\)",
    "crumbs": [
      "Week 6: <span class='sec-w06-date'>Sep 29</span>"
    ]
  },
  {
    "objectID": "w06/index.html#factoring-into-embarrassing-and-non-embarrassing-pieces",
    "href": "w06/index.html#factoring-into-embarrassing-and-non-embarrassing-pieces",
    "title": "Week 6: Introduction to Spark",
    "section": "‚ÄúFactoring‚Äù into Embarrassing and Non-Embarrassing Pieces",
    "text": "‚ÄúFactoring‚Äù into Embarrassing and Non-Embarrassing Pieces\n\nProblem from DSAN 5000/5100: Computing SSR (Sum of Squared Residuals)\n\\(y = (1,3,2), \\widehat{y} = (2, 5, 0) \\implies \\text{SSR} = (1-2)^2 + (3-5)^2 + (2-0)^2 = 9\\)\nComputing pieces separately:\n\nmap(do_something_with_piece, list_of_pieces)\n\nmy_map = map(lambda input: input**2, [(1-2), (3-5), (2-0)])\nmap_result = list(my_map)\nmap_result\n\n[1, 4, 4]\n\n\n\nCombining solved pieces\n\nreduce(how_to_combine_pair_of_pieces, pieces_to_combine)\n\nfrom functools import reduce\nmy_reduce = reduce(lambda piece1, piece2: piece1 + piece2, map_result)\nmy_reduce\n\n9",
    "crumbs": [
      "Week 6: <span class='sec-w06-date'>Sep 29</span>"
    ]
  },
  {
    "objectID": "w06/index.html#the-killer-application-matrix-multiplication",
    "href": "w06/index.html#the-killer-application-matrix-multiplication",
    "title": "Week 6: Introduction to Spark",
    "section": "The ‚ÄúKiller Application‚Äù: Matrix Multiplication",
    "text": "The ‚ÄúKiller Application‚Äù: Matrix Multiplication\n\n(I learned from Jeff Ullman, who did the obnoxious Stanford thing of mentioning in passing how ‚Äútwo previous students in the class did this for a cool final project on web crawling and, well, it escalated quickly‚Äù, aka became Google)\n\n\n\n\nFrom Leskovec, Rajaraman, and Ullman (2014), which is (legally) free online!",
    "crumbs": [
      "Week 6: <span class='sec-w06-date'>Sep 29</span>"
    ]
  },
  {
    "objectID": "w06/index.html#the-killer-way-to-learn-text-counts",
    "href": "w06/index.html#the-killer-way-to-learn-text-counts",
    "title": "Week 6: Introduction to Spark",
    "section": "The Killer Way-To-Learn: Text Counts!",
    "text": "The Killer Way-To-Learn: Text Counts!\n\n(2014): Text counts (2.2) \\(\\rightarrow\\) Matrix multiplication (2.3) \\(\\rightarrow \\cdots \\rightarrow\\) PageRank (5.1)\n(And yall thought it was just busywork for HW3 üòè)\nThe goal: User searches ‚ÄúDenzel Curry‚Äù‚Ä¶ How relevant is a given webpage?\nScenario 1: Entire internet fits on CPU \\(\\Rightarrow\\) Can just make a big big hash table:\n\n\n\n\n\n\n\n\nG\n\n\n\ninternet\n\nScan in O(n):\nToday Denzel Washington\nate a big bowl of Yum's\ncurry. Denzel allegedly\nrubbed his tum and said\n\"yum yum yum\" when he\ntasted today's curry.\n\"Yum! It is me Denzel,\ncurry is my fav!\", he\nexclaimed. According to\nhis friend Steph, curry\nis indeed Denzel's fav.\nWe are live with Del\nCurry in Washington for\na Denzel curry update.\n\n\n\n\nccounts\n\nOverall Counts\n\n('according',1)\n('allegedly',1)\n('ate',1)\n('big',1)\n('bowl',1)\n('curry',6)\n('del',1)\n('denzel',5)\n('exclaimed',1)\n('fav',2)\n('friend',1)\n('indeed',1)\n('live',1)\n('rubbed',1)\n('said',1)\n('steph',1)\n('tasted',1)\n('today',2)\n('tum',1)\n('update',1)\n('washington',2)\n('yum',4)\n\n\n\n\ninternet-&gt;ccounts\n\n\nHash Table",
    "crumbs": [
      "Week 6: <span class='sec-w06-date'>Sep 29</span>"
    ]
  },
  {
    "objectID": "w06/index.html#if-everything-doesnt-fit-on-cpu",
    "href": "w06/index.html#if-everything-doesnt-fit-on-cpu",
    "title": "Week 6: Introduction to Spark",
    "section": "If Everything Doesn‚Äôt Fit on CPU‚Ä¶",
    "text": "If Everything Doesn‚Äôt Fit on CPU‚Ä¶\n\n\n\nFrom Cornell Virtual Workshop, ‚ÄúUnderstanding GPU Architecture‚Äù",
    "crumbs": [
      "Week 6: <span class='sec-w06-date'>Sep 29</span>"
    ]
  },
  {
    "objectID": "w06/index.html#break-problem-into-chunks-for-the-green-bois",
    "href": "w06/index.html#break-problem-into-chunks-for-the-green-bois",
    "title": "Week 6: Introduction to Spark",
    "section": "Break Problem into Chunks for the Green Bois!",
    "text": "Break Problem into Chunks for the Green Bois!\n\n\n\n\n\n\n\nG\n\n\n\nchunked\n\nChunked Document\n\nToday Denzel Washington\nate a big bowl of Yum's\ncurry. Denzel allegedly\nrubbed his tum and said\n\n\"yum yum yum\" when he\ntasted today's curry.\n\"Yum! It is me Denzel,\ncurry is my fav!\", he\n\nexclaimed. According to\nhis friend Steph, curry\nis indeed Denzel's fav.\nWe are live with Del\nCurry in Washington for\na Denzel curry update.\n\n\n\n\nchcounts\n\nChunked Counts\n\n('today',1)\n('denzel',1)\n...\n('tum',1)\n('said',1)\n\n('yum',1)\n('yum',1)\n('yum',1)\n...\n('fav',1)\n\n('exclaimed',1)\n...\n('del',1)\n('curry',1)\n('washington',1)\n('denzel',1)\n('curry',1)\n('update',1)\n\n\n\n\nchunked:p1-&gt;chcounts:p1\n\n\nO(n/4)\n\n\n\nchunked:p2-&gt;chcounts:p2\n\n\nO(n/4)\n\n\n\nchunked:p3-&gt;chcounts:p3\n\n\nO(n/4)\n\n\n\nchunked:p4-&gt;chcounts:p4\n\n\nO(n/4)\n\n\n\nscounts\n\nHashed Counts\n\n('allegedly',1)\n...\n('curry',1)\n('denzel',2)\n...\n('yum',1)\n\n('curry',2)\n('denzel',1)\n...\n('yum',4)\n\n('according',1)\n('curry',1)\n('del',1)\n('denzel',1)\n...\n('curry',2)\n('denzel',1)\n('update',1)\n('washington',1)\n\n\n\n\nchcounts:p1-&gt;scounts:p1\n\n\nO(n/4)\n\n\n\nchcounts:p2-&gt;scounts:p2\n\n\nO(n/4)\n\n\n\nchcounts:p3-&gt;scounts:p3\n\n\nO(n/4)\n\n\n\nchcounts:p4-&gt;scounts:p4\n\n\nO(n/4)\n\n\n\nccounts\n\nOverall Counts\n('according',1)\n('allegedly',1)\n('ate',1)\n('big',1)\n('bowl',1)\n('curry',6)\n('del',1)\n('denzel',5)\n('exclaimed',1)\n('fav',2)\n('friend',1)\n('indeed',1)\n('live',1)\n('rubbed',1)\n('said',1)\n('steph',1)\n('tasted',1)\n('today',2)\n('tum',1)\n('update',1)\n('washington',2)\n('yum',4)\n\n\n\n\nscounts:p1-&gt;ccounts:p1\n\n\n\n\n\nscounts:p2-&gt;ccounts:p1\n\n\n\n\n\nscounts:p3-&gt;ccounts:p1\n\n\n\n\n\nscounts:p4-&gt;ccounts:p1\n\n\n\n\n\nscounts:p2-&gt;ccounts\n\n\n \n\nMerge in\nO(n)\n\n\n\n\n\n\n\n\n\n\\(\\implies\\) Total = \\(O(3n) = O(n)\\)\nBut also optimized in terms of constants, because of sequential memory reads",
    "crumbs": [
      "Week 6: <span class='sec-w06-date'>Sep 29</span>"
    ]
  },
  {
    "objectID": "w06/index.html#in-class-demo-1",
    "href": "w06/index.html#in-class-demo-1",
    "title": "Week 6: Introduction to Spark",
    "section": "In-Class Demo 1",
    "text": "In-Class Demo 1\n\n\n\n\n\nMap-Reduced Matrix-Vector Multiplication Demo",
    "crumbs": [
      "Week 6: <span class='sec-w06-date'>Sep 29</span>"
    ]
  },
  {
    "objectID": "w06/index.html#in-class-demo-2",
    "href": "w06/index.html#in-class-demo-2",
    "title": "Week 6: Introduction to Spark",
    "section": "In-Class Demo 2",
    "text": "In-Class Demo 2\n\n\n\n\n\n\nCars! Driving!\n\n\n\nAdapted from/almost identical to Jacob Celestine‚Äôs tutorial",
    "crumbs": [
      "Week 6: <span class='sec-w06-date'>Sep 29</span>"
    ]
  },
  {
    "objectID": "w06/index.html#in-class-demo-3",
    "href": "w06/index.html#in-class-demo-3",
    "title": "Week 6: Introduction to Spark",
    "section": "In-Class Demo 3",
    "text": "In-Class Demo 3\n\n\n\n\n\nTennis. Swingin. Rackets. Scorin goals.",
    "crumbs": [
      "Week 6: <span class='sec-w06-date'>Sep 29</span>"
    ]
  },
  {
    "objectID": "w06/index.html#lab-time",
    "href": "w06/index.html#lab-time",
    "title": "Week 6: Introduction to Spark",
    "section": "Lab Time!",
    "text": "Lab Time!",
    "crumbs": [
      "Week 6: <span class='sec-w06-date'>Sep 29</span>"
    ]
  },
  {
    "objectID": "w06/index.html#references",
    "href": "w06/index.html#references",
    "title": "Week 6: Introduction to Spark",
    "section": "References",
    "text": "References\n\n\nLeskovec, Jure, Anand Rajaraman, and Jeffrey David Ullman. 2014. Mining of Massive Datasets. Cambridge University Press.",
    "crumbs": [
      "Week 6: <span class='sec-w06-date'>Sep 29</span>"
    ]
  },
  {
    "objectID": "w06/slides.html#what-happens-when-not-embarrassingly-parallel",
    "href": "w06/slides.html#what-happens-when-not-embarrassingly-parallel",
    "title": "Week 6: Introduction to Spark",
    "section": "What Happens When Not Embarrassingly Parallel?",
    "text": "What Happens When Not Embarrassingly Parallel?\n\nThink of the difference between linear and quadratic equations in algebra:\n\\(3x - 1 = 0\\) is ‚Äúembarrassingly‚Äù solvable, on its own: you can solve it directly, by adding 3 to both sides \\(\\implies x = \\frac{1}{3}\\). Same for \\(2x + 3 = 0 \\implies x = -\\frac{3}{2}\\)\nNow consider \\(6x^2 + 7x - 3 = 0\\): Harder to solve ‚Äúdirectly‚Äù, so your instinct might be to turn to the laborious quadratic equation:\n\n\\[\n\\begin{align*}\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} = \\frac{-7 \\pm \\sqrt{49 - 4(6)(-3)}}{2(6)} = \\frac{-7 \\pm 11}{12} = \\left\\{\\frac{1}{3},-\\frac{3}{2}\\right\\}\n\\end{align*}\n\\]\n\nAnd yet, \\(6x^2 + 7x - 3 = (3x - 1)(2x + 3)\\), meaning that we could have split the problem into two ‚Äúembarrassingly‚Äù solvable pieces, then multiplied to get result!"
  },
  {
    "objectID": "w06/slides.html#the-analogy-to-map-reduce",
    "href": "w06/slides.html#the-analogy-to-map-reduce",
    "title": "Week 6: Introduction to Spark",
    "section": "The Analogy to Map-Reduce",
    "text": "The Analogy to Map-Reduce\n\n\n\n\n\n\n\n\\(\\leadsto\\) If code is not embarrassingly parallel (instinctually requiring laborious serial execution),\n\\(\\underbrace{6x^2 + 7x - 3 = 0}_{\\text{Solve using Quadratic Eqn}}\\)\n\n\nBut can be split into‚Ä¶\n\\((3x - 1)(2x + 3) = 0\\)\n\n\nEmbarrassingly parallel pieces which combine to same result,\n\\(\\underbrace{3x - 1 = 0}_{\\text{Solve directly}}, \\underbrace{2x + 3 = 0}_{\\text{Solve directly}}\\)\n\n\nWe can use map-reduce to achieve ultra speedup (running ‚Äúpieces‚Äù on GPU!)\n\\(\\underbrace{(3x-1)(2x+3) = 0}_{\\text{Solutions satisfy this product}}\\)"
  },
  {
    "objectID": "w06/slides.html#factoring-into-embarrassing-and-non-embarrassing-pieces",
    "href": "w06/slides.html#factoring-into-embarrassing-and-non-embarrassing-pieces",
    "title": "Week 6: Introduction to Spark",
    "section": "‚ÄúFactoring‚Äù into Embarrassing and Non-Embarrassing Pieces",
    "text": "‚ÄúFactoring‚Äù into Embarrassing and Non-Embarrassing Pieces\n\nProblem from DSAN 5000/5100: Computing SSR (Sum of Squared Residuals)\n\\(y = (1,3,2), \\widehat{y} = (2, 5, 0) \\implies \\text{SSR} = (1-2)^2 + (3-5)^2 + (2-0)^2 = 9\\)\nComputing pieces separately:\n\nmap(do_something_with_piece, list_of_pieces)\n\n\n[1, 4, 4]\n\n\n\nCombining solved pieces\n\nreduce(how_to_combine_pair_of_pieces, pieces_to_combine)\n\n\n9"
  },
  {
    "objectID": "w06/slides.html#the-killer-application-matrix-multiplication",
    "href": "w06/slides.html#the-killer-application-matrix-multiplication",
    "title": "Week 6: Introduction to Spark",
    "section": "The ‚ÄúKiller Application‚Äù: Matrix Multiplication",
    "text": "The ‚ÄúKiller Application‚Äù: Matrix Multiplication\n\n(I learned from Jeff Ullman, who did the obnoxious Stanford thing of mentioning in passing how ‚Äútwo previous students in the class did this for a cool final project on web crawling and, well, it escalated quickly‚Äù, aka became Google)\n\n\nFrom Leskovec, Rajaraman, and Ullman (2014), which is (legally) free online!"
  },
  {
    "objectID": "w06/slides.html#the-killer-way-to-learn-text-counts",
    "href": "w06/slides.html#the-killer-way-to-learn-text-counts",
    "title": "Week 6: Introduction to Spark",
    "section": "The Killer Way-To-Learn: Text Counts!",
    "text": "The Killer Way-To-Learn: Text Counts!\n\n(2014): Text counts (2.2) \\(\\rightarrow\\) Matrix multiplication (2.3) \\(\\rightarrow \\cdots \\rightarrow\\) PageRank (5.1)\n(And yall thought it was just busywork for HW3 üòè)\nThe goal: User searches ‚ÄúDenzel Curry‚Äù‚Ä¶ How relevant is a given webpage?\nScenario 1: Entire internet fits on CPU \\(\\Rightarrow\\) Can just make a big big hash table:\n\n\n\n\n\n\n\n\nG\n\n\n\ninternet\n\nScan in O(n):\nToday Denzel Washington\nate a big bowl of Yum's\ncurry. Denzel allegedly\nrubbed his tum and said\n\"yum yum yum\" when he\ntasted today's curry.\n\"Yum! It is me Denzel,\ncurry is my fav!\", he\nexclaimed. According to\nhis friend Steph, curry\nis indeed Denzel's fav.\nWe are live with Del\nCurry in Washington for\na Denzel curry update.\n\n\n\n\nccounts\n\nOverall Counts\n\n('according',1)\n('allegedly',1)\n('ate',1)\n('big',1)\n('bowl',1)\n('curry',6)\n('del',1)\n('denzel',5)\n('exclaimed',1)\n('fav',2)\n('friend',1)\n('indeed',1)\n('live',1)\n('rubbed',1)\n('said',1)\n('steph',1)\n('tasted',1)\n('today',2)\n('tum',1)\n('update',1)\n('washington',2)\n('yum',4)\n\n\n\n\ninternet-&gt;ccounts\n\n\nHash Table"
  },
  {
    "objectID": "w06/slides.html#if-everything-doesnt-fit-on-cpu",
    "href": "w06/slides.html#if-everything-doesnt-fit-on-cpu",
    "title": "Week 6: Introduction to Spark",
    "section": "If Everything Doesn‚Äôt Fit on CPU‚Ä¶",
    "text": "If Everything Doesn‚Äôt Fit on CPU‚Ä¶\n\nFrom Cornell Virtual Workshop, ‚ÄúUnderstanding GPU Architecture‚Äù"
  },
  {
    "objectID": "w06/slides.html#break-problem-into-chunks-for-the-green-bois",
    "href": "w06/slides.html#break-problem-into-chunks-for-the-green-bois",
    "title": "Week 6: Introduction to Spark",
    "section": "Break Problem into Chunks for the Green Bois!",
    "text": "Break Problem into Chunks for the Green Bois!\n\n\n\n\n\n\n\nG\n\n\n\nchunked\n\nChunked Document\n\nToday Denzel Washington\nate a big bowl of Yum's\ncurry. Denzel allegedly\nrubbed his tum and said\n\n\"yum yum yum\" when he\ntasted today's curry.\n\"Yum! It is me Denzel,\ncurry is my fav!\", he\n\nexclaimed. According to\nhis friend Steph, curry\nis indeed Denzel's fav.\nWe are live with Del\nCurry in Washington for\na Denzel curry update.\n\n\n\n\nchcounts\n\nChunked Counts\n\n('today',1)\n('denzel',1)\n...\n('tum',1)\n('said',1)\n\n('yum',1)\n('yum',1)\n('yum',1)\n...\n('fav',1)\n\n('exclaimed',1)\n...\n('del',1)\n('curry',1)\n('washington',1)\n('denzel',1)\n('curry',1)\n('update',1)\n\n\n\n\nchunked:p1-&gt;chcounts:p1\n\n\nO(n/4)\n\n\n\nchunked:p2-&gt;chcounts:p2\n\n\nO(n/4)\n\n\n\nchunked:p3-&gt;chcounts:p3\n\n\nO(n/4)\n\n\n\nchunked:p4-&gt;chcounts:p4\n\n\nO(n/4)\n\n\n\nscounts\n\nHashed Counts\n\n('allegedly',1)\n...\n('curry',1)\n('denzel',2)\n...\n('yum',1)\n\n('curry',2)\n('denzel',1)\n...\n('yum',4)\n\n('according',1)\n('curry',1)\n('del',1)\n('denzel',1)\n...\n('curry',2)\n('denzel',1)\n('update',1)\n('washington',1)\n\n\n\n\nchcounts:p1-&gt;scounts:p1\n\n\nO(n/4)\n\n\n\nchcounts:p2-&gt;scounts:p2\n\n\nO(n/4)\n\n\n\nchcounts:p3-&gt;scounts:p3\n\n\nO(n/4)\n\n\n\nchcounts:p4-&gt;scounts:p4\n\n\nO(n/4)\n\n\n\nccounts\n\nOverall Counts\n('according',1)\n('allegedly',1)\n('ate',1)\n('big',1)\n('bowl',1)\n('curry',6)\n('del',1)\n('denzel',5)\n('exclaimed',1)\n('fav',2)\n('friend',1)\n('indeed',1)\n('live',1)\n('rubbed',1)\n('said',1)\n('steph',1)\n('tasted',1)\n('today',2)\n('tum',1)\n('update',1)\n('washington',2)\n('yum',4)\n\n\n\n\nscounts:p1-&gt;ccounts:p1\n\n\n\n\n\nscounts:p2-&gt;ccounts:p1\n\n\n\n\n\nscounts:p3-&gt;ccounts:p1\n\n\n\n\n\nscounts:p4-&gt;ccounts:p1\n\n\n\n\n\nscounts:p2-&gt;ccounts\n\n\n \n\nMerge in\nO(n)\n\n\n\n\n\n\n\n\n\n\\(\\implies\\) Total = \\(O(3n) = O(n)\\)\nBut also optimized in terms of constants, because of sequential memory reads"
  },
  {
    "objectID": "w06/slides.html#in-class-demo-1",
    "href": "w06/slides.html#in-class-demo-1",
    "title": "Week 6: Introduction to Spark",
    "section": "In-Class Demo 1",
    "text": "In-Class Demo 1\n\nMap-Reduced Matrix-Vector Multiplication Demo"
  },
  {
    "objectID": "w06/slides.html#in-class-demo-2",
    "href": "w06/slides.html#in-class-demo-2",
    "title": "Week 6: Introduction to Spark",
    "section": "In-Class Demo 2",
    "text": "In-Class Demo 2\n\n\n\n\n\n\nCars! Driving!\n\n\n\nAdapted from/almost identical to Jacob Celestine‚Äôs tutorial"
  },
  {
    "objectID": "w06/slides.html#in-class-demo-3",
    "href": "w06/slides.html#in-class-demo-3",
    "title": "Week 6: Introduction to Spark",
    "section": "In-Class Demo 3",
    "text": "In-Class Demo 3\n\nTennis. Swingin. Rackets. Scorin goals."
  },
  {
    "objectID": "w06/slides.html#lab-time",
    "href": "w06/slides.html#lab-time",
    "title": "Week 6: Introduction to Spark",
    "section": "Lab Time!",
    "text": "Lab Time!"
  },
  {
    "objectID": "w06/slides.html#references",
    "href": "w06/slides.html#references",
    "title": "Week 6: Introduction to Spark",
    "section": "References",
    "text": "References\n\n\nLeskovec, Jure, Anand Rajaraman, and Jeffrey David Ullman. 2014. Mining of Massive Datasets. Cambridge University Press."
  }
]